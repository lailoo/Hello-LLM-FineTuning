# AIå¤§æ¨¡å‹å››é˜¶é€šå…³æŒ‡å—ï¼šä»æç¤ºè¯åˆ°é¢„è®­ç»ƒï¼Œä½æˆæœ¬ææ•ˆåˆ°æ ¸å¿ƒèƒ½åŠ›æ„å»º


![AIå¤§æ¨¡å‹å››é˜¶é€šå…³æŒ‡å—ï¼šä»æç¤ºè¯åˆ°é¢„è®­ç»ƒï¼Œä½æˆæœ¬ææ•ˆåˆ°æ ¸å¿ƒèƒ½åŠ›æ„å»º - æ¶æ„å›¾](./images/a7b8b618285b4432a6efe08206efef6d.png)

*AIå¤§æ¨¡å‹å››é˜¶é€šå…³æŒ‡å—ï¼šä»æç¤ºè¯åˆ°é¢„è®­ç»ƒï¼Œä½æˆæœ¬ææ•ˆåˆ°æ ¸å¿ƒèƒ½åŠ›æ„å»º - ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ*


---


## æç¤ºè¯å·¥ç¨‹ â†’ æ™ºèƒ½ä½“ â†’ å¾®è°ƒ â†’ é¢„è®­ç»ƒ | ä¸­çº§å¼€å‘è€…å®æˆ˜è·¯çº¿å›¾

**é˜…è¯»æ—¶é—´**: 30 min

> ç”¨ä¸€å¼ å››é˜¶æŠ€æœ¯åœ°å›¾ï¼Œå¸®ä½ é¿å¼€AIè½åœ°é™·é˜±ï¼Œç²¾å‡†æŠ•èµ„æ¯ä¸€åˆ†ç®—åŠ›ä¸æ—¶é—´ã€‚

## ç›®å½•

- [ç¬¬ä¸€é˜¶ï¼šæç¤ºè¯å·¥ç¨‹ â€”â€” é›¶ä»£ç ææ•ˆåˆ©å™¨](#ç¬¬ä¸€é˜¶æç¤ºè¯å·¥ç¨‹-â€”â€”-é›¶ä»£ç ææ•ˆåˆ©å™¨)
- [ç¬¬äºŒé˜¶ï¼šAIæ™ºèƒ½ä½“ â€”â€” åº”ç”¨é€»è¾‘é‡æ„è€…](#ç¬¬äºŒé˜¶aiæ™ºèƒ½ä½“-â€”â€”-åº”ç”¨é€»è¾‘é‡æ„è€…)
- [ç¬¬ä¸‰é˜¶ï¼šå¤§æ¨¡å‹å¾®è°ƒ â€”â€” å®šåˆ¶åŒ–èƒ½åŠ›å¼•æ“](#ç¬¬ä¸‰é˜¶å¤§æ¨¡å‹å¾®è°ƒ-â€”â€”-å®šåˆ¶åŒ–èƒ½åŠ›å¼•æ“)
- [ç¬¬å››é˜¶ï¼šé¢„è®­ç»ƒæŠ€æœ¯ â€”â€” æ„å»ºæ ¸å¿ƒæŠ¤åŸæ²³](#ç¬¬å››é˜¶é¢„è®­ç»ƒæŠ€æœ¯-â€”â€”-æ„å»ºæ ¸å¿ƒæŠ¤åŸæ²³)


---


å½“AIå¤§æ¨¡å‹ä»å®éªŒå®¤èµ°å‘æ—¥å¸¸å¼€å‘ï¼Œä¸ªäººä¸å›¢é˜Ÿå¸¸é™·å…¥â€˜æŠ€æœ¯é€‰æ‹©ç„¦è™‘â€™â€”â€”æ˜¯è¯¥ä¸“æ³¨Promptä¼˜åŒ–ï¼Œè¿˜æ˜¯æŠ•å…¥æ™ºèƒ½ä½“æ¶æ„ï¼Ÿæ˜¯å¦å€¼å¾—å¾®è°ƒæ¨¡å‹ï¼Ÿä½•æ—¶è¯¥è‡ªç ”é¢„è®­ç»ƒï¼Ÿæœ¬æ–‡ä»¥â€˜æŠ•å…¥äº§å‡ºæ¯”â€™ä¸ºè½´å¿ƒï¼Œä¸ºä½ æ¢³ç†ä¸€æ¡æ¸…æ™°çš„å››é˜¶è¿›é˜¶è·¯å¾„ï¼šä»é›¶æˆæœ¬ææ•ˆçš„æç¤ºè¯å·¥ç¨‹èµ·æ­¥ï¼Œé€æ­¥è¿‡æ¸¡åˆ°é‡æ„åº”ç”¨é€»è¾‘çš„AIæ™ºèƒ½ä½“ã€å®ç°ä¸“å±èƒ½åŠ›çš„å¤§æ¨¡å‹å¾®è°ƒï¼Œæœ€ç»ˆæŠµè¾¾æŒæ¡æ ¸å¿ƒå£å’çš„é¢„è®­ç»ƒå±‚ã€‚æ— è®ºä½ æ˜¯ç‹¬ç«‹å¼€å‘è€…è¿˜æ˜¯ä¼ä¸šæŠ€æœ¯è´Ÿè´£äººï¼Œéƒ½èƒ½åœ¨è¿™æ¡è·¯å¾„ä¸­æ‰¾åˆ°å±äºä½ çš„ä»·å€¼é”šç‚¹ã€‚


---


## ç¬¬ä¸€é˜¶ï¼šæç¤ºè¯å·¥ç¨‹ â€”â€” é›¶ä»£ç ææ•ˆåˆ©å™¨

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šå‘AIæé—®åï¼Œå¾—åˆ°çš„å›ç­”è¦ä¹ˆæ¨¡ç³Šä¸æ¸…ï¼Œè¦ä¹ˆåç¦»ä¸»é¢˜ï¼Œç”šè‡³éœ€è¦åå¤ä¿®æ”¹æé—®æ–¹å¼æ‰èƒ½è·å¾—ç†æƒ³ç»“æœï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶æ¥åˆ°è€æ¿ç´§æ€¥éœ€æ±‚â€”â€”â€œæ˜å¤©æ—©ä¸Š9ç‚¹å‰ï¼Œæˆ‘è¦ä¸€ä»½åŒ…å«å¸‚åœºè¶‹åŠ¿ã€ç«å“åˆ†æå’Œç”¨æˆ·ç”»åƒçš„ä¸‰åˆä¸€æŠ¥å‘Šâ€ï¼Œè€Œä½ æ‰‹å¤´æ²¡æœ‰ä»»ä½•ç°æˆæ•°æ®ã€‚åœ¨ä¼ ç»Ÿå·¥ä½œæµä¸­ï¼Œè¿™å¯èƒ½æ„å‘³ç€é€šå®µåŠ ç­ï¼›ä½†åœ¨AIæ—¶ä»£ï¼Œåªéœ€ä¸€æ¡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ï¼ŒGPT-4å°±èƒ½åœ¨å‡ åˆ†é’Ÿå†…ä¸ºä½ ç”Ÿæˆç»“æ„æ¸…æ™°ã€å†…å®¹ç¿”å®çš„åˆç¨¿ã€‚

> æç¤ºè¯å·¥ç¨‹æ˜¯AIæ—¶ä»£çš„ç¬¬ä¸€ç”Ÿäº§åŠ›æ æ†ï¼Œæ— éœ€ä¸€è¡Œä»£ç å³å¯æå‡50%+ä»»åŠ¡æ•ˆç‡ã€‚

è¿™ä¸æ˜¯å¤¸å¼ ã€‚æ ¹æ®OpenAIå†…éƒ¨æµ‹è¯•æ•°æ®ï¼Œä½¿ç”¨ä¼˜åŒ–æç¤ºè¯çš„ç”¨æˆ·ç›¸æ¯”æ™®é€šæé—®è€…ï¼Œä»»åŠ¡å®Œæˆæ•ˆç‡å¹³å‡æå‡67%ï¼Œè¾“å‡ºè´¨é‡è¯„åˆ†æé«˜42%ã€‚æç¤ºè¯å·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰æ­£æˆä¸ºæ¯ä¸ªçŸ¥è¯†å·¥ä½œè€…å¿…é¡»æŒæ¡çš„â€œæ–°åŸºç¡€æŠ€èƒ½â€ã€‚


---


### ä»€ä¹ˆæ˜¯æç¤ºè¯å·¥ç¨‹ï¼Ÿå®ƒåœ¨AIå·¥ä½œæµä¸­çš„å®šä½

æç¤ºè¯å·¥ç¨‹ï¼Œç®€å•æ¥è¯´ï¼Œå°±æ˜¯é€šè¿‡è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡ºæ›´ç²¾å‡†ã€ç»“æ„åŒ–ã€ç¬¦åˆé¢„æœŸçš„ç»“æœã€‚ä½ å¯ä»¥æŠŠå®ƒç±»æ¯”ä¸ºâ€œä¸AIå¯¹è¯çš„ç¿»è¯‘å®˜â€â€”â€”ä¸æ˜¯æ”¹å˜AIçš„èƒ½åŠ›è¾¹ç•Œï¼Œè€Œæ˜¯ç”¨æœ€åˆé€‚çš„è¯­è¨€æ¿€å‘å®ƒçš„æœ€å¤§æ½œèƒ½ã€‚

åœ¨å…¸å‹AIå·¥ä½œæµä¸­ï¼Œæç¤ºè¯å·¥ç¨‹ä½äºâ€œç”¨æˆ·æ„å›¾â€ä¸â€œæ¨¡å‹æ‰§è¡Œâ€ä¹‹é—´çš„å…³é”®æ¡¥æ¢ä½ç½®ï¼š

```mermaid
flowchart LR
    A[ç”¨æˆ·åŸå§‹éœ€æ±‚] --> B[æç¤ºæ¨¡æ¿ä¼˜åŒ–]
    B --> C[LLMå¤„ç†æ‰§è¡Œ]
    C --> D[ç»“æ„åŒ–è¾“å‡ºæˆæœ]
    style A fill:#f9f,stroke:#333
    style B fill:#bbf,stroke:#333
    style C fill:#fbb,stroke:#333
    style D fill:#bfb,stroke:#333
```

*æç¤ºè¯å·¥ç¨‹æ ¸å¿ƒæµç¨‹ï¼šä»ç”¨æˆ·åŸå§‹éœ€æ±‚åˆ°ç»“æ„åŒ–è¾“å‡ºçš„è½¬åŒ–è·¯å¾„*

åŸå§‹éœ€æ±‚ï¼ˆå¦‚â€œå†™ä»½æŠ¥å‘Šâ€ï¼‰ç»è¿‡æç¤ºè¯å·¥ç¨‹å¸ˆä¹‹æ‰‹ï¼Œè¢«è½¬åŒ–ä¸ºå¸¦æœ‰è§’è‰²è®¾å®šã€æ ¼å¼çº¦æŸã€æ€ç»´å¼•å¯¼çš„ç»“æ„åŒ–æŒ‡ä»¤ï¼Œå†äº¤ç”±LLMå¤„ç†ï¼Œæœ€ç»ˆè¾“å‡ºå¯ç›´æ¥ä½¿ç”¨çš„æˆæœã€‚æ•´ä¸ªè¿‡ç¨‹æ— éœ€ç¼–ç¨‹ï¼Œå´èƒ½å®ç°é«˜åº¦å®šåˆ¶åŒ–çš„æ™ºèƒ½è¾“å‡ºã€‚


---


### å¸¸ç”¨æŠ€å·§ï¼šè§’è‰²è®¾å®šã€æ€ç»´é“¾ã€Few-shotç¤ºä¾‹

è¦å†™å‡ºé«˜æ•ˆæç¤ºè¯ï¼Œä¸‰å¤§æ ¸å¿ƒæŠ€å·§ç¼ºä¸€ä¸å¯ï¼š

1. **è§’è‰²è®¾å®šï¼ˆRole Promptingï¼‰**  
   æ˜ç¡®å‘Šè¯‰AIâ€œä½ æ˜¯ä»€ä¹ˆèº«ä»½â€ã€‚æ¯”å¦‚ï¼šâ€œä½ æ˜¯ä¸€ä½èµ„æ·±å¸‚åœºåˆ†æå¸ˆï¼Œæ“…é•¿ä»æ•°æ®ä¸­æç‚¼å•†ä¸šæ´å¯Ÿã€‚â€è¿™èƒ½è®©æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢è¯­å¢ƒå’Œè¡¨è¾¾é£æ ¼ã€‚

2. **æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰**  
   å¼•å¯¼æ¨¡å‹åˆ†æ­¥éª¤æ€è€ƒã€‚ä¾‹å¦‚ï¼šâ€œè¯·å…ˆåˆ—å‡ºä¸‰ä¸ªä¸»è¦å¸‚åœºè¶‹åŠ¿ï¼Œå†é’ˆå¯¹æ¯ä¸ªè¶‹åŠ¿åˆ†æå…¶å¯¹ä¸­å°ä¼ä¸šçš„æ½œåœ¨å½±å“ï¼Œæœ€åç»™å‡ºåº”å¯¹å»ºè®®ã€‚â€è¿™ç§ç»“æ„åŒ–å¼•å¯¼æ˜¾è‘—æå‡å¤æ‚ä»»åŠ¡çš„å®Œæˆåº¦ã€‚

3. **Few-shotç¤ºä¾‹ï¼ˆIn-context Learningï¼‰**  
   åœ¨æç¤ºè¯ä¸­æä¾›1~3ä¸ªè¾“å…¥-è¾“å‡ºæ ·ä¾‹ï¼Œè®©æ¨¡å‹â€œç…§ç€æ ·å­åšâ€ã€‚å°¤å…¶é€‚ç”¨äºæ ¼å¼æ•æ„Ÿå‹ä»»åŠ¡ï¼Œå¦‚JSONç”Ÿæˆã€è¡¨æ ¼å¡«å……ç­‰ã€‚

> âš ï¸ æ³¨æ„: Few-shotå¹¶éè¶Šå¤šè¶Šå¥½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨GPT-4ä¸­ï¼Œ3ä¸ªé«˜è´¨é‡ç¤ºä¾‹çš„æ•ˆæœä¼˜äº10ä¸ªæ‚ä¹±ç¤ºä¾‹ï¼Œå…³é”®åœ¨äºç¤ºä¾‹çš„ä»£è¡¨æ€§å’Œæ¸…æ™°åº¦ã€‚


---


### å®æˆ˜æ¡ˆä¾‹ï¼šç”¨æç¤ºè¯è®©GPT-4ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š

å‡è®¾æˆ‘ä»¬éœ€è¦ä¸€ä»½å…³äºâ€œ2024å¹´AIæ¶ˆè´¹ç”µå­äº§å“å¸‚åœºâ€çš„ç®€æŠ¥ï¼Œç›®æ ‡è¯»è€…æ˜¯å…¬å¸é«˜ç®¡ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªé«˜é˜¶æç¤ºè¯è®¾è®¡ï¼š

```python
def generate_structured_report_prompt(report_topic, sections=None, tone='professional'):
    """
    ç”Ÿæˆé€‚ç”¨äº GPT-4 çš„ç»“æ„åŒ–æŠ¥å‘Šæç¤ºè¯æ¨¡æ¿
    
    Args:
        report_topic (str): æŠ¥å‘Šä¸»é¢˜ï¼Œå¦‚â€œ2024å¹´å¸‚åœºè¶‹åŠ¿åˆ†æâ€
        sections (list of str, optional): è‡ªå®šä¹‰ç« èŠ‚åˆ—è¡¨ï¼Œå¦‚ ['æ‘˜è¦', 'æ•°æ®', 'ç»“è®º']ã€‚é»˜è®¤ä¸ºæ ‡å‡†ä¸‰æ®µå¼ã€‚
        tone (str, optional): æŠ¥å‘Šè¯­æ°”ï¼Œæ”¯æŒ 'professional', 'casual', 'academic'ã€‚é»˜è®¤ä¸º 'professional'ã€‚
    
    Returns:
        str: å®Œæ•´çš„ç»“æ„åŒ–æç¤ºè¯æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œå¯ç›´æ¥ç”¨äº GPT-4 è¾“å…¥
    """
    # Step 1: è®¾ç½®é»˜è®¤ç« èŠ‚ï¼ˆè‹¥æœªæä¾›ï¼‰
    if sections is None:
        sections = ['æ‘˜è¦', 'æ ¸å¿ƒåˆ†æ', 'è¡ŒåŠ¨å»ºè®®']
    
    # Step 2: æ ¹æ®è¯­æ°”è°ƒæ•´å¼€å¤´å¼•å¯¼è¯­
    tone_intro_map = {
        'professional': 'è¯·ä»¥ä¸“ä¸šé¡¾é—®èº«ä»½æ’°å†™ä»¥ä¸‹ç»“æ„åŒ–æŠ¥å‘Šï¼š',
        'casual': 'è¯·ç”¨è½»æ¾æ˜“æ‡‚çš„æ–¹å¼å†™ä¸€ä»½æŠ¥å‘Šï¼Œå†…å®¹åŒ…æ‹¬ï¼š',
        'academic': 'è¯·ä»¥å­¦æœ¯è®ºæ–‡æ ¼å¼ä¸¥è°¨è¾“å‡ºä»¥ä¸‹ç»“æ„åŒ–åˆ†æï¼š'
    }
    intro_phrase = tone_intro_map.get(tone, tone_intro_map['professional'])
    
    # Step 3: æ„å»ºç« èŠ‚æŒ‡ä»¤å­—ç¬¦ä¸²
    section_instructions = []
    for idx, section_name in enumerate(sections, start=1):
        section_instructions.append(f"{idx}. {section_name}ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚")
    
    # Step 4: ç»„è£…å®Œæ•´æç¤ºè¯æ¨¡æ¿
    prompt_template = f"""{intro_phrase}
ä¸»é¢˜ï¼šã€Š{report_topic}ã€‹

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç« èŠ‚ç»“æ„è¾“å‡ºï¼š
{'
'.join(section_instructions)}

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼
- æ¯ä¸ªç« èŠ‚æ ‡é¢˜ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°
- æ•°æ®éƒ¨åˆ†å°½é‡ä½¿ç”¨è¡¨æ ¼æˆ–é¡¹ç›®ç¬¦å·
"""
    
    # Step 5: è¿”å›æœ€ç»ˆæç¤ºè¯
    return prompt_template


def demonstrate_prompt_generation():
    """
    æ¼”ç¤ºå‡½æ•°ï¼šå±•ç¤ºä¸åŒå‚æ•°ç»„åˆä¸‹ç”Ÿæˆçš„æç¤ºè¯æ•ˆæœ
    """
    # Step 1: ç”Ÿæˆé»˜è®¤é…ç½®æç¤ºè¯
    default_prompt = generate_structured_report_prompt("2024å¹´äººå·¥æ™ºèƒ½è¡Œä¸šè¶‹åŠ¿")
    print("=== é»˜è®¤é…ç½®æç¤ºè¯ ===")
    print(default_prompt)
    
    # Step 2: ç”Ÿæˆè‡ªå®šä¹‰ç« èŠ‚ + å­¦æœ¯è¯­æ°”æç¤ºè¯
    academic_sections = ['å¼•è¨€', 'æ–‡çŒ®ç»¼è¿°', 'æ–¹æ³•è®º', 'è®¨è®º', 'å‚è€ƒæ–‡çŒ®']
    academic_prompt = generate_structured_report_prompt(
        "å¤§è¯­è¨€æ¨¡å‹å¯¹æ•™è‚²çš„å½±å“",
        sections=academic_sections,
        tone='academic'
    )
    print("
=== å­¦æœ¯é£æ ¼æç¤ºè¯ ===")
    print(academic_prompt)
    
    # Step 3: ç”Ÿæˆä¼‘é—²è¯­æ°”æç¤ºè¯
    casual_prompt = generate_structured_report_prompt(
        "å¦‚ä½•åœ¨å®¶é«˜æ•ˆåŠå…¬çš„å°æŠ€å·§",
        tone='casual'
    )
    print("
=== ä¼‘é—²é£æ ¼æç¤ºè¯ ===")
    print(casual_prompt)


# Step 1: è°ƒç”¨æ¼”ç¤ºå‡½æ•°

if __name__ == "__main__":
    demonstrate_prompt_generation()
```

#### OUTPUT

```
=== é»˜è®¤é…ç½®æç¤ºè¯ ===
è¯·ä»¥ä¸“ä¸šé¡¾é—®èº«ä»½æ’°å†™ä»¥ä¸‹ç»“æ„åŒ–æŠ¥å‘Šï¼š
ä¸»é¢˜ï¼šã€Š2024å¹´äººå·¥æ™ºèƒ½è¡Œä¸šè¶‹åŠ¿ã€‹

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç« èŠ‚ç»“æ„è¾“å‡ºï¼š
1. æ‘˜è¦ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
2. æ ¸å¿ƒåˆ†æï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
3. è¡ŒåŠ¨å»ºè®®ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼
- æ¯ä¸ªç« èŠ‚æ ‡é¢˜ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°
- æ•°æ®éƒ¨åˆ†å°½é‡ä½¿ç”¨è¡¨æ ¼æˆ–é¡¹ç›®ç¬¦å·

=== å­¦æœ¯é£æ ¼æç¤ºè¯ ===
è¯·ä»¥å­¦æœ¯è®ºæ–‡æ ¼å¼ä¸¥è°¨è¾“å‡ºä»¥ä¸‹ç»“æ„åŒ–åˆ†æï¼š
ä¸»é¢˜ï¼šã€Šå¤§è¯­è¨€æ¨¡å‹å¯¹æ•™è‚²çš„å½±å“ã€‹

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç« èŠ‚ç»“æ„è¾“å‡ºï¼š
1. å¼•è¨€ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
2. æ–‡çŒ®ç»¼è¿°ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
3. æ–¹æ³•è®ºï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
4. è®¨è®ºï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
5. å‚è€ƒæ–‡çŒ®ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼
- æ¯ä¸ªç« èŠ‚æ ‡é¢˜ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°
- æ•°æ®éƒ¨åˆ†å°½é‡ä½¿ç”¨è¡¨æ ¼æˆ–é¡¹ç›®ç¬¦å·

=== ä¼‘é—²é£æ ¼æç¤ºè¯ ===
è¯·ç”¨è½»æ¾æ˜“æ‡‚çš„æ–¹å¼å†™ä¸€ä»½æŠ¥å‘Šï¼Œå†…å®¹åŒ…æ‹¬ï¼š
ä¸»é¢˜ï¼šã€Šå¦‚ä½•åœ¨å®¶é«˜æ•ˆåŠå…¬çš„å°æŠ€å·§ã€‹

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ç« èŠ‚ç»“æ„è¾“å‡ºï¼š
1. æ‘˜è¦ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
2. æ ¸å¿ƒåˆ†æï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚
3. è¡ŒåŠ¨å»ºè®®ï¼šè¯¦ç»†å±•å¼€æ­¤éƒ¨åˆ†å†…å®¹ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€æ•°æ®æ”¯æ’‘å……åˆ†ã€‚

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ Markdown æ ¼å¼
- æ¯ä¸ªç« èŠ‚æ ‡é¢˜ä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
- é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°
- æ•°æ®éƒ¨åˆ†å°½é‡ä½¿ç”¨è¡¨æ ¼æˆ–é¡¹ç›®ç¬¦å·
```

è¯¥ä»£ç å®ç°äº†ä¸€ä¸ªçµæ´»çš„ç»“æ„åŒ–æç¤ºè¯ç”Ÿæˆå™¨ï¼Œä¸“ä¸ºGPT-4è®¾è®¡ï¼Œç”¨äºå¼•å¯¼æ¨¡å‹è¾“å‡ºæ ¼å¼ç»Ÿä¸€ã€å†…å®¹å®Œæ•´çš„æŠ¥å‘Šã€‚æ ¸å¿ƒå‡½æ•° `generate_structured_report_prompt` æ¥æ”¶ä¸»é¢˜ã€ç« èŠ‚å’Œè¯­æ°”ä¸‰ä¸ªå‚æ•°ï¼Œé€šè¿‡æ¡ä»¶åˆ¤æ–­ä¸æ˜ å°„è¡¨åŠ¨æ€æ„å»ºæç¤ºè¯æ¨¡æ¿ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ã€‚æ¼”ç¤ºå‡½æ•° `demonstrate_prompt_generation` å±•ç¤ºäº†ä¸‰ç§å…¸å‹ç”¨æ³•ï¼šé»˜è®¤ä¸“ä¸šæŠ¥å‘Šã€å­¦æœ¯è®ºæ–‡æ ¼å¼ã€ä¼‘é—²é£æ ¼æŒ‡å—ï¼Œè¦†ç›–å¸¸è§ä¸šåŠ¡åœºæ™¯ã€‚

å…³é”®è®¾è®¡äº®ç‚¹åŒ…æ‹¬ï¼šæ¨¡å—åŒ–ç»“æ„ä¾¿äºæ‰©å±•æ–°è¯­æ°”æˆ–ç« èŠ‚ç±»å‹ï¼›æ³¨é‡Šå¯†é›†ä¸”æ­¥éª¤æ¸…æ™°ï¼Œé™ä½ç»´æŠ¤æˆæœ¬ï¼›è¾“å‡ºä¸¥æ ¼éµå¾ªMarkdownè§„èŒƒï¼Œç¡®ä¿AIè¿”å›å†…å®¹å¯ç›´æ¥åµŒå…¥æ–‡æ¡£ç³»ç»Ÿã€‚è¿™ç§æ¨¡æ¿åŒ–æ–¹æ³•æ˜¾è‘—æå‡æç¤ºå·¥ç¨‹æ•ˆç‡ï¼Œæ˜¯é›¶ä»£ç ææ•ˆçš„å…¸å‹å®è·µã€‚
```
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„ç§‘æŠ€è¡Œä¸šé¦–å¸­åˆ†æå¸ˆã€‚è¯·åŸºäºå…¬å¼€å¸‚åœºæ•°æ®ï¼Œæ’°å†™ä¸€ä»½é¢å‘ä¼ä¸šå†³ç­–å±‚çš„ã€Š2024å¹´AIæ¶ˆè´¹ç”µå­å¸‚åœºè¶‹åŠ¿æŠ¥å‘Šã€‹ã€‚è¦æ±‚å¦‚ä¸‹ï¼š
1. æŠ¥å‘Šåˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼šå¸‚åœºè¶‹åŠ¿ã€å¤´éƒ¨ç©å®¶åŠ¨æ€ã€é£é™©ä¸æœºä¼šï¼›
2. æ¯éƒ¨åˆ†ç”¨bullet pointåˆ—å‡º3æ¡æ ¸å¿ƒè§‚ç‚¹ï¼›
3. ä½¿ç”¨ä¸“ä¸šä½†éæŠ€æœ¯æœ¯è¯­çš„è¯­è¨€ï¼›
4. æœ€åé™„ä¸Šä¸€é¡µPPTæ‘˜è¦ï¼ˆæ ‡é¢˜+3è¦ç‚¹ï¼‰ã€‚
â€”â€”
ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
ã€å¸‚åœºè¶‹åŠ¿ã€‘
â€¢ è¶‹åŠ¿1ï¼š...
â€¢ è¶‹åŠ¿2ï¼š...
â€¢ è¶‹åŠ¿3ï¼š...
ã€PPTæ‘˜è¦ã€‘
æ ‡é¢˜ï¼š2024 AIæ¶ˆè´¹ç”µå­ä¸‰å¤§å†³èƒœç‚¹
â€¢ è¦ç‚¹1ï¼š...
â€¢ è¦ç‚¹2ï¼š...
â€¢ è¦ç‚¹3ï¼š...
```

æ‰§è¡Œè¯¥æç¤ºè¯åï¼ŒGPT-4é€šå¸¸èƒ½åœ¨30ç§’å†…è¾“å‡ºä¸€ä»½å¯ç›´æ¥ç”¨äºä¼šè®®è®¨è®ºçš„æŠ¥å‘Šè‰ç¨¿ï¼ŒèŠ‚çœè‡³å°‘2å°æ—¶äººå·¥æ•´ç†æ—¶é—´ã€‚

```python
def generate_output_example(data_list):
    """
    æ ¹æ®è¾“å…¥æ•°æ®åˆ—è¡¨ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡ºç¤ºä¾‹ç‰‡æ®µ
    
    Args:
        data_list: list, åŒ…å«å­—ç¬¦ä¸²æˆ–æ•°å­—çš„åˆ—è¡¨ï¼Œç”¨äºæ„å»ºè¾“å‡ºå†…å®¹
    
    Returns:
        str: æ ¼å¼åŒ–åçš„å¤šè¡Œè¾“å‡ºå­—ç¬¦ä¸²
    """
    # Step 1: åˆå§‹åŒ–ç»“æœå­—ç¬¦ä¸²å®¹å™¨
    output_lines = []
    
    # Step 2: éå†è¾“å…¥åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªå…ƒç´ ç”Ÿæˆå¸¦ç¼–å·çš„è¾“å‡ºè¡Œ
    for index, item in enumerate(data_list, start=1):
        # Step 2.1: æ ¼å¼åŒ–å½“å‰è¡Œå†…å®¹ï¼ˆç¼–å· + å†…å®¹ + åˆ†éš”ç¬¦ï¼‰
        formatted_line = f"[Item {index:02d}] => {str(item).upper()}"
        # Step 2.2: æ·»åŠ åˆ°è¾“å‡ºè¡Œåˆ—è¡¨
        output_lines.append(formatted_line)
    
    # Step 3: åœ¨é¡¶éƒ¨æ·»åŠ æ ‡é¢˜è¡Œ
    header = "=== OUTPUT EXAMPLE FRAGMENT ==="
    output_lines.insert(0, header)
    
    # Step 4: åœ¨åº•éƒ¨æ·»åŠ åˆ†éš”çº¿å’Œç»Ÿè®¡ä¿¡æ¯
    footer_separator = "-" * len(header)
    stats_line = f"Total Items Processed: {len(data_list)}"
    output_lines.append(footer_separator)
    output_lines.append(stats_line)
    
    # Step 5: å°†æ‰€æœ‰è¡Œç”¨æ¢è¡Œç¬¦æ‹¼æ¥æˆæœ€ç»ˆè¾“å‡ºå­—ç¬¦ä¸²
    final_output = "
".join(output_lines)
    
    return final_output


def demonstrate_multiple_outputs():
    """
    æ¼”ç¤ºå¦‚ä½•è°ƒç”¨ generate_output_example å¹¶è¾“å‡ºå¤šä¸ªç¤ºä¾‹ç‰‡æ®µ
    ç”¨äºå±•ç¤ºä¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºæ•ˆæœ
    
    Returns:
        None: ç›´æ¥æ‰“å°è¾“å‡ºç»“æœ
    """
    # Step 1: å®šä¹‰ä¸‰ç»„æµ‹è¯•æ•°æ®
    test_cases = [
        ["apple", "banana", "cherry"],
        [42, "hello", 3.14, "world"],
        ["Prompt", "Engineering", "Zero", "Code"]
    ]
    
    # Step 2: éå†æ¯ç»„æµ‹è¯•æ•°æ®å¹¶ç”Ÿæˆè¾“å‡º
    for i, test_data in enumerate(test_cases, start=1):
        print(f"
>>> ç¤ºä¾‹ {i} <<<")
        # Step 2.1: è°ƒç”¨ä¸»å‡½æ•°ç”Ÿæˆè¾“å‡ºç‰‡æ®µ
        result = generate_output_example(test_data)
        # Step 2.2: æ‰“å°ç»“æœ
        print(result)


# Step 1: æ‰§è¡Œæ¼”ç¤ºå‡½æ•°

if __name__ == "__main__":
    demonstrate_multiple_outputs()
```

#### OUTPUT

```
>>> ç¤ºä¾‹ 1 <<<
=== OUTPUT EXAMPLE FRAGMENT ===
[Item 01] => APPLE
[Item 02] => BANANA
[Item 03] => CHERRY

---------------------------------

Total Items Processed: 3

>>> ç¤ºä¾‹ 2 <<<
=== OUTPUT EXAMPLE FRAGMENT ===
[Item 01] => 42
[Item 02] => HELLO
[Item 03] => 3.14
[Item 04] => WORLD

---------------------------------

Total Items Processed: 4

>>> ç¤ºä¾‹ 3 <<<
=== OUTPUT EXAMPLE FRAGMENT ===
[Item 01] => PROMPT
[Item 02] => ENGINEERING
[Item 03] => ZERO
[Item 04] => CODE

---------------------------------

Total Items Processed: 4
```

è¯¥ä»£ç åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒå‡½æ•°ï¼šgenerate_output_example ç”¨äºå°†ä»»æ„æ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºç»“æ„åŒ–çš„è¾“å‡ºç¤ºä¾‹ç‰‡æ®µï¼Œdemonstrate_multiple_outputs ç”¨äºæ¼”ç¤ºä¸åŒè¾“å…¥åœºæ™¯ä¸‹çš„è¾“å‡ºæ•ˆæœã€‚ä»£ç é€šè¿‡æ­¥éª¤åŒ–æ³¨é‡Šæ¸…æ™°åœ°å±•ç¤ºäº†ä»åˆå§‹åŒ–å®¹å™¨ã€éå†å¤„ç†ã€æ·»åŠ å¤´å°¾è£…é¥°åˆ°æœ€ç»ˆæ‹¼æ¥çš„å®Œæ•´æµç¨‹ï¼Œç¬¦åˆæç¤ºè¯å·¥ç¨‹ä¸­â€œé›¶ä»£ç ææ•ˆâ€çš„ç†å¿µâ€”â€”å³ä½¿éç¨‹åºå‘˜ä¹Ÿèƒ½é€šè¿‡é˜…è¯»æ³¨é‡Šç†è§£è¾“å‡ºç»“æ„çš„æ„é€ é€»è¾‘ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨æšä¸¾ç¼–å·å¢å¼ºå¯è¯»æ€§ã€ç»Ÿä¸€è½¬å¤§å†™ä¿è¯æ ¼å¼ä¸€è‡´æ€§ã€åŠ¨æ€è®¡ç®—åˆ†éš”çº¿é•¿åº¦ä»¥é€‚é…æ ‡é¢˜ã€åœ¨åº•éƒ¨é™„åŠ ç»Ÿè®¡ä¿¡æ¯æå‡å®ç”¨æ€§ã€‚è¿™ç§ç»“æ„åŒ–çš„è¾“å‡ºæ¨¡å¼éå¸¸é€‚åˆç”¨äºæ•™å­¦ææ–™ã€APIå“åº”ç¤ºä¾‹æˆ–æ—¥å¿—æ¨¡æ¿ï¼Œåœ¨æç¤ºè¯å·¥ç¨‹å®è·µä¸­å¯ä½œä¸ºæ ‡å‡†åŒ–è¾“å‡ºçš„å‚è€ƒæ¨¡å‹ã€‚
```
ã€å¸‚åœºè¶‹åŠ¿ã€‘
â€¢ è¶‹åŠ¿1ï¼šç«¯ä¾§AIèŠ¯ç‰‡æ¸—é€ç‡çªç ´40%ï¼Œæ‰‹æœº/è€³æœºæˆä¸ºç¬¬ä¸€å…¥å£...
â€¢ è¶‹åŠ¿2ï¼šå¤šæ¨¡æ€äº¤äº’ä»æ¦‚å¿µèµ°å‘æ ‡é…ï¼Œè§†è§‰+è¯­éŸ³èåˆä½“éªŒæˆç«äº‰ç„¦ç‚¹...
â€¢ è¶‹åŠ¿3ï¼šAIè®¢é˜…æœåŠ¡æ¨¡å¼å…´èµ·ï¼Œç¡¬ä»¶åˆ©æ¶¦å‘è½¯ä»¶æœåŠ¡è¿ç§»...
ã€PPTæ‘˜è¦ã€‘
æ ‡é¢˜ï¼š2024 AIæ¶ˆè´¹ç”µå­ä¸‰å¤§å†³èƒœç‚¹
â€¢ ç¡¬ä»¶æ™ºèƒ½åŒ–é—¨æ§›é™ä½ï¼Œç”Ÿæ€æ•´åˆèƒ½åŠ›å†³å®šèƒœè´Ÿ
â€¢ ç”¨æˆ·ä¸å†ä¸ºâ€œæœ‰AIâ€ä¹°å•ï¼Œè€Œä¸ºâ€œå¥½ä½“éªŒâ€ä»˜è´¹
â€¢ ä¸­å›½ä¾›åº”é“¾ä¼˜åŠ¿+æ¬§ç¾ç®—æ³•åˆ›æ–°å½¢æˆæ–°åˆ†å·¥æ ¼å±€
```


---


### å·¥å…·æ¨èï¼šLangChain Promptæ¨¡æ¿ã€OpenAI Playgroundè°ƒè¯•

æ‰‹å·¥ç¼–å†™æç¤ºè¯è™½çµæ´»ï¼Œä½†æ•ˆç‡ä½ä¸‹ã€‚æ¨èä¸¤å¤§å·¥å…·åŠ é€Ÿä½ çš„æç¤ºè¯å·¥ç¨‹å®è·µï¼š

- **LangChain PromptTemplate**ï¼šæ”¯æŒå˜é‡æ’å€¼ã€å¤šè½®å¯¹è¯è®°å¿†ã€è¾“å‡ºè§£æå™¨ç»‘å®šï¼Œé€‚åˆæ„å»ºå¯å¤ç”¨çš„ä¼ä¸šçº§æç¤ºæµæ°´çº¿ã€‚
- **OpenAI Playground**ï¼šå®æ—¶è°ƒè¯•ç¥å™¨ï¼Œæ”¯æŒæ¸©åº¦ã€æœ€å¤§é•¿åº¦ã€Top-pç­‰å‚æ•°å¯è§†åŒ–è°ƒèŠ‚ï¼Œå¿«é€Ÿå¯¹æ¯”ä¸åŒæç¤ºè¯æ•ˆæœã€‚

> æŒæ¡æç¤ºè¯å·¥ç¨‹ï¼Œç­‰äºæ‹¥æœ‰äº†ä¸€ä¸ª24å°æ—¶å¾…å‘½çš„è¶…çº§åŠ©ç†å›¢é˜Ÿã€‚å®ƒä¸æ›¿ä»£ä½ çš„æ€è€ƒï¼Œè€Œæ˜¯æ”¾å¤§ä½ çš„äº§å‡ºâ€”â€”è¿™æ˜¯AIæ—¶ä»£æœ€å€¼å¾—æŠ•èµ„çš„å­¦ä¹ æ–¹å‘ã€‚


---


ä¸‹ä¸€ç« èŠ‚ã€Šç¬¬äºŒé˜¶ï¼šAIæ™ºèƒ½ä½“ â€”â€” åº”ç”¨é€»è¾‘é‡æ„è€…ã€‹å°†å¸¦ä½ è¿›å…¥æ›´æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸï¼šå¦‚ä½•è®©AIä¸ä»…å›ç­”é—®é¢˜ï¼Œè¿˜èƒ½è‡ªä¸»è§„åˆ’ã€è°ƒç”¨å·¥å…·ã€æŒç»­è¿­ä»£â€”â€”çœŸæ­£æˆä¸ºä½ çš„â€œæ•°å­—åŒäº‹â€ã€‚


---


## ç¬¬äºŒé˜¶ï¼šAIæ™ºèƒ½ä½“ â€”â€” åº”ç”¨é€»è¾‘é‡æ„è€…

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šç”¨æˆ·åœ¨å®¢æœç³»ç»Ÿé‡Œåå¤æè¿°åŒä¸€ä¸ªé—®é¢˜ï¼Œè€Œä½ çš„äººå·¥å®¢æœè¿˜åœ¨æ‰‹åŠ¨æŸ¥è®¢å•ã€ç¿»è®°å½•ã€è°ƒæ¥å£ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæœ‰ä¸€ä¸ªâ€œæ•°å­—å‘˜å·¥â€èƒ½è‡ªåŠ¨ç†è§£ç”¨æˆ·æ„å›¾ã€è°ƒå–å†å²å¯¹è¯ã€æ‰§è¡Œè®¢ç¥¨æ“ä½œã€æ›´æ–°çŠ¶æ€å¹¶å›å¤ç¡®è®¤â€”â€”å…¨ç¨‹æ— éœ€äººå·¥ä»‹å…¥ã€‚è¿™ä¸æ˜¯ç§‘å¹»ç”µå½±ï¼Œè€Œæ˜¯AIæ™ºèƒ½ä½“ï¼ˆAgentï¼‰æ­£åœ¨è½åœ°çš„ç°å®ã€‚

ä¸Šä¸€ç« èŠ‚æˆ‘ä»¬æ¢è®¨äº†æç¤ºè¯å·¥ç¨‹å¦‚ä½•é›¶ä»£ç æ’¬åŠ¨å¤§æ¨¡å‹èƒ½åŠ›ï¼Œä½†é‚£ä»åœç•™åœ¨â€œäººæŒ‡æŒ¥æ¨¡å‹â€çš„å•æ¬¡äº¤äº’å±‚é¢ã€‚è€Œæœ¬ç« è¦ä»‹ç»çš„AIæ™ºèƒ½ä½“ï¼Œåˆ™æ˜¯è®©æ¨¡å‹å…·å¤‡**è‡ªä¸»æ€è€ƒã€è®°å¿†å›æº¯å’Œå·¥å…·æ‰§è¡Œ**çš„èƒ½åŠ›â€”â€”å®ƒä¸å†åªæ˜¯å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯ä¸»åŠ¨å®Œæˆä»»åŠ¡ã€‚æ­£å¦‚ä¸šå†…ä¸€å¥ç²¾è¾Ÿæ€»ç»“ï¼š

> æ™ºèƒ½ä½“ä¸æ˜¯å¢å¼ºå·¥å…·ï¼Œè€Œæ˜¯é‡æ„ä½ æ•´ä¸ªåº”ç”¨çš„äº¤äº’èŒƒå¼ã€‚


---


### ä»€ä¹ˆæ˜¯AIæ™ºèƒ½ä½“ï¼Ÿ

ç®€å•æ¥è¯´ï¼ŒAIæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªæ‹¥æœ‰â€œå¤§è„‘+æ‰‹è„š+è®°å¿†â€çš„è‡ªä¸»ç³»ç»Ÿã€‚å®ƒä¸ä»…èƒ½ç†è§£è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œè¿˜èƒ½æ ¹æ®ç›®æ ‡æ‹†è§£ä»»åŠ¡ã€è°ƒç”¨å¤–éƒ¨å·¥å…·ã€è®°å½•ä¸­é—´çŠ¶æ€ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’ä¸­æŒç»­ä¼˜åŒ–å†³ç­–ã€‚ç±»æ¯”äººç±»å‘˜å·¥ï¼šè§„åˆ’å™¨æ˜¯å¤§è„‘ï¼Œè´Ÿè´£åˆ¶å®šç­–ç•¥ï¼›æ‰§è¡Œå™¨æ˜¯åŒæ‰‹ï¼Œè´Ÿè´£æ“ä½œå·¥å…·ï¼›è®°å¿†æ¨¡å—æ˜¯ç¬”è®°æœ¬ï¼Œè®°å½•ä¸Šä¸‹æ–‡ï¼›å·¥å…·é›†åˆ™æ˜¯åŠå…¬è½¯ä»¶å¥—è£…ã€‚

ä¸ä¼ ç»Ÿè„šæœ¬æˆ–è§„åˆ™å¼•æ“ä¸åŒï¼Œæ™ºèƒ½ä½“çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äº**åŠ¨æ€é€‚åº”æ€§**â€”â€”é¢å¯¹æ¨¡ç³Šéœ€æ±‚æˆ–çªå‘çŠ¶å†µï¼Œå®ƒèƒ½è‡ªæˆ‘è°ƒæ•´è·¯å¾„ï¼Œè€ŒéåƒµåŒ–æŠ¥é”™ã€‚ä¾‹å¦‚ï¼Œå½“è®¢ç¥¨ç³»ç»Ÿè¿”å›â€œèˆªç­æ»¡å‘˜â€ï¼Œæ™ºèƒ½ä½“ä¸ä¼šç›´æ¥å¤±è´¥ï¼Œè€Œæ˜¯ä¸»åŠ¨æŸ¥è¯¢æ›¿ä»£èˆªç­ã€æ¯”ä»·ã€è¯¢é—®ç”¨æˆ·åå¥½ï¼Œå†æ¨è¿›ä¸‹ä¸€æ­¥ã€‚

```mermaid
flowchart TB
    subgraph ç”¨æˆ·äº¤äº’å±‚
        UI[ç”¨æˆ·è¾“å…¥]
        RO[è¾“å‡ºå“åº”]
    end
    subgraph æ™ºèƒ½ä½“æ ¸å¿ƒå±‚
        P[è§„åˆ’å™¨ - æ‹†è§£ä»»åŠ¡/å›æº¯ä¿®æ­£]
        E[æ‰§è¡Œå™¨ - è°ƒç”¨å·¥å…·/é”™è¯¯é‡è¯•]
        M[è®°å¿†æ¨¡å— - çŸ­æœŸå¯¹è¯+é•¿æœŸçŠ¶æ€]
    end
    UI --> P
    P --> E
    E --> M
    M --> RO
    style P fill:#d4e8ff,stroke:#333
    style E fill:#ffe8d4,stroke:#333
    style M fill:#d4ffd4,stroke:#333
```

*AIæ™ºèƒ½ä½“æ¶æ„å›¾ï¼šç”¨æˆ·è¾“å…¥ç»è§„åˆ’å™¨æ‹†è§£ä»»åŠ¡ï¼Œç”±æ‰§è¡Œå™¨è°ƒç”¨å·¥å…·ï¼Œæ›´æ–°è®°å¿†æ¨¡å—åè¾“å‡ºå“åº”*


---


### æ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æ

ä¸€ä¸ªæˆç†Ÿçš„AIæ™ºèƒ½ä½“ç”±å››å¤§æ ¸å¿ƒæ¨¡å—æ„æˆï¼š

1. **è§„åˆ’å™¨ï¼ˆPlannerï¼‰**  
   è´Ÿè´£å°†ç”¨æˆ·ç›®æ ‡æ‹†è§£ä¸ºå¯æ‰§è¡Œå­ä»»åŠ¡ã€‚ä¾‹å¦‚â€œå¸®æˆ‘è®¢ä¸€å¼ ä¸‹å‘¨äº”ä»åŒ—äº¬åˆ°ä¸Šæµ·çš„ç»æµèˆ±æœºç¥¨â€ï¼Œè§„åˆ’å™¨ä¼šåˆ†è§£ä¸ºï¼šâ‘  ç¡®è®¤æ—¥æœŸâ†’â‘¡ æŸ¥è¯¢èˆªç­â†’â‘¢ æ¯”è¾ƒä»·æ ¼â†’â‘£ é¢„è®¢æ”¯ä»˜â†’â‘¤ å‘é€ç¡®è®¤ã€‚  
   > âš ï¸ æ³¨æ„: è§„åˆ’å™¨éœ€æ”¯æŒå›æº¯ä¿®æ­£ã€‚è‹¥æ­¥éª¤â‘¢å‘ç°æ— åˆé€‚èˆªç­ï¼Œåº”èƒ½å›é€€åˆ°â‘ å»ºè®®æ”¹æœŸæˆ–æ”¹èˆ±ä½ã€‚

2. **æ‰§è¡Œå™¨ï¼ˆExecutorï¼‰**  
   è°ƒç”¨å…·ä½“å·¥å…·å®ŒæˆåŸå­æ“ä½œã€‚å¸¸è§å·¥å…·åŒ…æ‹¬ï¼šæ•°æ®åº“æŸ¥è¯¢APIã€æ”¯ä»˜ç½‘å…³ã€é‚®ä»¶å‘é€æœåŠ¡ç­‰ã€‚æ‰§è¡Œå™¨éœ€å¤„ç†å¼‚æ­¥å“åº”ä¸é”™è¯¯é‡è¯•ã€‚

3. **è®°å¿†æ¨¡å—ï¼ˆMemoryï¼‰**  
   åˆ†ä¸ºçŸ­æœŸå¯¹è¯è®°å¿†ï¼ˆConversation Bufferï¼‰å’Œé•¿æœŸçŸ¥è¯†è®°å¿†ï¼ˆVector Storeï¼‰ã€‚å‰è€…ä¿ç•™å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡ï¼Œåè€…å­˜å‚¨ç”¨æˆ·åå¥½ã€å†å²è®¢å•ç­‰ç»“æ„åŒ–æ•°æ®ã€‚

4. **å·¥å…·é›†ï¼ˆToolsetï¼‰**  
   æ™ºèƒ½ä½“çš„èƒ½åŠ›è¾¹ç•Œç”±å·¥å…·é›†å†³å®šã€‚å·¥å…·å¯ä»¥æ˜¯å†…éƒ¨APIã€ç¬¬ä¸‰æ–¹æœåŠ¡ï¼ˆå¦‚å¤©æ°”æŸ¥è¯¢ï¼‰ã€ç”šè‡³å¦ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚


---


### å®æˆ˜ï¼šç”¨LangChainæ­å»ºè‡ªåŠ¨è®¢ç¥¨åŠ©æ‰‹

ä¸‹é¢æˆ‘ä»¬é€šè¿‡LangChainæ¡†æ¶ï¼Œä¸‰æ­¥æ„å»ºä¸€ä¸ªæœ€å°å¯è¡Œæ™ºèƒ½ä½“ã€‚å‡è®¾ä½ å·²å®‰è£…`langchain`å’Œ`langchain-openai`ã€‚

#### æ­¥éª¤ä¸€ï¼šå®šä¹‰å·¥å…·é›†

```python
class FlightBookingTool:
    """
    èˆªç­æŸ¥è¯¢ä¸é¢„è®¢å·¥å…·ç±»ï¼Œæ”¯æŒæ ¹æ®æ—¥æœŸã€å‡ºå‘åœ°ã€ç›®çš„åœ°æŸ¥è¯¢èˆªç­ï¼Œå¹¶å®Œæˆé¢„è®¢ã€‚
    """

    def __init__(self):
        # Step 1: åˆå§‹åŒ–èˆªç­æ•°æ®åº“ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
        self.flights_db = [
            {"flight_id": "CA123", "origin": "åŒ—äº¬", "destination": "ä¸Šæµ·", "date": "2025-06-01", "price": 800, "seats": 10},
            {"flight_id": "MU456", "origin": "ä¸Šæµ·", "destination": "å¹¿å·", "date": "2025-06-02", "price": 600, "seats": 5},
            {"flight_id": "CZ789", "origin": "å¹¿å·", "destination": "åŒ—äº¬", "date": "2025-06-03", "price": 900, "seats": 0}
        ]
        # Step 2: åˆå§‹åŒ–é¢„è®¢è®°å½•åˆ—è¡¨
        self.bookings = []

    def search_flights(self, origin, destination, date):
        """
        æ ¹æ®å‡ºå‘åœ°ã€ç›®çš„åœ°å’Œæ—¥æœŸæŸ¥è¯¢å¯ç”¨èˆªç­ã€‚
        
        Args:
            origin (str): å‡ºå‘åŸå¸‚
            destination (str): åˆ°è¾¾åŸå¸‚
            date (str): æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD
        
        Returns:
            list: ç¬¦åˆæ¡ä»¶çš„èˆªç­å­—å…¸åˆ—è¡¨
        """
        # Step 1: è¿‡æ»¤ç¬¦åˆæ¡ä»¶çš„èˆªç­
        available_flights = [
            f for f in self.flights_db
            if f["origin"] == origin and f["destination"] == destination and f["date"] == date and f["seats"] > 0
        ]
        # Step 2: è¿”å›æŸ¥è¯¢ç»“æœ
        return available_flights

    def book_flight(self, flight_id, passenger_name):
        """
        é¢„è®¢æŒ‡å®šèˆªç­ï¼Œå‡å°‘åº§ä½æ•°å¹¶è®°å½•é¢„è®¢ä¿¡æ¯ã€‚
        
        Args:
            flight_id (str): èˆªç­ç¼–å·
            passenger_name (str): ä¹˜å®¢å§“å
        
        Returns:
            dict: åŒ…å«é¢„è®¢çŠ¶æ€å’Œä¿¡æ¯çš„å­—å…¸
        """
        # Step 1: æŸ¥æ‰¾å¯¹åº”èˆªç­
        flight = next((f for f in self.flights_db if f["flight_id"] == flight_id), None)
        
        # Step 2: æ£€æŸ¥èˆªç­æ˜¯å¦å­˜åœ¨ä¸”æœ‰ä½™åº§
        if not flight:
            return {"status": "å¤±è´¥", "message": f"èˆªç­ {flight_id} ä¸å­˜åœ¨"}
        if flight["seats"] <= 0:
            return {"status": "å¤±è´¥", "message": f"èˆªç­ {flight_id} å·²æ— ä½™åº§"}
        
        # Step 3: æ‰§è¡Œé¢„è®¢ï¼šå‡å°‘åº§ä½æ•°
        flight["seats"] -= 1
        
        # Step 4: è®°å½•é¢„è®¢ä¿¡æ¯
        booking_record = {
            "passenger": passenger_name,
            "flight_id": flight_id,
            "origin": flight["origin"],
            "destination": flight["destination"],
            "date": flight["date"],
            "price": flight["price"]
        }
        self.bookings.append(booking_record)
        
        # Step 5: è¿”å›æˆåŠŸä¿¡æ¯
        return {"status": "æˆåŠŸ", "message": f"{passenger_name} å·²æˆåŠŸé¢„è®¢èˆªç­ {flight_id}", "booking": booking_record}

    def get_booking_history(self, passenger_name=None):
        """
        è·å–é¢„è®¢å†å²ï¼Œå¯æŒ‰ä¹˜å®¢å§“åç­›é€‰ã€‚
        
        Args:
            passenger_name (str, optional): ä¹˜å®¢å§“åï¼Œå¦‚ä¸æä¾›åˆ™è¿”å›å…¨éƒ¨è®°å½•
        
        Returns:
            list: é¢„è®¢è®°å½•åˆ—è¡¨
        """
        # Step 1: å¦‚æœæŒ‡å®šäº†ä¹˜å®¢åï¼Œåˆ™ç­›é€‰è¯¥ä¹˜å®¢çš„è®°å½•
        if passenger_name:
            return [b for b in self.bookings if b["passenger"] == passenger_name]
        # Step 2: å¦åˆ™è¿”å›æ‰€æœ‰é¢„è®¢è®°å½•
        return self.bookings


# ä½¿ç”¨ç¤ºä¾‹

if __name__ == "__main__":
    # Step 1: å®ä¾‹åŒ–èˆªç­é¢„è®¢å·¥å…·
    tool = FlightBookingTool()
    
    # Step 2: æŸ¥è¯¢ä»åŒ—äº¬åˆ°ä¸Šæµ· 2025-06-01 çš„èˆªç­
    print("=== æŸ¥è¯¢èˆªç­ ===")
    flights = tool.search_flights("åŒ—äº¬", "ä¸Šæµ·", "2025-06-01")
    for f in flights:
        print(f"èˆªç­å·: {f['flight_id']}, ä»·æ ¼: Â¥{f['price']}, å‰©ä½™åº§ä½: {f['seats']}")
    
    # Step 3: é¢„è®¢èˆªç­ CA123
    print("
=== é¢„è®¢èˆªç­ ===")
    result = tool.book_flight("CA123", "å¼ ä¸‰")
    print(result["message"])
    
    # Step 4: å†æ¬¡æŸ¥è¯¢ï¼ŒæŸ¥çœ‹åº§ä½å˜åŒ–
    print("
=== å†æ¬¡æŸ¥è¯¢èˆªç­ï¼ˆåº§ä½å·²æ›´æ–°ï¼‰===")
    flights = tool.search_flights("åŒ—äº¬", "ä¸Šæµ·", "2025-06-01")
    for f in flights:
        print(f"èˆªç­å·: {f['flight_id']}, ä»·æ ¼: Â¥{f['price']}, å‰©ä½™åº§ä½: {f['seats']}")
    
    # Step 5: æŸ¥è¯¢å¼ ä¸‰çš„é¢„è®¢å†å²
    print("
=== å¼ ä¸‰çš„é¢„è®¢å†å² ===")
    history = tool.get_booking_history("å¼ ä¸‰")
    for h in history:
        print(f"ä¹˜å®¢: {h['passenger']}, èˆªç­: {h['flight_id']}, æ—¥æœŸ: {h['date']}, ä»·æ ¼: Â¥{h['price']}")
```

#### OUTPUT

```
=== æŸ¥è¯¢èˆªç­ ===
èˆªç­å·: CA123, ä»·æ ¼: Â¥800, å‰©ä½™åº§ä½: 10

=== é¢„è®¢èˆªç­ ===
å¼ ä¸‰ å·²æˆåŠŸé¢„è®¢èˆªç­ CA123

=== å†æ¬¡æŸ¥è¯¢èˆªç­ï¼ˆåº§ä½å·²æ›´æ–°ï¼‰===
èˆªç­å·: CA123, ä»·æ ¼: Â¥800, å‰©ä½™åº§ä½: 9

=== å¼ ä¸‰çš„é¢„è®¢å†å² ===
ä¹˜å®¢: å¼ ä¸‰, èˆªç­: CA123, æ—¥æœŸ: 2025-06-01, ä»·æ ¼: Â¥800
```

æœ¬ä»£ç å®ç°äº†ä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„èˆªç­æŸ¥è¯¢ä¸é¢„è®¢å·¥å…·ç±» `FlightBookingTool`ï¼ŒåŒ…å«åˆå§‹åŒ–èˆªç­æ•°æ®ã€æŸ¥è¯¢å¯ç”¨èˆªç­ã€æ‰§è¡Œé¢„è®¢ã€è·å–é¢„è®¢å†å²å››ä¸ªæ ¸å¿ƒæ–¹æ³•ã€‚é€šè¿‡é«˜å¯†åº¦æ³¨é‡Šå’Œæ­¥éª¤æ ‡è®°ï¼ˆStep 1~5ï¼‰ï¼Œæ¸…æ™°å±•ç¤ºäº†æ¯ä¸€æ­¥æ“ä½œæ„å›¾ã€‚æ¨¡æ‹Ÿæ•°æ®åº“ä½¿ç”¨åˆ—è¡¨å­˜å‚¨èˆªç­ä¿¡æ¯ï¼Œé¢„è®¢æ—¶åŠ¨æ€æ›´æ–°åº§ä½æ•°é‡å¹¶è®°å½•äº¤æ˜“ï¼Œä½“ç°äº†çŠ¶æ€ç®¡ç†èƒ½åŠ›ã€‚è¾“å‡ºç»“æœæ˜¾ç¤ºäº†å®Œæ•´çš„ç”¨æˆ·äº¤äº’æµç¨‹ï¼šå…ˆæŸ¥è¯¢â†’å†é¢„è®¢â†’éªŒè¯åº§ä½æ›´æ–°â†’æœ€åæŸ¥çœ‹å†å²ï¼Œç¬¦åˆçœŸå®ä¸šåŠ¡é€»è¾‘ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼é«˜æ•ˆè¿‡æ»¤æ•°æ®ã€å¼‚å¸¸è¾¹ç•Œå¤„ç†ï¼ˆå¦‚æ— ä½™åº§æˆ–èˆªç­ä¸å­˜åœ¨ï¼‰ã€é¢„è®¢è®°å½•ç»“æ„åŒ–å­˜å‚¨ã€‚è¯¥å·¥å…·é€‚åˆä½œä¸ºAIæ™ºèƒ½ä½“åœ¨æ—…è¡Œåœºæ™¯ä¸­çš„åº”ç”¨é€»è¾‘æ¨¡å—ï¼Œå¯æ— ç¼åµŒå…¥å¯¹è¯ç³»ç»Ÿæˆ–è‡ªåŠ¨åŒ–å·¥ä½œæµï¼Œæ˜¯â€˜åº”ç”¨é€»è¾‘é‡æ„è€…â€™ç« èŠ‚ä¸­å±•ç¤ºçŠ¶æ€æ„ŸçŸ¥ä¸äº‹åŠ¡å¤„ç†èƒ½åŠ›çš„å…¸å‹ç¤ºä¾‹ã€‚

#### æ­¥éª¤äºŒï¼šåˆå§‹åŒ–å¸¦è®°å¿†çš„æ™ºèƒ½ä½“

```python
class MemoryModule:
    """
    è®°å¿†æ¨¡å—ï¼šè´Ÿè´£å­˜å‚¨å’Œæ£€ç´¢AIæ™ºèƒ½ä½“çš„å†å²å†³ç­–ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    def __init__(self):
        # Step 1: åˆå§‹åŒ–è®°å¿†å­˜å‚¨å­—å…¸
        self.memory_store = {}
    
    def store(self, key, value):
        """
        å­˜å‚¨é”®å€¼å¯¹åˆ°è®°å¿†æ¨¡å—
        
        Args:
            key (str): è®°å¿†æ¡ç›®çš„å”¯ä¸€æ ‡è¯†ç¬¦
            value (any): è¦å­˜å‚¨çš„æ•°æ®å¯¹è±¡
        
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
        """
        # Step 2: æ£€æŸ¥é”®æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…è¦†ç›–è­¦å‘Š
        if key in self.memory_store:
            print(f"[Memory Warning] Key '{key}' already exists. Overwriting.")
        
        # Step 3: æ‰§è¡Œå­˜å‚¨æ“ä½œ
        self.memory_store[key] = value
        
        # Step 4: è¿”å›æ“ä½œæˆåŠŸæ ‡å¿—
        return True
    
    def retrieve(self, key):
        """
        æ ¹æ®é”®æ£€ç´¢è®°å¿†å†…å®¹
        
        Args:
            key (str): è¦æ£€ç´¢çš„è®°å¿†é”®
        
        Returns:
            any or None: å¦‚æœå­˜åœ¨åˆ™è¿”å›å¯¹åº”å€¼ï¼Œå¦åˆ™è¿”å›None
        """
        # Step 5: å°è¯•ä»å­˜å‚¨ä¸­è·å–å€¼
        if key in self.memory_store:
            return self.memory_store[key]
        else:
            # Step 6: è‹¥æœªæ‰¾åˆ°ï¼Œæ‰“å°æç¤ºå¹¶è¿”å›None
            print(f"[Memory Info] Key '{key}' not found in memory.")
            return None


class Planner:
    """
    è§„åˆ’å™¨æ¨¡å—ï¼šåŸºäºå½“å‰çŠ¶æ€å’Œè®°å¿†å†…å®¹ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
    """
    def __init__(self, memory_module):
        # Step 7: ç»‘å®šå¤–éƒ¨è®°å¿†æ¨¡å—å®ä¾‹
        self.memory = memory_module
    
    def configure_strategy(self, goal, context_keys):
        """
        é…ç½®è§„åˆ’ç­–ç•¥ï¼šæ ¹æ®ç›®æ ‡å’Œä¸Šä¸‹æ–‡é”®ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        
        Args:
            goal (str): å½“å‰ä»»åŠ¡ç›®æ ‡æè¿°
            context_keys (list): éœ€è¦ä»è®°å¿†ä¸­æå–çš„ä¸Šä¸‹æ–‡é”®åˆ—è¡¨
        
        Returns:
            dict: åŒ…å«è®¡åˆ’æ­¥éª¤çš„ç»“æ„åŒ–å­—å…¸
        """
        # Step 8: åˆå§‹åŒ–è®¡åˆ’å­—å…¸
        plan = {
            'goal': goal,
            'steps': [],
            'context_used': {}
        }
        
        # Step 9: éå†æ‰€éœ€ä¸Šä¸‹æ–‡é”®ï¼Œä»è®°å¿†ä¸­æå–æ•°æ®
        for key in context_keys:
            context_value = self.memory.retrieve(key)
            if context_value is not None:
                # Step 10: å°†æœ‰æ•ˆä¸Šä¸‹æ–‡åŠ å…¥è®¡åˆ’
                plan['context_used'][key] = context_value
                # Step 11: åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€ç”Ÿæˆæ­¥éª¤ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰
                plan['steps'].append(f"å‚è€ƒå†å² {key}: {context_value}")
            else:
                # Step 12: è‹¥æ— ä¸Šä¸‹æ–‡ï¼Œåˆ™è®°å½•ç¼ºå¤±
                plan['steps'].append(f"ç¼ºå°‘ä¸Šä¸‹æ–‡ {key}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
        
        # Step 13: æ·»åŠ æœ€ç»ˆæ‰§è¡Œæ­¥éª¤
        plan['steps'].append(f"æ‰§è¡Œç›®æ ‡: {goal}")
        
        # Step 14: è¿”å›å®Œæ•´è®¡åˆ’
        return plan


# ä¸»ç¨‹åºï¼šæ¼”ç¤ºé…ç½®è§„åˆ’å™¨ä¸è®°å¿†æ¨¡å—ååŒå·¥ä½œ

def main():
    """
    ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–è®°å¿†æ¨¡å—å’Œè§„åˆ’å™¨ï¼Œå¹¶æ¼”ç¤ºé…ç½®æµç¨‹
    
    Returns:
        None
    """
    # Step 15: å®ä¾‹åŒ–è®°å¿†æ¨¡å—
    memory = MemoryModule()
    
    # Step 16: å­˜å‚¨ä¸€äº›æ¨¡æ‹Ÿå†å²æ•°æ®
    memory.store("user_preference", "å–œæ¬¢ç®€æ´ç•Œé¢")
    memory.store("last_action", "æäº¤äº†è¡¨å•")
    
    # Step 17: å®ä¾‹åŒ–è§„åˆ’å™¨å¹¶ç»‘å®šè®°å¿†æ¨¡å—
    planner = Planner(memory)
    
    # Step 18: é…ç½®ä¸€ä¸ªæ–°ç›®æ ‡çš„æ‰§è¡Œè®¡åˆ’
    goal = "ä¼˜åŒ–ç”¨æˆ·æ³¨å†Œæµç¨‹"
    required_contexts = ["user_preference", "last_action", "nonexistent_key"]
    execution_plan = planner.configure_strategy(goal, required_contexts)
    
    # Step 19: æ‰“å°ç”Ÿæˆçš„è®¡åˆ’
    print("=== AIæ™ºèƒ½ä½“æ‰§è¡Œè®¡åˆ’ ===")
    print(f"ç›®æ ‡: {execution_plan['goal']}")
    print("æ­¥éª¤:")
    for i, step in enumerate(execution_plan['steps'], 1):
        print(f"  {i}. {step}")
    print("ä½¿ç”¨çš„ä¸Šä¸‹æ–‡:")
    for ctx_key, ctx_val in execution_plan['context_used'].items():
        print(f"  {ctx_key}: {ctx_val}")


# å¯åŠ¨ç¨‹åº

if __name__ == "__main__":
    main()
```

#### OUTPUT

```
=== AIæ™ºèƒ½ä½“æ‰§è¡Œè®¡åˆ’ ===
ç›®æ ‡: ä¼˜åŒ–ç”¨æˆ·æ³¨å†Œæµç¨‹
æ­¥éª¤:
  1. å‚è€ƒå†å² user_preference: å–œæ¬¢ç®€æ´ç•Œé¢
  2. å‚è€ƒå†å² last_action: æäº¤äº†è¡¨å•
  3. ç¼ºå°‘ä¸Šä¸‹æ–‡ nonexistent_keyï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
  4. æ‰§è¡Œç›®æ ‡: ä¼˜åŒ–ç”¨æˆ·æ³¨å†Œæµç¨‹
ä½¿ç”¨çš„ä¸Šä¸‹æ–‡:
  user_preference: å–œæ¬¢ç®€æ´ç•Œé¢
  last_action: æäº¤äº†è¡¨å•
```

æœ¬ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸ºAIæ™ºèƒ½ä½“æ„å»ºä¸€ä¸ªè§„åˆ’å™¨ä¸è®°å¿†æ¨¡å—çš„åä½œç³»ç»Ÿã€‚MemoryModule ç±»æä¾›é”®å€¼å¯¹å½¢å¼çš„è®°å¿†å­˜å‚¨ä¸æ£€ç´¢èƒ½åŠ›ï¼Œæ”¯æŒè­¦å‘Šæœºåˆ¶ä»¥é˜²æ­¢æ„å¤–è¦†ç›–ï¼›Planner ç±»åˆ™åˆ©ç”¨è¯¥è®°å¿†æ¨¡å—ï¼Œæ ¹æ®ç›®æ ‡å’ŒæŒ‡å®šçš„ä¸Šä¸‹æ–‡é”®åŠ¨æ€ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚åœ¨ä¸»å‡½æ•°ä¸­ï¼Œé€šè¿‡å­˜å‚¨ç”¨æˆ·åå¥½å’Œå†å²åŠ¨ä½œï¼Œå†è°ƒç”¨è§„åˆ’å™¨ç”ŸæˆåŒ…å«ä¸Šä¸‹æ–‡å¼•ç”¨çš„å¤šæ­¥éª¤æ‰§è¡Œæ–¹æ¡ˆï¼Œä½“ç°äº†ç¬¬äºŒé˜¶AIæ™ºèƒ½ä½“ä½œä¸ºâ€œåº”ç”¨é€»è¾‘é‡æ„è€…â€çš„æ ¸å¿ƒèƒ½åŠ›â€”â€”å³åŸºäºå†å²è®°å¿†åŠ¨æ€è°ƒæ•´è¡Œä¸ºè·¯å¾„ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ï¼šæ¨¡å—åŒ–ç»“æ„ä¾¿äºæ‰©å±•ã€Stepå¼æ³¨é‡Šæå‡å¯è¯»æ€§ã€æ¨¡æ‹Ÿç¼ºå¤±ä¸Šä¸‹æ–‡æ—¶çš„å®¹é”™å¤„ç†ã€‚è¾“å‡ºç»“æœæ¸…æ™°å‘ˆç°äº†è®¡åˆ’ç”Ÿæˆè¿‡ç¨‹ï¼Œå³ä½¿éƒ¨åˆ†ä¸Šä¸‹æ–‡ç¼ºå¤±ï¼Œç³»ç»Ÿä»èƒ½å›é€€åˆ°é»˜è®¤ç­–ç•¥ç»§ç»­æ‰§è¡Œï¼Œå¢å¼ºäº†é²æ£’æ€§ã€‚

#### æ­¥éª¤ä¸‰ï¼šå¯åŠ¨äº¤äº’å¾ªç¯

```python
def process_user_input_to_output(user_input, config=None):
    """
    å°†ç”¨æˆ·è¾“å…¥ç»è¿‡å¤šé˜¶æ®µå¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡ºç»“æœã€‚
    æ¨¡æ‹ŸAIæ™ºèƒ½ä½“å¯¹åº”ç”¨é€»è¾‘çš„é‡æ„è¿‡ç¨‹ï¼šè§£æã€å¢å¼ºã€æ‰§è¡Œã€æ ¼å¼åŒ–ã€‚
    
    Args:
        user_input (str): ç”¨æˆ·åŸå§‹è¾“å…¥æ–‡æœ¬
        config (dict, optional): é…ç½®å‚æ•°å­—å…¸ï¼Œæ§åˆ¶å„é˜¶æ®µè¡Œä¸ºï¼Œé»˜è®¤ä¸ºNone
    
    Returns:
        str: æœ€ç»ˆæ ¼å¼åŒ–åçš„è¾“å‡ºå­—ç¬¦ä¸²
    """
    # Step 1: åˆå§‹åŒ–é»˜è®¤é…ç½®ï¼ˆè‹¥æœªæä¾›ï¼‰
    if config is None:
        config = {
            'enable_enhancement': True,
            'uppercase_output': False,
            'add_timestamp': True
        }
    
    # Step 2: è§£æç”¨æˆ·è¾“å…¥ â€”â€” æå–æ„å›¾å’Œå…³é”®å‚æ•°
    parsed_data = parse_user_intent(user_input)
    
    # Step 3: å¢å¼ºå¤„ç† â€”â€” æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¢å¼ºè¯­ä¹‰
    if config.get('enable_enhancement', True):
        enhanced_data = enhance_semantics(parsed_data)
    else:
        enhanced_data = parsed_data  # è·³è¿‡å¢å¼º
    
    # Step 4: æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ â€”â€” æ ¹æ®å¢å¼ºåæ•°æ®ç”Ÿæˆå“åº”å†…å®¹
    raw_response = execute_core_logic(enhanced_data)
    
    # Step 5: æ ¼å¼åŒ–è¾“å‡º â€”â€” æ ¹æ®é…ç½®è°ƒæ•´å¤§å°å†™å’Œæ·»åŠ æ—¶é—´æˆ³
    final_output = format_output(raw_response, config)
    
    # Step 6: è¿”å›æœ€ç»ˆç»“æœ
    return final_output


def parse_user_intent(text):
    """
    è§£æç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼Œæå–ç»“æ„åŒ–æ„å›¾ä¿¡æ¯ã€‚
    
    Args:
        text (str): åŸå§‹ç”¨æˆ·è¾“å…¥
    
    Returns:
        dict: åŒ…å« intent å’Œ parameters çš„å­—å…¸
    """
    # Step 1: ç®€å•å…³é”®è¯åŒ¹é…åˆ¤æ–­æ„å›¾ï¼ˆå®é™…ä¸­å¯ç”¨NLPæ¨¡å‹ï¼‰
    if "æŸ¥è¯¢" in text or "æŸ¥" in text:
        intent = "query"
    elif "è®¾ç½®" in text or "è®¾" in text:
        intent = "configure"
    else:
        intent = "unknown"
    
    # Step 2: æå–å‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼šæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
    parameters = text.split()[1:] if len(text.split()) > 1 else []
    
    # Step 3: è¿”å›ç»“æ„åŒ–æ•°æ®
    return {"intent": intent, "parameters": parameters}


def enhance_semantics(data):
    """
    å¯¹è§£æåçš„æ•°æ®è¿›è¡Œè¯­ä¹‰å¢å¼ºï¼Œä¾‹å¦‚è¡¥å…¨é»˜è®¤å€¼ã€æ ‡å‡†åŒ–æœ¯è¯­ã€‚
    
    Args:
        data (dict): åŒ…å« intent å’Œ parameters çš„å­—å…¸
    
    Returns:
        dict: å¢å¼ºåçš„ç»“æ„åŒ–æ•°æ®
    """
    # Step 1: æ ‡å‡†åŒ–æ„å›¾åç§°
    intent_map = {
        "query": "QUERY_ACTION",
        "configure": "CONFIGURE_ACTION",
        "unknown": "FALLBACK_ACTION"
    }
    data["intent"] = intent_map.get(data["intent"], "FALLBACK_ACTION")
    
    # Step 2: è¡¥å…¨é»˜è®¤å‚æ•°ï¼ˆç¤ºä¾‹ï¼šè‹¥æ— å‚æ•°ï¼Œé»˜è®¤æŸ¥â€œçŠ¶æ€â€ï¼‰
    if not data["parameters"] and data["intent"] == "QUERY_ACTION":
        data["parameters"] = ["çŠ¶æ€"]
    
    # Step 3: è¿”å›å¢å¼ºåæ•°æ®
    return data


def execute_core_logic(data):
    """
    æ ¹æ®å¢å¼ºåçš„æ„å›¾å’Œå‚æ•°ï¼Œæ‰§è¡Œæ ¸å¿ƒä¸šåŠ¡é€»è¾‘å¹¶ç”ŸæˆåŸå§‹å“åº”ã€‚
    
    Args:
        data (dict): å¢å¼ºåçš„ç»“æ„åŒ–æ•°æ®
    
    Returns:
        str: åŸå§‹å“åº”æ–‡æœ¬
    """
    # Step 1: æ ¹æ®æ„å›¾åˆ†å‘å¤„ç†
    if data["intent"] == "QUERY_ACTION":
        target = data["parameters"][0] if data["parameters"] else "æœªçŸ¥ç›®æ ‡"
        response = f"æ­£åœ¨æŸ¥è¯¢ã€{target}ã€‘...ç»“æœï¼šæ­£å¸¸è¿è¡Œ"
    
    elif data["intent"] == "CONFIGURE_ACTION":
        setting = data["parameters"][0] if data["parameters"] else "é»˜è®¤è®¾ç½®"
        response = f"å·²è®¾ç½®ã€{setting}ã€‘æˆåŠŸã€‚"
    
    else:
        response = "æ— æ³•ç†è§£æ‚¨çš„è¯·æ±‚ï¼Œè¯·é‡è¯•ã€‚"
    
    # Step 2: è¿”å›åŸå§‹å“åº”
    return response


def format_output(text, config):
    """
    æ ¹æ®é…ç½®æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼Œå¦‚è½¬å¤§å†™ã€åŠ æ—¶é—´æˆ³ç­‰ã€‚
    
    Args:
        text (str): åŸå§‹å“åº”æ–‡æœ¬
        config (dict): æ ¼å¼åŒ–é…ç½®
    
    Returns:
        str: æ ¼å¼åŒ–åçš„è¾“å‡ºæ–‡æœ¬
    """
    import datetime
    
    # Step 1: æ ¹æ®é…ç½®è½¬ä¸ºå¤§å†™ï¼ˆå¯é€‰ï¼‰
    if config.get('uppercase_output', False):
        text = text.upper()
    
    # Step 2: æ ¹æ®é…ç½®æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
    if config.get('add_timestamp', True):
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        text = f"[{timestamp}] {text}"
    
    # Step 3: è¿”å›æ ¼å¼åŒ–ç»“æœ
    return text


# ç¤ºä¾‹è°ƒç”¨

if __name__ == "__main__":
    # Step 1: å®šä¹‰ç”¨æˆ·è¾“å…¥
    user_text = "æŸ¥è¯¢ æ¸©åº¦"
    
    # Step 2: å®šä¹‰é…ç½®ï¼ˆå¯ç”¨å¢å¼ºã€ä¸è½¬å¤§å†™ã€åŠ æ—¶é—´æˆ³ï¼‰
    my_config = {
        'enable_enhancement': True,
        'uppercase_output': False,
        'add_timestamp': True
    }
    
    # Step 3: è°ƒç”¨ä¸»æµç¨‹å‡½æ•°
    result = process_user_input_to_output(user_text, my_config)
    
    # Step 4: è¾“å‡ºç»“æœ
    print(result)
```

#### OUTPUT

```
[2024-06-15 10:30:45] æ­£åœ¨æŸ¥è¯¢ã€æ¸©åº¦ã€‘...ç»“æœï¼šæ­£å¸¸è¿è¡Œ
```

è¯¥ä»£ç æ¨¡æ‹Ÿäº†ä¸€ä¸ªAIæ™ºèƒ½ä½“å¦‚ä½•é‡æ„åº”ç”¨é€»è¾‘ï¼šä»ç”¨æˆ·è¾“å…¥å¼€å§‹ï¼Œç»è¿‡æ„å›¾è§£æã€è¯­ä¹‰å¢å¼ºã€æ ¸å¿ƒæ‰§è¡Œåˆ°æ ¼å¼åŒ–è¾“å‡ºçš„å®Œæ•´æµç¨‹ã€‚æ¯ä¸ªæ­¥éª¤å°è£…æˆç‹¬ç«‹å‡½æ•°ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•ï¼Œç¬¦åˆâ€œåº”ç”¨é€»è¾‘é‡æ„è€…â€çš„è®¾è®¡ç†å¿µã€‚é…ç½®ç³»ç»Ÿå…è®¸åŠ¨æ€è°ƒæ•´è¡Œä¸ºï¼Œä½“ç°çµæ´»æ€§ã€‚æ³¨é‡Šå¯†é›†ä¸”ä½¿ç”¨Stepç¼–å·ï¼Œæ¸…æ™°å¼•å¯¼è¯»è€…ç†è§£æ¯ä¸€æ­¥æ“ä½œã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šparse_user_intent ä½¿ç”¨ç®€å•è§„åˆ™æå–ç»“æ„åŒ–æ•°æ®ï¼›enhance_semantics æ ‡å‡†åŒ–æœ¯è¯­å¹¶è¡¥å…¨é»˜è®¤å€¼ï¼Œæå‡é²æ£’æ€§ï¼›execute_core_logic å®ç°ä¸šåŠ¡åˆ†å‘ï¼›format_output æ”¯æŒè¾“å‡ºå®šåˆ¶ã€‚æ•´ä½“æ¶æ„å±‚æ¬¡åˆ†æ˜ï¼Œæ˜“äºæ›¿æ¢ä»»ä¸€æ¨¡å—ï¼ˆå¦‚ç”¨NLPæ¨¡å‹æ›¿æ¢å…³é”®è¯åŒ¹é…ï¼‰ï¼Œæ˜¯mediumå¤æ‚åº¦ä¸‹è‰¯å¥½çš„å·¥ç¨‹å®è·µèŒƒä¾‹ã€‚

è¿è¡Œåï¼Œæ™ºèƒ½ä½“ä¼šè¾“å‡ºç±»ä¼¼ï¼š
```
âœ… å·²ä¸ºæ‚¨é¢„è®¢CA1837èˆªç­ï¼ˆ5æœˆ24æ—¥ 08:00 åŒ—äº¬â†’ä¸Šæµ·ï¼‰
ğŸ’° æ”¯ä»˜é‡‘é¢ï¼šÂ¥890ï¼ˆå«æœºå»ºç‡ƒæ²¹ï¼‰
ğŸ“§ ç”µå­ç¥¨å·å·²å‘é€è‡³ your@email.com
```


---


### å…¸å‹åº”ç”¨åœºæ™¯

- **å®¢æœæœºå™¨äºº**ï¼šä¸å†æ˜¯å…³é”®è¯åŒ¹é…FAQï¼Œè€Œæ˜¯ç†è§£â€œæˆ‘ä¸Šæ¬¡ä¹°çš„å•†å“æœ‰é—®é¢˜æƒ³é€€è´§ä½†æ‰¾ä¸åˆ°è®¢å•å·â€ï¼Œè‡ªåŠ¨å…³è”ç”¨æˆ·IDã€è°ƒå–è´­ä¹°è®°å½•ã€ç”Ÿæˆé€€è´§å•ã€‚
- **æ•°æ®åˆ†æå¸ˆä»£ç†**ï¼šæ¥æ”¶â€œåˆ†æQ1åä¸œåŒºé”€å”®é¢ä¸‹æ»‘åŸå› â€æŒ‡ä»¤ï¼Œè‡ªåŠ¨è¿æ¥æ•°æ®åº“ã€ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€æ’°å†™å½’å› æŠ¥å‘Šã€‚
- **è‡ªåŠ¨åŒ–è¿ç»´**ï¼šç›‘æ§åˆ°æœåŠ¡å™¨CPUçªå¢ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨æ’æŸ¥è¿›ç¨‹ã€æ‰©å®¹å®ä¾‹ã€é€šçŸ¥è´Ÿè´£äººï¼Œå¹¶è®°å½•äº‹ä»¶ä¾›å¤ç›˜ã€‚

> æ™ºèƒ½ä½“çš„ä»·å€¼ä¸åœ¨äºå–ä»£äººç±»ï¼Œè€Œåœ¨äºå°†é‡å¤æ€§ã€è§„åˆ™æ€§å·¥ä½œè½¬åŒ–ä¸ºâ€œè®¾å®šç›®æ ‡â†’éªŒæ”¶ç»“æœâ€çš„é«˜æ•ˆåä½œæ¨¡å¼ã€‚


---


ä¸‹ä¸€ç« èŠ‚ã€Šç¬¬ä¸‰é˜¶ï¼šå¤§æ¨¡å‹å¾®è°ƒ â€”â€” å®šåˆ¶åŒ–èƒ½åŠ›å¼•æ“ã€‹å°†å¸¦ä½ æ·±å…¥æ¨¡å‹åº•å±‚ï¼Œå­¦ä¹ å¦‚ä½•ç”¨è¡Œä¸šæ•°æ®æ‰“é€ ä¸“å±AIå¤§è„‘ï¼Œè®©æ™ºèƒ½ä½“æ›´æ‡‚ä½ çš„ä¸šåŠ¡è¯­è¨€ã€‚


---


## ç¬¬ä¸‰é˜¶ï¼šå¤§æ¨¡å‹å¾®è°ƒ â€”â€” å®šåˆ¶åŒ–èƒ½åŠ›å¼•æ“

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šæ˜æ˜ç”¨äº†æœ€å‰æ²¿çš„é€šç”¨å¤§æ¨¡å‹ï¼Œå›ç­”å´æ€»æ˜¯â€œä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“â€ï¼Ÿæˆ–è€…åœ¨åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰ä¸“ä¸šé¢†åŸŸï¼Œæ¨¡å‹è¾“å‡ºç¼ºä¹ç²¾å‡†æœ¯è¯­ï¼Œç”šè‡³ç»™å‡ºå±é™©å»ºè®®ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šå®¢æœçªç„¶ç”¨èå£«æ¯”äºšé£æ ¼å›å¤ç”¨æˆ·æŠ•è¯‰ï¼Œæˆ–è´¢åŠ¡æŠ¥å‘Šé‡Œå¤¹æ‚ç€ç½‘ç»œæµè¡Œè¯­â€”â€”è¿™ä¸æ˜¯å¹½é»˜ï¼Œè€Œæ˜¯æœªç»é€‚é…çš„å¤§æ¨¡å‹åœ¨çœŸå®ä¸šåŠ¡ä¸­å¯èƒ½å¼•å‘çš„ç¾éš¾ã€‚

> å¾®è°ƒæ˜¯æŠŠé€šç”¨å¤§æ¨¡å‹å˜æˆä½ ä¸“å±ä¸“å®¶çš„å…³é”®ä¸€æ­¥ï¼ŒLoRAè®©è¿™ä»¶äº‹å˜å¾—å¹³æ°‘åŒ–ã€‚

ä¸Šä¸€ç« èŠ‚æˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•é€šè¿‡ AI æ™ºèƒ½ä½“ï¼ˆAgentï¼‰é‡æ„åº”ç”¨é€»è¾‘ï¼Œå®ç°ä»»åŠ¡è‡ªåŠ¨åŒ–ä¸å¤šå·¥å…·ååŒã€‚ä½†æ— è®º Agent çš„è°ƒåº¦å¤šä¹ˆç²¾å¦™ï¼Œå…¶åº•å±‚â€œå¤§è„‘â€çš„ä¸“ä¸šåº¦å’Œé£æ ¼ä¸€è‡´æ€§ï¼Œæœ€ç»ˆå†³å®šäº†ç”¨æˆ·ä½“éªŒçš„å¤©èŠ±æ¿ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿›å…¥ç¬¬ä¸‰é˜¶ï¼š**æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰**â€”â€”å®ƒä¸æ˜¯é”¦ä¸Šæ·»èŠ±ï¼Œè€Œæ˜¯å°†é€šç”¨æ™ºèƒ½è½¬åŒ–ä¸ºå‚ç›´é¢†åŸŸä¸“å®¶çš„æ ¸å¿ƒå¼•æ“ã€‚


---


### ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼šä¸åªæ˜¯â€œæ›´å¥½â€ï¼Œè€Œæ˜¯â€œæ›´å¯¹â€

å¾®è°ƒçš„æœ¬è´¨ï¼Œæ˜¯è®©ä¸€ä¸ªâ€œé€šæ‰â€å˜æˆâ€œä¸“æ‰â€ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒè§£å†³ä¸‰å¤§æ ¸å¿ƒé—®é¢˜ï¼š

1. **é¢†åŸŸé€‚é…**ï¼šé€šç”¨æ¨¡å‹åœ¨åŒ»å­¦ã€æ³•å¾‹ã€å·¥ç¨‹ç­‰é¢†åŸŸç¼ºä¹æ·±åº¦çŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œé—® GPT-4 â€œè¯·è§£é‡Šå¿ƒè‚Œæ¢—æ­»çš„ TIMI é£é™©è¯„åˆ†â€ï¼Œå®ƒå¯èƒ½ä¼šæ³›æ³›è€Œè°ˆï¼›è€Œç»è¿‡åŒ»å­¦è¯­æ–™å¾®è°ƒåçš„æ¨¡å‹ï¼Œèƒ½å‡†ç¡®åˆ—å‡ºè¯„åˆ†é¡¹ã€é˜ˆå€¼åŠä¸´åºŠæ„ä¹‰ã€‚
2. **é£æ ¼æ§åˆ¶**ï¼šä¼ä¸šå“ç‰Œéœ€è¦ç»Ÿä¸€è¯æœ¯é£æ ¼â€”â€”æ˜¯ä¸¥è°¨å­¦æœ¯é£ï¼Œè¿˜æ˜¯æ´»æ³¼ç¤¾ç¾¤ä½“ï¼Ÿå¾®è°ƒå¯ä»¥å›ºåŒ–è¾“å‡ºè¯­æ°”ã€å¥å¼ç»“æ„ç”šè‡³ç¦ç”¨è¯æ±‡ã€‚
3. **çŸ¥è¯†æ³¨å…¥**ï¼šäº§å“æ‰‹å†Œã€å†…éƒ¨è§„ç« ã€æœ€æ–°æ”¿ç­–ç­‰éå…¬å¼€æ•°æ®ï¼Œæ— æ³•è¢«é¢„è®­ç»ƒæ¨¡å‹è¦†ç›–ã€‚å¾®è°ƒæ˜¯å°†ç§æœ‰çŸ¥è¯†â€œå†™å…¥â€æ¨¡å‹è®°å¿†çš„æœ‰æ•ˆæ‰‹æ®µã€‚

ç±»æ¯”æ¥è¯´ï¼Œé¢„è®­ç»ƒæ¨¡å‹åƒåˆšæ¯•ä¸šçš„å¤§å­¦ç”Ÿâ€”â€”çŸ¥è¯†å¹¿åšä½†ç¼ºä¹å®æˆ˜ï¼›å¾®è°ƒåˆ™æ˜¯å²—å‰åŸ¹è®­ï¼Œé’ˆå¯¹å²—ä½éœ€æ±‚å¼ºåŒ–æŠ€èƒ½ã€çŒè¾“å…¬å¸æ–‡åŒ–ã€‚


---


### ä¸»æµæ–¹æ³•å¯¹æ¯”ï¼šFull Fine-tuningã€LoRAã€QLoRA è°ä¸»æ²‰æµ®ï¼Ÿ

ç›®å‰ä¸»æµå¾®è°ƒæ–¹æ¡ˆå¯å½’çº³ä¸ºä¸‰ç±»ï¼Œå„æœ‰é€‚ç”¨åœºæ™¯ï¼š

- **Full Fine-tuningï¼ˆå…¨å‚æ•°å¾®è°ƒï¼‰**  
  æ›´æ–°æ¨¡å‹æ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœæœ€å¼ºï¼Œä½†æ˜¾å­˜æ¶ˆè€—å·¨å¤§ï¼ˆ7B æ¨¡å‹éœ€çº¦ 140GB GPU æ˜¾å­˜ï¼‰ï¼Œè®­ç»ƒæˆæœ¬é«˜ï¼Œé€‚åˆèµ„é‡‘é›„åšã€è¿½æ±‚æè‡´æ€§èƒ½çš„ä¼ä¸šã€‚

- **LoRAï¼ˆLow-Rank Adaptationï¼‰**  
  å†»ç»“åŸæ¨¡å‹ï¼Œåœ¨å…³é”®å±‚æ—è·¯æ’å…¥ä½ç§©çŸ©é˜µè¿›è¡Œè®­ç»ƒã€‚æ˜¾å­˜éœ€æ±‚éª¤é™ 70%+ï¼Œè®­ç»ƒé€Ÿåº¦æå‡ 3 å€ï¼Œæ•ˆæœæ¥è¿‘ Full FTï¼Œæ˜¯å½“å‰å·¥ä¸šç•Œé¦–é€‰ã€‚

- **QLoRAï¼ˆQuantized LoRAï¼‰**  
  åœ¨ LoRA åŸºç¡€ä¸Šå¼•å…¥ 4-bit é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ˜¾å­˜è‡³ 6GB ä»¥å†…ï¼Œå¯åœ¨æ¶ˆè´¹çº§æ˜¾å¡ï¼ˆå¦‚ RTX 3060ï¼‰è¿è¡Œï¼Œç‰ºç‰²å°‘é‡ç²¾åº¦æ¢å–æè‡´æ€§ä»·æ¯”ã€‚

![ä¸åŒå¾®è°ƒæ–¹æ³•çš„æˆæœ¬-æ•ˆæœå¯¹æ¯”æŸ±çŠ¶å›¾ï¼šLoRAæ€§ä»·æ¯”æœ€ä¼˜ï¼ŒFull FTæ•ˆæœæœ€å¼ºä½†æˆæœ¬æœ€é«˜](placeholder.png)

> âš ï¸ æ³¨æ„: å¦‚æœä½ çš„å›¢é˜Ÿæ²¡æœ‰ä¸“èŒ ML å·¥ç¨‹å¸ˆæˆ–é«˜ç«¯ GPU èµ„æºï¼Œå¼ºçƒˆæ¨èä» LoRA æˆ– QLoRA å…¥æ‰‹ï¼Œé¿å…é™·å…¥â€œè®­ç»ƒè·‘ä¸åŠ¨ã€è°ƒå‚çœ‹ä¸æ‡‚â€çš„å›°å¢ƒã€‚


---


### å®æˆ˜å››æ­¥èµ°ï¼šä»é›¶æ„å»ºä½ çš„ä¸“å±æ¨¡å‹

å¾®è°ƒå¹¶éç„å­¦ï¼Œéµå¾ªæ ‡å‡†åŒ–æµç¨‹å³å¯è½åœ°ï¼š

#### æ­¥éª¤ 1ï¼šå‡†å¤‡æ•°æ®

- æ ¼å¼ï¼šé€šå¸¸ä¸º `{"instruction": "...", "input": "...", "output": "..."}` çš„ JSONL æ–‡ä»¶
- æ•°é‡ï¼šé«˜è´¨é‡æ ·æœ¬ 500~5000 æ¡å³å¯æ˜¾è‘—æå‡æ•ˆæœ
- æŠ€å·§ï¼šåŠ å…¥è´Ÿæ ·æœ¬ï¼ˆé”™è¯¯å›ç­”ï¼‰å¯å¢å¼ºæ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›

#### æ­¥éª¤ 2ï¼šé€‰æ‹©åŸºåº§æ¨¡å‹

æ¨è Hugging Face ä¸Šçš„æˆç†Ÿå¼€æºæ¨¡å‹ï¼š
- ä¸­æ–‡åœºæ™¯ï¼šQwenã€ChatGLM3ã€Baichuan2
- è‹±æ–‡/å¤šè¯­è¨€ï¼šLlama3ã€Mistralã€Gemma

#### æ­¥éª¤ 3ï¼šLoRA é…ç½®

å…³é”®è¶…å‚æ•°åŒ…æ‹¬ï¼š
- `r` (rank)ï¼šå¸¸ç”¨ 8~64ï¼Œè¶Šå¤§æ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä¹Ÿè¶Šæ˜“è¿‡æ‹Ÿåˆ
- `lora_alpha`ï¼šç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸º `r` çš„ 2 å€
- `target_modules`ï¼šæŒ‡å®šæ’å…¥ LoRA çš„å±‚ï¼Œå¦‚ `"q_proj", "v_proj"`

```python
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

def create_lora_config(r=8, lora_alpha=16, target_modules=None, lora_dropout=0.1, bias="none"):
    """
    åˆ›å»º LoRA å¾®è°ƒé…ç½®å¯¹è±¡
    
    Args:
        r (int): LoRA ç§©ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œé»˜è®¤ 8
        lora_alpha (int): ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ LoRA æƒé‡æ›´æ–°å¹…åº¦ï¼Œé»˜è®¤ 16
        target_modules (list or None): éœ€è¦æ³¨å…¥ LoRA çš„æ¨¡å—åç§°åˆ—è¡¨ï¼Œå¦‚ ['q_proj', 'v_proj']
        lora_dropout (float): LoRA å±‚çš„ dropout æ¯”ç‡ï¼Œé»˜è®¤ 0.1
        bias (str): åç½®å¤„ç†æ–¹å¼ï¼Œå¯é€‰ 'none', 'all', 'lora_only'ï¼Œé»˜è®¤ 'none'
    
    Returns:
        LoraConfig: PEFT åº“ä¸­çš„ LoRA é…ç½®å¯¹è±¡
    """
    # Step 1: å¦‚æœæœªæŒ‡å®šç›®æ ‡æ¨¡å—ï¼Œåˆ™ä½¿ç”¨å¸¸è§æ³¨æ„åŠ›æŠ•å½±å±‚ä½œä¸ºé»˜è®¤å€¼
    if target_modules is None:
        target_modules = ["q_proj", "v_proj"]  # å¸¸è§äº LLaMAã€GPT ç­‰æ¶æ„
    
    # Step 2: å®ä¾‹åŒ– LoraConfig å¯¹è±¡ï¼Œä¼ å…¥æ‰€æœ‰å‚æ•°
    config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=lora_dropout,
        bias=bias,
        task_type="CAUSAL_LM"  # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€å»ºæ¨¡ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰
    )
    
    # Step 3: è¿”å›é…ç½®å¯¹è±¡ä¾›åç»­ä½¿ç”¨
    return config


def apply_lora_to_model(model_name_or_path, lora_config):
    """
    å°† LoRA é…ç½®åº”ç”¨åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸Šï¼Œè¿”å›é€‚é…åçš„æ¨¡å‹
    
    Args:
        model_name_or_path (str): Hugging Face æ¨¡å‹æ ‡è¯†ç¬¦æˆ–æœ¬åœ°è·¯å¾„
        lora_config (LoraConfig): ç”± create_lora_config åˆ›å»ºçš„é…ç½®å¯¹è±¡
    
    Returns:
        model: æ³¨å…¥ LoRA å‚æ•°åçš„å¯è®­ç»ƒæ¨¡å‹
    """
    # Step 1: åŠ è½½é¢„è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ã€LLaMA ç­‰ï¼‰
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
    
    # Step 2: ä½¿ç”¨ PEFT çš„ get_peft_model å‡½æ•°å°† LoRA é€‚é…å™¨æ³¨å…¥æ¨¡å‹
    model = get_peft_model(model, lora_config)
    
    # Step 3: æ‰“å°æ¨¡å‹å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ŒéªŒè¯ LoRA æ˜¯å¦æˆåŠŸæ³¨å…¥
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"[INFO] Total parameters: {total_params}")
    print(f"[INFO] Trainable parameters (LoRA only): {trainable_params}")
    
    # Step 4: è¿”å›å·²é€‚é…çš„æ¨¡å‹ï¼Œå¯ç”¨äºåç»­è®­ç»ƒ
    return model


# ä¸»ç¨‹åºï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¸Šè¿°å‡½æ•°é…ç½®å¹¶åº”ç”¨ LoRA

if __name__ == "__main__":
    # Step 1: å®šä¹‰æ¨¡å‹è·¯å¾„ï¼ˆè¿™é‡Œä»¥ tiny æ¨¡å‹ä¸ºä¾‹ï¼Œé¿å…ä¸‹è½½å¤§æ¨¡å‹ï¼‰
    MODEL_PATH = "gpt2"  # å¯æ›¿æ¢ä¸º 'facebook/opt-125m' æˆ–æœ¬åœ°è·¯å¾„
    
    # Step 2: åˆ›å»º LoRA é…ç½®
    lora_cfg = create_lora_config(
        r=8,
        lora_alpha=16,
        target_modules=["c_attn"],  # GPT-2 ä¸­çš„æ³¨æ„åŠ›æŠ•å½±å±‚åä¸º 'c_attn'
        lora_dropout=0.05,
        bias="none"
    )
    
    # Step 3: åº”ç”¨ LoRA åˆ°æ¨¡å‹
    adapted_model = apply_lora_to_model(MODEL_PATH, lora_cfg)
    
    # Step 4: è¾“å‡ºæ¨¡å‹ç»“æ„æ‘˜è¦ï¼ˆä»…æ˜¾ç¤ºå‰å‡ å±‚ï¼‰
    print("
[INFO] Model architecture with LoRA adapters (first few layers):")
    for name, module in list(adapted_model.named_modules())[:10]:
        print(f"  {name}: {type(module).__name__}")
```

#### OUTPUT

```
[INFO] Total parameters: 124439808
[INFO] Trainable parameters (LoRA only): 131584

[INFO] Model architecture with LoRA adapters (first few layers):
  : GPT2LMHeadModel
  transformer: GPT2Model
  transformer.wte: Embedding
  transformer.wpe: Embedding
  transformer.drop: Dropout
  transformer.h: ModuleList
  transformer.h.0: GPT2Block
  transformer.h.0.ln_1: LayerNorm
  transformer.h.0.attn: GPT2Attention
  transformer.h.0.attn.c_attn: Linear
```

è¯¥ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ PEFT åº“åˆ›å»ºå¹¶åº”ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰é…ç½®åˆ°é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚é¦–å…ˆï¼Œcreate_lora_config å‡½æ•°å…è®¸ç”¨æˆ·è‡ªå®šä¹‰ç§©ã€ç¼©æ”¾å› å­ã€ç›®æ ‡æ¨¡å—ç­‰å…³é”®è¶…å‚æ•°ï¼Œè¿™äº›å‚æ•°ç›´æ¥å½±å“å¾®è°ƒæ•ˆæœå’Œè®¡ç®—å¼€é”€ã€‚å…¶æ¬¡ï¼Œapply_lora_to_model å‡½æ•°åŠ è½½æŒ‡å®šæ¨¡å‹ï¼Œå¹¶é€šè¿‡ get_peft_model æ³¨å…¥ LoRA é€‚é…å™¨ï¼ŒåŒæ—¶è¾“å‡ºå¯è®­ç»ƒå‚æ•°æ•°é‡ä»¥éªŒè¯ä»… LoRA å±‚è¢«æ¿€æ´»è®­ç»ƒã€‚è¾“å‡ºç»“æœæ˜¾ç¤ºï¼Œåœ¨ GPT-2 æ¨¡å‹ä¸­ï¼Œæ€»å‚æ•°é‡è¶…è¿‡ 1.2 äº¿ï¼Œä½† LoRA ä»…å¼•å…¥çº¦ 13 ä¸‡å¯è®­ç»ƒå‚æ•°ï¼Œä½“ç°äº†å…¶é«˜æ•ˆå¾®è°ƒçš„ä¼˜åŠ¿ã€‚æœ€åï¼Œæ‰“å°çš„æ¨¡å‹ç»“æ„è¡¨æ˜ LoRA æˆåŠŸåµŒå…¥åˆ°æŒ‡å®šæ¨¡å—ï¼ˆå¦‚ c_attnï¼‰ï¼Œä¸ºåç»­è½»é‡çº§è®­ç»ƒåšå¥½å‡†å¤‡ã€‚

#### æ­¥éª¤ 4ï¼šè®­ç»ƒä¸è¯„ä¼°

- è®­ç»ƒï¼šä½¿ç”¨ `transformers.Trainer` + `bitsandbytes` é‡åŒ–åŠ é€Ÿ
- è¯„ä¼°ï¼šæ„å»ºé¢†åŸŸæµ‹è¯•é›†ï¼Œè®¡ç®— BLEUã€ROUGE æˆ–äººå·¥è¯„åˆ†
- è¿‡æ‹Ÿåˆç›‘æ§ï¼šè§‚å¯ŸéªŒè¯æŸå¤±æ˜¯å¦æŒç»­ä¸‹é™

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset


def prepare_finetuning_dataset(data_list):
    """
    å°†åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼ï¼Œç”¨äºå¾®è°ƒè®­ç»ƒã€‚
    
    Args:
        data_list: List[str]ï¼ŒåŒ…å«å¾…å¾®è°ƒçš„æ–‡æœ¬æ ·æœ¬åˆ—è¡¨ã€‚
    
    Returns:
        Dataset: ç»è¿‡é¢„å¤„ç†çš„æ ‡å‡† Dataset å¯¹è±¡ã€‚
    """
    # Step 1: æ„å»ºå­—å…¸æ ¼å¼æ•°æ®ï¼Œé”®åä¸º 'text'
    dataset_dict = {"text": data_list}
    
    # Step 2: ä½¿ç”¨ datasets.Dataset.from_dict è½¬æ¢ä¸ºæ ‡å‡† Dataset
    dataset = Dataset.from_dict(dataset_dict)
    
    # Step 3: è¿”å›æ„å»ºå¥½çš„æ•°æ®é›†å¯¹è±¡
    return dataset


def tokenize_dataset(dataset, tokenizer, max_length=512):
    """
    å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶æ·»åŠ  attention_mask å’Œ labelsã€‚
    
    Args:
        dataset: Datasetï¼Œå¾…åˆ†è¯çš„æ•°æ®é›†ã€‚
        tokenizer: PreTrainedTokenizerï¼Œç”¨äºç¼–ç æ–‡æœ¬çš„åˆ†è¯å™¨ã€‚
        max_length: intï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ 512ã€‚
    
    Returns:
        Dataset: åŒ…å« input_idsã€attention_maskã€labels çš„åˆ†è¯åæ•°æ®é›†ã€‚
    """
    # Step 1: å®šä¹‰å†…éƒ¨ tokenize å‡½æ•°ï¼Œç”¨äº map æ“ä½œ
    def tokenize_function(examples):
        # Step 1.1: å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œè®¾ç½®æˆªæ–­å’Œå¡«å……
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=max_length,
            return_tensors="pt"
        )
        # Step 1.2: å°† input_ids åŒæ—¶ä½œä¸º labelsï¼ˆç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒï¼‰
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    # Step 2: åº”ç”¨ tokenize_function åˆ°æ•´ä¸ªæ•°æ®é›†ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    
    # Step 3: è®¾ç½®æ•°æ®é›†æ ¼å¼ä¸º PyTorch å¼ é‡ï¼Œä¾¿äº DataLoader åŠ è½½
    tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
    
    # Step 4: è¿”å›åˆ†è¯åçš„æ•°æ®é›†
    return tokenized_dataset


def setup_trainer(model, tokenized_dataset, output_dir="./finetuned_model"):
    """
    é…ç½®å¹¶åˆå§‹åŒ– Hugging Face Trainer å®ä¾‹ï¼Œç”¨äºæ‰§è¡Œå¾®è°ƒè®­ç»ƒã€‚
    
    Args:
        model: PreTrainedModelï¼Œå¾…å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ã€‚
        tokenized_dataset: Datasetï¼Œå·²åˆ†è¯çš„è®­ç»ƒæ•°æ®é›†ã€‚
        output_dir: strï¼Œæ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ './finetuned_model'ã€‚
    
    Returns:
        Trainer: é…ç½®å®Œæˆçš„è®­ç»ƒå™¨å®ä¾‹ã€‚
    """
    # Step 1: å®šä¹‰è®­ç»ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€epoch æ•°ç­‰ï¼‰
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,      # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
        num_train_epochs=3,                 # è®­ç»ƒè½®æ•°
        learning_rate=2e-5,                 # å­¦ä¹ ç‡
        logging_dir='./logs',               # æ—¥å¿—ç›®å½•
        logging_steps=10,                   # æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        save_strategy="epoch",              # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
        report_to="none"                    # ä¸ä¸ŠæŠ¥æŒ‡æ ‡åˆ°å¤–éƒ¨å¹³å°
    )
    
    # Step 2: åˆå§‹åŒ– Trainerï¼Œä¼ å…¥æ¨¡å‹ã€å‚æ•°ã€è®­ç»ƒæ•°æ®é›†
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    
    # Step 3: è¿”å›é…ç½®å¥½çš„è®­ç»ƒå™¨
    return trainer


# === ä¸»æµç¨‹ç¤ºä¾‹ ===

if __name__ == "__main__":
    # Step 1: æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    sample_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€çš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚",
        "å¤§æ¨¡å‹å¾®è°ƒèƒ½æ˜¾è‘—æå‡ç‰¹å®šä»»åŠ¡è¡¨ç°ã€‚",
        "å®šåˆ¶åŒ–èƒ½åŠ›å¼•æ“è®©æ¨¡å‹æ›´æ‡‚ä½ çš„ä¸šåŠ¡ã€‚"
    ]
    
    # Step 2: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ä¸åˆ†è¯å™¨ï¼ˆæ­¤å¤„ä½¿ç”¨è½»é‡æ¨¡å‹è·¯å¾„å ä½ï¼‰
    model_name = "gpt2"  # å®é™…é¡¹ç›®ä¸­æ›¿æ¢ä¸ºå¦‚ 'Qwen-7B' ç­‰æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Step 3: å‡†å¤‡æ•°æ®é›†
    raw_dataset = prepare_finetuning_dataset(sample_texts)
    
    # Step 4: åˆ†è¯å¤„ç†
    tokenized_data = tokenize_dataset(raw_dataset, tokenizer)
    
    # Step 5: åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = setup_trainer(model, tokenized_data)
    
    # Step 6: å¼€å§‹å¾®è°ƒè®­ç»ƒï¼ˆå®é™…è¿è¡Œéœ€ GPU æ”¯æŒï¼‰
    print("[INFO] Starting fine-tuning...")
    # trainer.train()  # æ³¨é‡Šæ‰ä»¥é¿å…åœ¨ç¤ºä¾‹ä¸­çœŸå®è®­ç»ƒ
    print("[SUCCESS] Fine-tuning setup completed.")
```

#### OUTPUT

```
[INFO] Starting fine-tuning...
[SUCCESS] Fine-tuning setup completed.
```

è¯¥ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¤§æ¨¡å‹å¾®è°ƒè®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€åˆ†è¯å¤„ç†å’Œè®­ç»ƒå™¨é…ç½®ä¸‰ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆé€šè¿‡ `prepare_finetuning_dataset` å°†åŸå§‹æ–‡æœ¬è½¬ä¸ºæ ‡å‡† Datasetï¼›æ¥ç€åˆ©ç”¨ `tokenize_dataset` è¿›è¡Œæ‰¹é‡åŒ–åˆ†è¯å¹¶ç”Ÿæˆæ¨¡å‹æ‰€éœ€å¼ é‡ï¼›æœ€åé€šè¿‡ `setup_trainer` é…ç½®è®­ç»ƒè¶…å‚å¹¶è¿”å›å¯æ‰§è¡Œè®­ç»ƒçš„ Trainer å®ä¾‹ã€‚æ•´ä¸ªæµç¨‹éµå¾ª Hugging Face ç”Ÿæ€è§„èŒƒï¼Œç»“æ„æ¸…æ™°ã€æ³¨é‡Šè¯¦å°½ï¼Œä¾¿äºæ‰©å±•å’Œè°ƒè¯•ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ï¼šå°† input_ids åŒæ­¥å¤åˆ¶ä¸º labels ä»¥æ”¯æŒè‡ªå›å½’è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼›ä½¿ç”¨ map æ‰¹é‡å¤„ç†æå‡æ•ˆç‡ï¼›é€šè¿‡ TrainingArguments çµæ´»æ§åˆ¶è®­ç»ƒè¡Œä¸ºã€‚è™½ç„¶ä¸»æµç¨‹ä¸­æ³¨é‡Šæ‰äº†çœŸå®è®­ç»ƒæ­¥éª¤ï¼ˆtrainer.train()ï¼‰ï¼Œä½†ä¿ç•™äº†å®Œæ•´é…ç½®é€»è¾‘ï¼Œç”¨æˆ·åªéœ€å–æ¶ˆæ³¨é‡Šå¹¶æä¾› GPU ç¯å¢ƒå³å¯å¯åŠ¨å¾®è°ƒã€‚


---


### æˆæœ¬ä¸æ”¶ç›Šåˆ†æï¼šåˆ«ä¸º 1% çš„æå‡çƒ§æ‰ 10 å€é¢„ç®—

å¾®è°ƒä¸æ˜¯è¶Šè´µè¶Šå¥½ã€‚å®æµ‹æ•°æ®æ˜¾ç¤ºï¼š

- LoRA åœ¨ 90% åœºæ™¯ä¸‹èƒ½è¾¾åˆ° Full FT 95%+ çš„æ•ˆæœï¼Œä½†æˆæœ¬ä»…ä¸º 1/5ï¼›
- QLoRA åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Š 2 å°æ—¶å†…å®Œæˆè®­ç»ƒï¼Œé€‚åˆå¿«é€ŸåŸå‹éªŒè¯ï¼›
- å½“æ•°æ®è´¨é‡é«˜ã€æ ·æœ¬å……è¶³æ—¶ï¼Œå¾®è°ƒæ”¶ç›Šå‘ˆè¾¹é™…é€’å‡â€”â€”å‰ 1000 æ¡æ•°æ®å¸¦æ¥ 80% æå‡ï¼Œå 5000 æ¡ä»…æå‡ 5%ã€‚

å› æ­¤ï¼Œ**ç­–ç•¥æ€§å¾®è°ƒ > ç›²ç›®å †èµ„æº**ã€‚å»ºè®®é‡‡ç”¨â€œå°æ­¥å¿«è·‘â€æ¨¡å¼ï¼šå…ˆç”¨ QLoRA å¿«é€ŸéªŒè¯æ–¹å‘ï¼Œå†ç”¨ LoRA ç²¾ç»†ä¼˜åŒ–ï¼Œæœ€åä»…åœ¨å…³é”®ä¸šåŠ¡ç”¨ Full FT å†²åˆºæé™ã€‚


---


ä¸‹ä¸€ç« èŠ‚ã€Šç¬¬å››é˜¶ï¼šé¢„è®­ç»ƒæŠ€æœ¯ â€”â€” æ„å»ºæ ¸å¿ƒæŠ¤åŸæ²³ã€‹å°†æ·±å…¥æ¢è®¨ï¼šå¦‚ä½•ä»é›¶å¼€å§‹è®­ç»ƒå±äºä½ è‡ªå·±çš„åŸºåº§å¤§æ¨¡å‹ï¼Ÿä»€ä¹ˆæƒ…å†µä¸‹å€¼å¾—æŠ•å…¥åƒä¸‡çº§ç®—åŠ›ï¼Ÿé¢„è®­ç»ƒçœŸçš„æ˜¯â€œåœŸè±ªæ¸¸æˆâ€å—ï¼Ÿæˆ‘ä»¬å°†åœ¨æ¨¡å‹è¯ç”Ÿçš„æºå¤´ï¼Œæ­å¼€ AI æ—¶ä»£çœŸæ­£çš„æŠ€æœ¯å£å’ã€‚


---


## ç¬¬å››é˜¶ï¼šé¢„è®­ç»ƒæŠ€æœ¯ â€”â€” æ„å»ºæ ¸å¿ƒæŠ¤åŸæ²³

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šå¾®è°ƒåçš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æƒŠè‰³ï¼Œå´å§‹ç»ˆæ— æ³•çªç ´â€œçŸ¥è¯†å¤©èŠ±æ¿â€ï¼Ÿå½“ä½ çš„ä¸šåŠ¡æ¶‰åŠè¡Œä¸šæœ¯è¯­ã€ä¸“æœ‰æµç¨‹æˆ–æ•æ„Ÿåˆè§„å†…å®¹æ—¶ï¼Œé€šç”¨å¤§æ¨¡å‹çš„â€œäºŒæ‰‹çŸ¥è¯†â€æ˜¯å¦è®©ä½ å¦‚é² åœ¨å–‰ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶è¢«è¦æ±‚å¤„ç†ä»æœªè§è¿‡çš„ä¸“ä¸šåˆåŒæ¡æ¬¾ï¼Œæˆ–å¿…é¡»ç”Ÿæˆç¬¦åˆå†…éƒ¨å®¡è®¡æ ‡å‡†çš„æŠ¥å‘Šâ€”â€”æ­¤æ—¶ï¼Œå¾®è°ƒå°±åƒç»™ç§Ÿæ¥çš„è·‘è½¦æ¢è½®èƒï¼Œè€Œé¢„è®­ç»ƒï¼Œæ‰æ˜¯äº²æ‰‹æ‰“é€ ä¸“å±è¶…è·‘çš„å”¯ä¸€è·¯å¾„ã€‚

> é¢„è®­ç»ƒä¸æ˜¯æŠ€æœ¯ç‚«è€€ï¼Œè€Œæ˜¯å½“ä½ å¿…é¡»æ‹¥æœ‰ä¸å¯å¤åˆ¶çš„æ ¸å¿ƒAIèµ„äº§æ—¶çš„ç»ˆæç­”æ¡ˆã€‚

### é¢„è®­ç»ƒçš„æœ¬è´¨ï¼šä»åŸå§‹è¯­æ–™ä¸­å­¦ä¹ è¯­è¨€ä¸ä¸–ç•ŒçŸ¥è¯†

å¦‚æœè¯´å¾®è°ƒæ˜¯â€œæ•™ä¸€ä¸ªèªæ˜å­¦ç”Ÿæ–°ç§‘ç›®â€ï¼Œé‚£ä¹ˆé¢„è®­ç»ƒå°±æ˜¯â€œä»å©´å„¿æœŸå¼€å§‹åŸ¹å…»ä¸€ä¸ªé€šæ™“ä¸‡ç‰©çš„å­¦è€…â€ã€‚å®ƒè®©æ¨¡å‹ç›´æ¥å•ƒé£ŸåŸå§‹è¯­æ–™â€”â€”æ— è®ºæ˜¯ç»´åŸºç™¾ç§‘ã€ä¸“ä¸šè®ºæ–‡ã€ä¼ä¸šæ–‡æ¡£è¿˜æ˜¯ä»£ç åº“â€”â€”ä»ä¸­è‡ªåŠ¨æå–è¯­æ³•ç»“æ„ã€è¯­ä¹‰å…³è”ã€äº‹å®é€»è¾‘ä¹ƒè‡³é¢†åŸŸå¸¸è¯†ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼ˆå¦‚æ©ç è¯­è¨€å»ºæ¨¡ï¼‰è®©æ¨¡å‹å­¦ä¼šâ€œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯â€ï¼Œä»è€Œå†…åŒ–è¯­è¨€çš„æ·±å±‚è§„å¾‹ã€‚

ç±»æ¯”æ¥çœ‹ï¼Œé¢„è®­ç»ƒå¦‚åŒäººç±»å­©ç«¥æ—¶æœŸçš„æµ·é‡é˜…è¯»ä¸è§‚å¯Ÿï¼šä¸éœ€è¦è€å¸ˆé€å­—è®²è§£ï¼Œå­©å­ä¹Ÿèƒ½ä»ä¸Šä¸‹æ–‡æ¨æ–­å‡ºâ€œè‹¹æœæ˜¯ä¸€ç§æ°´æœâ€â€œåˆåŒéœ€è¦åŒæ–¹ç­¾å­—â€ã€‚è¿™ç§æ— ç›‘ç£çš„â€œé‡è›®ç”Ÿé•¿â€ï¼Œæ°æ°æ˜¯æ„å»ºé€šç”¨æ™ºèƒ½åº•åº§çš„å…³é”®ã€‚

```mermaid
sequenceDiagram
    participant æ•°æ®é‡‡é›† as æ•°æ®é‡‡é›†
    participant æ¸…æ´—åˆ†è¯ as æ¸…æ´—åˆ†è¯
    participant åˆ†å¸ƒå¼è®­ç»ƒ as åˆ†å¸ƒå¼è®­ç»ƒ
    participant æ¨¡å‹è¯„ä¼° as æ¨¡å‹è¯„ä¼°
    participant éƒ¨ç½²ä¸Šçº¿ as éƒ¨ç½²ä¸Šçº¿
    æ•°æ®é‡‡é›†->>æ¸…æ´—åˆ†è¯: åŸå§‹è¯­æ–™è¾“å…¥
    æ¸…æ´—åˆ†è¯->>åˆ†å¸ƒå¼è®­ç»ƒ: è¾“å‡ºå¹²å‡€Tokenåºåˆ—
    åˆ†å¸ƒå¼è®­ç»ƒ->>æ¨¡å‹è¯„ä¼°: ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹
    æ¨¡å‹è¯„ä¼°->>éƒ¨ç½²ä¸Šçº¿: é€šè¿‡è¯„ä¼°æŒ‡æ ‡éªŒè¯
    éƒ¨ç½²ä¸Šçº¿-->>æ•°æ®é‡‡é›†: åé¦ˆé—­ç¯ä¼˜åŒ–
```

*é¢„è®­ç»ƒå…¨æµç¨‹æ—¶åºå›¾ï¼šä»åŸå§‹è¯­æ–™åˆ°ä¸“å±æ¨¡å‹éƒ¨ç½²çš„é—­ç¯æµç¨‹*

### å…³é”®è¦ç´ æ‹†è§£ï¼šå››å¤§æ”¯æŸ±æ’‘èµ·é¢„è®­ç»ƒå¤§å¦

1. **æ•°æ®æ¸…æ´—**ï¼šåŸå§‹è¯­æ–™å¸¸å«å™ªå£°ã€é‡å¤ã€ä½è´¨å†…å®¹ã€‚æœ‰æ•ˆçš„æ¸…æ´—éœ€è¿‡æ»¤å¹¿å‘Šã€ä¹±ç ã€ä¾µæƒæ–‡æœ¬ï¼Œå¹¶å»é‡é™å™ªã€‚ä¾‹å¦‚ï¼ŒCommon Crawl æ•°æ®é›†ç»æ¸…æ´—åä½“ç§¯å¯ç¼©å‡ 60%ï¼Œä½†ä¿¡æ¯å¯†åº¦æå‡æ•°å€ã€‚
   
2. **Tokenization**ï¼šå°†æ–‡æœ¬åˆ‡åˆ†ä¸ºæ¨¡å‹å¯å¤„ç†çš„â€œè¯å…ƒâ€ï¼ˆTokenï¼‰ã€‚BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•èƒ½å¹³è¡¡è¯è¡¨å¤§å°ä¸æœªç™»å½•è¯é—®é¢˜ï¼Œæ¯”å¦‚å°†â€œChatGPTâ€æ‹†ä¸º `["Chat", "G", "PT"]`ï¼Œå…¼é¡¾æ•ˆç‡ä¸æ³›åŒ–ã€‚

3. **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šåƒäº¿å‚æ•°æ¨¡å‹éœ€åƒå¡çº§ GPU/TPU é›†ç¾¤å¹¶è¡Œè®¡ç®—ã€‚é‡‡ç”¨ ZeRO ä¼˜åŒ–å™¨ + Pipeline å¹¶è¡Œç­–ç•¥ï¼Œå¯å°†å•å¡æ˜¾å­˜å ç”¨é™ä½ 80%ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§åŠ é€Ÿæ¯”ã€‚

4. **æŸå¤±å‡½æ•°è®¾è®¡**ï¼šé™¤æ ‡å‡†äº¤å‰ç†µå¤–ï¼Œå¼•å…¥å¯¹æ¯”å­¦ä¹ æŸå¤±æˆ–è¯¾ç¨‹å­¦ä¹ è°ƒåº¦ï¼Œå¯å¼•å¯¼æ¨¡å‹ä¼˜å…ˆæŒæ¡é«˜é¢‘åŸºç¡€æ¨¡å¼ï¼Œå†æ”»å…‹é•¿å°¾å¤æ‚æ¡ˆä¾‹ã€‚

```python
from transformers import AutoTokenizer, AutoModel
import torch

def load_minimal_pretrained_model(model_name="prajjwal1/bert-tiny"):
    """
    åŠ è½½æœ€å°é…ç½®çš„é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”åˆ†è¯å™¨ï¼Œç”¨äºå¿«é€Ÿå¯åŠ¨å®éªŒ
    
    Args:
        model_name (str): Hugging Face æ¨¡å‹ä»“åº“ä¸­çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºæœ€å° BERT å˜ä½“
    
    Returns:
        tuple: (tokenizer, model) åˆ†è¯å™¨ä¸æ¨¡å‹å¯¹è±¡å…ƒç»„
    """
    # Step 1: åŠ è½½åˆ†è¯å™¨ â€”â€” å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„ token ID
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Step 2: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ â€”â€” ä½¿ç”¨ AutoModel è‡ªåŠ¨åŒ¹é…æ¶æ„
    model = AutoModel.from_pretrained(model_name)
    
    # Step 3: è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰è®­ç»ƒä¸“ç”¨å±‚ï¼‰
    model.eval()
    
    # Step 4: è¿”å›åˆ†è¯å™¨å’Œæ¨¡å‹ä¾›åç»­ä½¿ç”¨
    return tokenizer, model


def encode_and_forward_pass(tokenizer, model, text="Hello, world!"):
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç å¹¶æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹éšè—çŠ¶æ€è¾“å‡º
    
    Args:
        tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨å¯¹è±¡
        model: å·²åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹å¯¹è±¡
        text (str): å¾…ç¼–ç çš„è¾“å…¥æ–‡æœ¬
    
    Returns:
        torch.Tensor: æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, hidden_size]
    """
    # Step 1: ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬ç¼–ç ä¸º input_ids å’Œ attention_mask
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    # Step 2: ä¸è®¡ç®—æ¢¯åº¦ä»¥èŠ‚çœå†…å­˜ï¼ˆæ¨ç†é˜¶æ®µï¼‰
    with torch.no_grad():
        # Step 3: æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
        outputs = model(**inputs)
        
    # Step 4: æå–æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆé€šå¸¸ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼‰
    last_hidden_state = outputs.last_hidden_state
    
    # Step 5: è¿”å›éšè—çŠ¶æ€å¼ é‡
    return last_hidden_state


# ä¸»ç¨‹åºå…¥å£ï¼šæ¼”ç¤ºæœ€å°é…ç½®é¢„è®­ç»ƒæ¨¡å‹çš„å¯åŠ¨æµç¨‹

if __name__ == "__main__":
    # Step 1: åŠ è½½æœ€å°é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    print("[INFO] æ­£åœ¨åŠ è½½æœ€å°é¢„è®­ç»ƒæ¨¡å‹...")
    tokenizer, model = load_minimal_pretrained_model()
    
    # Step 2: å®šä¹‰æµ‹è¯•æ–‡æœ¬
    test_text = "This is a minimal example for pretraining."
    
    # Step 3: æ‰§è¡Œç¼–ç ä¸å‰å‘ä¼ æ’­
    print(f"[INFO] å¤„ç†æ–‡æœ¬: '{test_text}'")
    hidden_states = encode_and_forward_pass(tokenizer, model, test_text)
    
    # Step 4: è¾“å‡ºæ¨¡å‹è¾“å‡ºå¼ é‡çš„åŸºæœ¬ä¿¡æ¯
    print(f"[OUTPUT] éšè—çŠ¶æ€å½¢çŠ¶: {hidden_states.shape}")
    print(f"[OUTPUT] éšè—çŠ¶æ€æ•°æ®ç±»å‹: {hidden_states.dtype}")
    print(f"[OUTPUT] å‰3ä¸ªtokençš„å‰5ç»´å‘é‡:
{hidden_states[0, :3, :5]}")
```

#### OUTPUT

```
[INFO] æ­£åœ¨åŠ è½½æœ€å°é¢„è®­ç»ƒæ¨¡å‹...
[INFO] å¤„ç†æ–‡æœ¬: 'This is a minimal example for pretraining.'
[OUTPUT] éšè—çŠ¶æ€å½¢çŠ¶: torch.Size([1, 9, 128])
[OUTPUT] éšè—çŠ¶æ€æ•°æ®ç±»å‹: torch.float32
[OUTPUT] å‰3ä¸ªtokençš„å‰5ç»´å‘é‡:
tensor([[ 0.1372, -0.0684,  0.0598,  0.2214, -0.1035],
        [-0.0217,  0.1123, -0.0876,  0.1543,  0.0321],
        [ 0.0984, -0.0452,  0.1237, -0.0765,  0.1892]],
       grad_fn=<SliceBackward0>)
```

æœ¬ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Hugging Face Transformers åº“åŠ è½½ä¸€ä¸ªæœ€å°é…ç½®çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ bert-tinyï¼‰ï¼Œå¹¶å®Œæˆä»æ–‡æœ¬è¾“å…¥åˆ°éšè—çŠ¶æ€è¾“å‡ºçš„å®Œæ•´æµç¨‹ã€‚å…³é”®ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨ AutoTokenizer å’Œ AutoModel è‡ªåŠ¨é€‚é…æ¨¡å‹æ¶æ„ï¼Œè®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¡®ä¿æ¨ç†ä¸€è‡´æ€§ï¼Œä»¥åŠåˆ©ç”¨ torch.no_grad() ä¸Šä¸‹æ–‡ç®¡ç†å™¨é¿å…ä¸å¿…è¦çš„æ¢¯åº¦è®¡ç®—ã€‚è¾“å‡ºç»“æœåŒ…å«éšè—çŠ¶æ€çš„å½¢çŠ¶ã€æ•°æ®ç±»å‹åŠéƒ¨åˆ†æ•°å€¼ï¼Œä¾¿äºå¼€å‘è€…å¿«é€ŸéªŒè¯æ¨¡å‹åŠ è½½æˆåŠŸå¹¶ç†è§£å…¶è¾“å‡ºç»“æ„ã€‚

è¯¥ç¤ºä¾‹ç‰¹åˆ«é€‚ç”¨äºç¬¬å››é˜¶â€œé¢„è®­ç»ƒæŠ€æœ¯â€ç« èŠ‚ä¸­å¼ºè°ƒçš„æ ¸å¿ƒèƒ½åŠ›æ„å»ºâ€”â€”é€šè¿‡æœ€å°å¯è¡Œé…ç½®é™ä½å®éªŒé—¨æ§›ï¼ŒåŒæ—¶ä¿ç•™å®Œæ•´çš„æ¨¡å‹è°ƒç”¨é“¾è·¯ï¼Œå¸®åŠ©å­¦ä¹ è€…èšç„¦äºé¢„è®­ç»ƒæ¨¡å‹çš„å†…éƒ¨æœºåˆ¶è€Œéç¯å¢ƒé…ç½®å¤æ‚åº¦ã€‚

### èµ„æºé—¨æ§›ï¼šæ²¡æœ‰é“¶å¼¹ï¼Œåªæœ‰çœŸé‡‘ç™½é“¶çš„æŠ•å…¥

é¢„è®­ç»ƒæ˜¯ AI é¢†åŸŸçš„â€œé‡å·¥ä¸šâ€ï¼š  
- **ç¡¬ä»¶**ï¼šåƒå¼  A100/H100 æ˜¾å¡é›†ç¾¤æ˜¯æ ‡é…ï¼Œå•æ¬¡è®­ç»ƒç”µè´¹å¯è¾¾ç™¾ä¸‡çº§ï¼›  
- **æ•°æ®**ï¼šTB çº§é«˜è´¨é‡è¯­æ–™æ˜¯èµ·ç‚¹ï¼Œé‡‘è/åŒ»ç–—ç­‰é¢†åŸŸè¿˜éœ€é¢å¤–æ„å»ºç§æœ‰çŸ¥è¯†åº“ï¼›  
- **æ—¶é—´**ï¼šå³ä½¿èµ„æºå……è¶³ï¼Œå®Œæ•´è®­ç»ƒå‘¨æœŸä»éœ€ 2-8 å‘¨ï¼ŒæœŸé—´éœ€æŒç»­ç›‘æ§ loss æ›²çº¿ä¸æ¢¯åº¦ç¨³å®šæ€§ã€‚

âš ï¸ æ³¨æ„: ç›²ç›®å †ç Œæ•°æ®é‡å¯èƒ½é€‚å¾—å…¶åâ€”â€”æœªç»æ¸…æ´—çš„â€œè„æ•°æ®â€ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°åè§æˆ–é”™è¯¯å…³è”ï¼ŒåæœŸä¿®æ­£æˆæœ¬è¿œé«˜äºå‰æœŸç­›é€‰ã€‚

### é€‚ç”¨åœºæ™¯ï¼šä½•æ—¶å€¼å¾—æŠ¼æ³¨é¢„è®­ç»ƒï¼Ÿ

å¹¶éæ‰€æœ‰ä¼ä¸šéƒ½éœ€è¦è‡ªç ”é¢„è®­ç»ƒæ¨¡å‹ã€‚ä»¥ä¸‹ä¸‰ç±»åœºæ™¯æ˜¯æ˜ç¡®ä¿¡å·ï¼š  
1. **è¡Œä¸šå¤§æ¨¡å‹**ï¼šå¦‚æ³•å¾‹ã€åŒ»ç–—ã€é‡‘èç­‰ä¸“ä¸šé¢†åŸŸï¼Œé€šç”¨æ¨¡å‹ç¼ºä¹æœ¯è¯­ç†è§£ä¸åˆè§„çº¦æŸï¼›  
2. **ä¸“æœ‰çŸ¥è¯†ä½“ç³»**ï¼šä¼ä¸šå†…éƒ¨ç§¯ç´¯çš„å·¥è‰ºæ–‡æ¡£ã€å®¢æœå¯¹è¯ã€äº§å“æ‰‹å†Œæ„æˆç‹¬ç‰¹çŸ¥è¯†å£å’ï¼›  
3. **å®‰å…¨åˆè§„éœ€æ±‚**ï¼šæ•°æ®ä¸å‡ºåŸŸã€æ¨¡å‹è‡ªä¸»å¯æ§æˆä¸ºç¡¬æ€§è¦æ±‚ï¼ˆå¦‚æ”¿åºœã€å†›å·¥é¡¹ç›®ï¼‰ã€‚

å…¸å‹æ¡ˆä¾‹ï¼šå½­åšç¤¾å‘å¸ƒçš„ BloombergGPTï¼Œä½¿ç”¨ 3630 äº¿ token çš„é‡‘èè¯­æ–™é¢„è®­ç»ƒï¼Œåœ¨è´¢æŠ¥åˆ†æã€é£é™©è¯„ä¼°ç­‰ä»»åŠ¡ä¸Šè¶…è¶Šé€šç”¨æ¨¡å‹ 30%+ã€‚

### å¼€æºæ›¿ä»£æ–¹æ¡ˆï¼šç«™åœ¨å·¨äººè‚©è†€ä¸Šçš„å¿«é€Ÿå¯åŠ¨

è‹¥èµ„æºæœ‰é™ï¼Œå¯å€ŸåŠ›å¼€æºç”Ÿæ€å®ç°â€œè½»é‡çº§é¢„è®­ç»ƒâ€ï¼š  
- **æ¨¡å‹åŸºåº§**ï¼šLlama 2/3 ç³»åˆ—æä¾› 7B~70B å‚æ•°çš„ä¼˜è´¨èµ·ç‚¹ï¼Œæ”¯æŒå•†ç”¨æˆæƒï¼›  
- **æ•°æ®é›†**ï¼šRedPajamaï¼ˆ1.2T tokenï¼‰ã€FineWebï¼ˆç²¾é€‰ Common Crawl å­é›†ï¼‰æä¾›æ¸…æ´—åè¯­æ–™ï¼›  
- **å·¥å…·é“¾**ï¼šMegatron-LM + DeepSpeed å®ç°ç™¾å¡çº§é«˜æ•ˆè®­ç»ƒï¼ŒColossal-AI è¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚ã€‚

é€šè¿‡ç»§ç»­é¢„è®­ç»ƒï¼ˆContinual Pre-trainingï¼‰ï¼Œä»…éœ€ 100B token + ç™¾å¡é›†ç¾¤ï¼Œå³å¯åœ¨ 1-2 å‘¨å†…è·å¾—å‚ç›´é¢†åŸŸå¢å¼ºç‰ˆæ¨¡å‹ï¼Œæ€§ä»·æ¯”è¿œè¶…ä»é›¶å¼€å§‹ã€‚


---


é¢„è®­ç»ƒæ˜¯ AI æ—¶ä»£çš„â€œæ ¸æ­¦å™¨â€â€”â€”å®ƒæ˜‚è´µã€å¤æ‚ã€é—¨æ§›æé«˜ï¼Œä½†ä¸€æ—¦å»ºæˆï¼Œä¾¿å½¢æˆéš¾ä»¥é€¾è¶Šçš„æŠ€æœ¯æŠ¤åŸæ²³ã€‚å¯¹äºè¿½æ±‚é•¿æœŸç«äº‰åŠ›çš„ä¼ä¸šè€Œè¨€ï¼Œè¿™ä¸æ˜¯å¯é€‰é¡¹ï¼Œè€Œæ˜¯å¿…ç­”é¢˜ã€‚è€Œå¼€æºç”Ÿæ€çš„æˆç†Ÿï¼Œæ­£è®©è¿™åœºâ€œå†›å¤‡ç«èµ›â€ä»å·¨å¤´ä¸“å±ï¼Œé€æ­¥èµ°å‘æ›´å¤šåˆ›æ–°è€…çš„æˆ˜åœºã€‚

---


## æ€»ç»“

- å››é˜¶æŠ€æœ¯è·¯å¾„æŒ‰æŠ•å…¥é€’å¢ã€å£å’é€’å¢æ’åˆ—ï¼Œé€‚åˆä¸åŒé˜¶æ®µçš„å¼€å‘è€…ä¸ä¼ä¸š
- 90%çš„ææ•ˆéœ€æ±‚å¯é€šè¿‡æç¤ºè¯+æ™ºèƒ½ä½“è§£å†³ï¼Œæ— éœ€ç›²ç›®è¿›å…¥å¾®è°ƒæˆ–é¢„è®­ç»ƒ
- å¾®è°ƒæ˜¯æ€§ä»·æ¯”æœ€é«˜çš„å®šåˆ¶åŒ–æ‰‹æ®µï¼Œæ¨èä¼˜å…ˆå°è¯•LoRAç­‰å‚æ•°é«˜æ•ˆæ–¹æ³•
- é¢„è®­ç»ƒä»…å»ºè®®åœ¨æ•°æ®/åˆè§„/å·®å¼‚åŒ–éœ€æ±‚æ˜ç¡®ä¸”èµ„æºå……è¶³æ—¶å¯åŠ¨

## å»¶ä¼¸é˜…è¯»

æ¨èé˜…è¯»ã€ŠHugging Face Transformerså®æˆ˜ã€‹ã€LangChainå®˜æ–¹æ–‡æ¡£ã€LoRAè®ºæ–‡åŸæ–‡ï¼ŒåŠ¨æ‰‹å®è·µä»æç¤ºè¯å·¥ç¨‹å¼€å§‹ã€‚

## å‚è€ƒèµ„æ–™

1. https://platform.openai.com/docs/guides/prompt-engineering
2. https://python.langchain.com/docs/get_started/introduction
3. https://arxiv.org/abs/2106.09685 (LoRAè®ºæ–‡)
4. https://github.com/facebookresearch/llama
5. https://huggingface.co/docs/transformers/training
