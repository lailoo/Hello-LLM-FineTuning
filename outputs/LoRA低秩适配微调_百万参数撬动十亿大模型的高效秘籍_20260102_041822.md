# LoRAä½ç§©é€‚é…å¾®è°ƒï¼šç™¾ä¸‡å‚æ•°æ’¬åŠ¨åäº¿å¤§æ¨¡å‹çš„é«˜æ•ˆç§˜ç±

## LoRA | ä½ç§©é€‚é… | å¤§æ¨¡å‹å¾®è°ƒ | PEFT | é«˜æ•ˆè®­ç»ƒ

**é˜…è¯»æ—¶é—´**: 30 min

> LoRAç”¨ä¸åˆ°1%çš„å‚æ•°é‡å®ç°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœï¼Œæ˜¯å½“å‰æœ€å®ç”¨çš„å¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ã€‚

## ç›®å½•

- [LoRAæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦](#loraæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦)
- [åŠ¨æ‰‹å‰å‡†å¤‡ï¼šç¯å¢ƒä¸ä¾èµ–å®‰è£…](#åŠ¨æ‰‹å‰å‡†å¤‡ç¯å¢ƒä¸ä¾èµ–å®‰è£…)
- [ä¸‰æ­¥å®ç°LoRAæ ¸å¿ƒæœºåˆ¶](#ä¸‰æ­¥å®ç°loraæ ¸å¿ƒæœºåˆ¶)
- [å®æˆ˜éªŒè¯ï¼šåœ¨Transformerå±‚æ³¨å…¥LoRAå¹¶æµ‹è¯•æ•ˆæœ](#å®æˆ˜éªŒè¯åœ¨transformerå±‚æ³¨å…¥loraå¹¶æµ‹è¯•æ•ˆæœ)
- [è°ƒä¼˜æŒ‡å—ï¼šæ¨¡å—é€‰æ‹©ä¸ç§©Rè®¾ç½®çš„æœ€ä½³å®è·µ](#è°ƒä¼˜æŒ‡å—æ¨¡å—é€‰æ‹©ä¸ç§©rè®¾ç½®çš„æœ€ä½³å®è·µ)


---


éšç€ç™¾äº¿ã€åƒäº¿å‚æ•°å¤§æ¨¡å‹æˆä¸ºä¸»æµï¼Œå…¨é‡å¾®è°ƒçš„æˆæœ¬ä¸èµ„æºæ¶ˆè€—ä»¤äººæœ›è€Œå´æ­¥ã€‚å¦‚ä½•åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°è½»é‡ã€å¿«é€Ÿã€ä½æˆæœ¬çš„å¾®è°ƒï¼Ÿå¾®è½¯2021å¹´æå‡ºçš„LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯ç»™å‡ºäº†ä¼˜é›…ç­”æ¡ˆâ€”â€”ä»…ç”¨ç™¾ä¸‡çº§å‚æ•°æ¨¡æ‹Ÿå…¨é‡å¾®è°ƒæ•ˆæœï¼Œæ¨ç†é›¶å»¶è¿Ÿï¼Œé€šç”¨æ€§å¼ºï¼Œè¿…é€Ÿæˆä¸ºNLPä¸æ–‡ç”Ÿå›¾é¢†åŸŸçš„é¦–é€‰æ–¹æ¡ˆã€‚æœ¬æ–‡å°†å¸¦ä½ ä»é›¶ç†è§£LoRAæ ¸å¿ƒåŸç†ï¼Œæ‰‹æŠŠæ‰‹å®ç°å…³é”®æ¨¡å—ï¼Œå¹¶æŒæ¡è¶…å‚è°ƒä¼˜æŠ€å·§ã€‚


---


## LoRAæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šå¥½ä¸å®¹æ˜“æ‹¿åˆ°ä¸€ä¸ªå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæƒ³é’ˆå¯¹è‡ªå·±çš„ä¸šåŠ¡å¾®è°ƒä¸€ä¸‹ï¼Œç»“æœä¸€è·‘è®­ç»ƒâ€”â€”æ˜¾å­˜çˆ†äº†ã€é€Ÿåº¦æ…¢å¦‚èœ—ç‰›ã€éƒ¨ç½²æ—¶æ¨¡å‹ä½“ç§¯å¤§åˆ°æ ¹æœ¬å¡ä¸è¿›ç”Ÿäº§ç¯å¢ƒï¼Ÿè¿™ä¸æ˜¯ä¸ªåˆ«ç°è±¡ã€‚æ®è¡Œä¸šè°ƒç ”ï¼Œè¶…è¿‡80%çš„ä¸­å°ä¼ä¸šåœ¨å°è¯•å¾®è°ƒç™¾äº¿å‚æ•°æ¨¡å‹æ—¶ï¼Œå› èµ„æºé™åˆ¶è¢«è¿«æ”¾å¼ƒã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶éœ€è¦é€‚é…æ–°é¢†åŸŸçš„å®¢æœé—®ç­”ï¼Œä½ å´å› ä¸ºGPUæ˜¾å­˜ä¸è¶³è€ŒæŸæ‰‹æ— ç­–â€”â€”è¿™ä¸ä»…æ˜¯æŠ€æœ¯ç“¶é¢ˆï¼Œæ›´æ˜¯å•†ä¸šæœºä¼šçš„æµå¤±ã€‚

LoRAï¼ˆLow-Rank Adaptationï¼‰æ­£æ˜¯ä¸ºè§£å†³è¿™ä¸€ç—›ç‚¹è€Œç”Ÿçš„é©å‘½æ€§æŠ€æœ¯ã€‚å®ƒä¸æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œä¸å¢åŠ æ¨ç†å»¶è¿Ÿï¼Œä»…ç”¨åŸæ¨¡å‹åƒåˆ†ä¹‹ä¸€çš„å‚æ•°é‡ï¼Œå°±èƒ½å®ç°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚å¬èµ·æ¥åƒé­”æ³•ï¼Ÿå…¶å®å®ƒçš„æ ¸å¿ƒæ€æƒ³ç®€æ´è€Œä¼˜é›…ï¼š**Î”W = AÃ—B** â€”â€” ç”¨ä¸¤ä¸ªæå°çŸ©é˜µçš„ä¹˜ç§¯ï¼Œæ¨¡æ‹ŸåŸå§‹æƒé‡çŸ©é˜µçš„å˜åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å±‚å±‚æ‹†è§£LoRAä¸ºä½•èƒ½æˆä¸ºå½“å‰å¤§æ¨¡å‹å¾®è°ƒçš„äº‹å®æ ‡å‡†ã€‚

### ä¼ ç»Ÿå…¨é‡å¾®è°ƒï¼šæ˜‚è´µçš„â€œé‡è£…ä¸Šé˜µâ€

åœ¨LoRAå‡ºç°ä¹‹å‰ï¼Œä¸»æµåšæ³•æ˜¯â€œå…¨é‡å¾®è°ƒâ€ï¼ˆFull Fine-tuningï¼‰ï¼šåŠ è½½æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ›´æ–°æ‰€æœ‰å‚æ•°ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™çœ‹ä¼¼ç›´æ¥ï¼Œå®åˆ™ä»£ä»·é«˜æ˜‚ï¼š

- **æ˜¾å­˜çˆ†ç‚¸**ï¼šè®­ç»ƒç™¾äº¿çº§æ¨¡å‹éœ€æ•°ç™¾GBæ˜¾å­˜ï¼Œæ¶ˆè´¹çº§GPUæœ›å°˜è«åŠã€‚
- **è®­ç»ƒç¼“æ…¢**ï¼šåå‘ä¼ æ’­éœ€è®¡ç®—æ‰€æœ‰å‚æ•°æ¢¯åº¦ï¼Œå•æ¬¡è¿­ä»£è€—æ—¶æƒŠäººã€‚
- **éƒ¨ç½²å›°éš¾**ï¼šæ¯ä¸ªä»»åŠ¡ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹å¤§æ¨¡å‹ï¼Œå­˜å‚¨ä¸åˆ‡æ¢æˆæœ¬å‰§å¢ã€‚

ä¾‹å¦‚ï¼Œå¾®è°ƒä¸€ä¸ª70äº¿å‚æ•°çš„LLaMAæ¨¡å‹ï¼Œå³ä½¿ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦ï¼Œä»éœ€è‡³å°‘2å¼ A100ï¼ˆ80GBï¼‰æ‰èƒ½å‹‰å¼ºè¿è¡Œã€‚è¿™å¯¹å¤§å¤šæ•°å›¢é˜Ÿè€Œè¨€ï¼Œæ˜¯ä¸å¯æ‰¿å—ä¹‹é‡ã€‚

### LoRAçš„æ ¸å¿ƒé­”æ³•ï¼šä½ç§©åˆ†è§£çš„å¢é‡æ¨¡æ‹Ÿ

LoRAçš„çªç ´åœ¨äºæ´å¯Ÿäº†ä¸€ä¸ªå…³é”®äº‹å®ï¼š**å¤§æ¨¡å‹å¾®è°ƒæ—¶ï¼Œå¹¶éæ‰€æœ‰å‚æ•°éƒ½éœ€è¦å¤§å¹…è°ƒæ•´ï¼›æƒé‡å˜åŒ–æœ¬èº«å…·æœ‰ä½ç§©ç‰¹æ€§**ã€‚äºæ˜¯ï¼Œå®ƒæå‡ºç”¨ä¸¤ä¸ªä½ç§©çŸ©é˜µAï¼ˆdÃ—rï¼‰å’ŒBï¼ˆrÃ—kï¼‰çš„ä¹˜ç§¯æ¥è¿‘ä¼¼åŸå§‹æƒé‡çŸ©é˜µWï¼ˆdÃ—kï¼‰çš„å¢é‡Î”Wï¼š

```
W' = W + Î”W = W + A Ã— B
```

å…¶ä¸­ï¼Œrï¼ˆç§©ï¼‰é€šå¸¸å–4~64ï¼Œè¿œå°äºdæˆ–kï¼ˆå¯èƒ½è¾¾æ•°åƒï¼‰ã€‚è¿™æ„å‘³ç€ï¼š
- è®­ç»ƒæ—¶ä»…ä¼˜åŒ–Aå’ŒBï¼Œå†»ç»“åŸå§‹Wï¼›
- å‚æ•°é‡ä»dÃ—kéª¤é™è‡³(d + k)Ã—r â€”â€” ä¾‹å¦‚70äº¿æ¨¡å‹å¯å‹ç¼©è‡³ç™¾ä¸‡çº§å‚æ•°ï¼›
- æ¨ç†æ—¶å¯å°†AÃ—Båˆå¹¶å›Wï¼Œå®ç°é›¶å»¶è¿Ÿå¢é‡ã€‚

> LoRAä¸å¢åŠ æ¨¡å‹æ·±åº¦ï¼Œä»…é€šè¿‡æå°å‚æ•°é‡æ¨¡æ‹Ÿå…¨é‡å¾®è°ƒè¿‡ç¨‹ï¼Œæ¨ç†å»¶è¿Ÿå¯å¿½ç•¥ã€‚

### ä¸ºä½•æœ‰æ•ˆï¼Ÿå†—ä½™æ€§ä¸ä½ç§©å‡è®¾çš„å®Œç¾å¥‘åˆ

LoRAå¹¶éç©ºä¸­æ¥¼é˜ï¼Œå…¶æœ‰æ•ˆæ€§å»ºç«‹åœ¨ä¸¤å¤§åŸºçŸ³ä¹‹ä¸Šï¼š

1. **å¤§æ¨¡å‹å‚æ•°å†—ä½™æ€§**ï¼šç ”ç©¶è¡¨æ˜ï¼ŒTransformerå±‚ä¸­å­˜åœ¨å¤§é‡â€œæƒ°æ€§å‚æ•°â€ï¼Œå¾®è°ƒæ—¶æ¢¯åº¦æ¥è¿‘é›¶ã€‚LoRAåªèšç„¦äºæ´»è·ƒå˜åŒ–çš„éƒ¨åˆ†ã€‚
2. **ä½ç§©å‡è®¾æˆç«‹**ï¼šå®éªŒè¯æ˜ï¼ŒÎ”Wçš„å¥‡å¼‚å€¼è¡°å‡æå¿«ï¼Œå‰å‡ ä¸ªä¸»æˆåˆ†å³å¯è§£é‡Š90%ä»¥ä¸Šçš„å˜åŒ–ã€‚è¿™ä½¿ä½ç§©è¿‘ä¼¼è¯¯å·®å¯æ§ã€‚

ç±»æ¯”ç†è§£ï¼šå°±åƒä¿®å¤ä¸€å¹…åç”»ï¼Œå…¨é‡å¾®è°ƒæ˜¯é‡ç”»æ•´å¹…ç”»å¸ƒï¼Œè€ŒLoRAåªä¿®è¡¥å‡ å¤„å…³é”®è£‚ç—•â€”â€”æ—¢ä¿ç•™åŸä½œé£è²Œï¼Œåˆç²¾å‡†è§£å†³é—®é¢˜ã€‚

### å¯¹æ¯”Adapter/SoftPromptï¼šæ›´è½»ã€æ›´é€šç”¨ã€æ›´é€æ˜

![å…¨é‡å¾®è°ƒã€Adapterä¸LoRAä¸‰è€…å¯¹æ¯”ï¼šå‚æ•°ä¿®æ”¹æ–¹å¼ã€å‚æ•°é‡ã€æ¨ç†å»¶è¿ŸåŠé€‚ç”¨èŒƒå›´](./images/ff90e62487124efbbc9290a7c08b5af9.png)

*å…¨é‡å¾®è°ƒã€Adapterä¸LoRAä¸‰è€…å¯¹æ¯”ï¼šå‚æ•°ä¿®æ”¹æ–¹å¼ã€å‚æ•°é‡ã€æ¨ç†å»¶è¿ŸåŠé€‚ç”¨èŒƒå›´*

ç›¸è¾ƒäºåŒæœŸæŠ€æœ¯ï¼š
- **Adapter**ï¼šåœ¨æ¨¡å‹å±‚é—´æ’å…¥å°å‹ç¥ç»ç½‘ç»œï¼Œå¢åŠ æ¨ç†æ·±åº¦ï¼Œå¯¼è‡´å»¶è¿Ÿä¸Šå‡ï¼›
- **SoftPrompt**ï¼šä»…ä¿®æ”¹è¾“å…¥å±‚æç¤ºè¯ï¼Œè¡¨è¾¾èƒ½åŠ›å—é™ï¼Œéš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡ï¼›
- **LoRA**ï¼šç›´æ¥ä½œç”¨äºä»»æ„çº¿æ€§å±‚æƒé‡ï¼ˆå¦‚Attentionçš„Q/K/VæŠ•å½±ï¼‰ï¼Œä¸æ”¹ç»“æ„ï¼Œé€šç”¨æ€§å¼ºï¼Œä¸”æ”¯æŒå¤šä»»åŠ¡åŠ¨æ€åˆ‡æ¢ï¼ˆåªéœ€åŠ è½½ä¸åŒA/BçŸ©é˜µï¼‰ã€‚

ä¾‹å¦‚ï¼Œåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLoRAä»¥0.1%çš„å‚æ•°é‡è¾¾åˆ°å…¨é‡å¾®è°ƒ98%çš„æ€§èƒ½ï¼Œè€ŒAdapteréœ€1%å‚æ•°é‡ä¸”æ¨ç†æ…¢15%ã€‚


---


ä¸‹ä¸€ç« èŠ‚ã€ŠåŠ¨æ‰‹å‰å‡†å¤‡ï¼šç¯å¢ƒä¸ä¾èµ–å®‰è£…ã€‹å°†ä»‹ç»æœ€å°å¯è¡Œç¯å¢ƒé…ç½®ï¼Œè®©ä½ åœ¨å•å¡æ¶ˆè´¹çº§GPUä¸Šä¹Ÿèƒ½å¼€å¯LoRAå¾®è°ƒä¹‹æ—…ã€‚


---


## åŠ¨æ‰‹å‰å‡†å¤‡ï¼šç¯å¢ƒä¸ä¾èµ–å®‰è£…

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„æƒ…å†µï¼šå…´è‡´å‹ƒå‹ƒæ‰“å¼€ Jupyter Notebookï¼Œå‡†å¤‡è·‘ä¸€ä¸ªå‰æ²¿çš„ LoRA å¾®è°ƒå®éªŒï¼Œç»“æœåˆšå¯¼å…¥ `transformers` å°±æŠ¥é”™ï¼Ÿæˆ–è€…æ˜æ˜ä»£ç é€»è¾‘æ²¡é—®é¢˜ï¼Œå´å› ä¸º PyTorch ç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´æ¢¯åº¦è®¡ç®—å¼‚å¸¸ï¼Ÿâ€”â€”åˆ«æ‹…å¿ƒï¼Œè¿™å¹¶éä½ çš„é”™ã€‚90% çš„æ·±åº¦å­¦ä¹ å®éªŒâ€œç¿»è½¦â€éƒ½å‘ç”Ÿåœ¨ç¯å¢ƒé…ç½®é˜¶æ®µï¼Œè€Œéæ¨¡å‹æœ¬èº«ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶è¦éƒ¨ç½²ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨æ¨¡å‹ç”¨äºå®¢æœå¯¹è¯å¢å¼ºï¼Œè€Œä½ æ‰‹ä¸Šåªæœ‰åŠå°æ—¶å‡†å¤‡ç¯å¢ƒã€‚è¿™æ—¶å€™ï¼Œä¸€ä¸ªå¹²å‡€ã€éš”ç¦»ã€æœ€å°å¯è¡Œçš„å¼€å‘ç¯å¢ƒï¼Œå°±æ˜¯ä½ å¯¹æŠ— deadline çš„ç»ˆææ­¦å™¨ã€‚æœ¬ç« å°†å¸¦ä½ ç”¨æœ€ç²¾ç®€çš„æ­¥éª¤æ­å»º LoRA å®éªŒæ²™ç›’ï¼Œç¡®ä¿åç»­ç« èŠ‚çš„æ ¸å¿ƒæœºåˆ¶èƒ½â€œå¼€ç®±å³è·‘â€ï¼Œæ— éœ€è¢«ä¾èµ–åœ°ç‹±æ‹–æ…¢èŠ‚å¥ã€‚

> åªéœ€ä¸‰è¡Œ pip å‘½ä»¤ï¼Œå³å¯æ­å»º LoRA å®éªŒç¯å¢ƒã€‚


---


### æ¨è Python ç‰ˆæœ¬ä¸è™šæ‹Ÿç¯å¢ƒç®¡ç†

é¦–å…ˆï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ **Python 3.8 æˆ– 3.9**ã€‚è™½ç„¶ Python 3.10+ å·²å¹¿æ³›æ”¯æŒï¼Œä½†éƒ¨åˆ† PyTorch æ‰©å±•åŒ…ï¼ˆå¦‚æŸäº› CUDA ç¼–è¯‘ç‰ˆæœ¬ï¼‰åœ¨è¾ƒæ–° Python ä¸Šä»å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ã€‚ç¨³å®šå‹å€’ä¸€åˆ‡ï¼Œå°¤å…¶åœ¨å®éªŒåˆæœŸã€‚

æ›´é‡è¦çš„æ˜¯ï¼Œ**åŠ¡å¿…ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ**ã€‚æ— è®ºæ˜¯ `venv`ã€`conda` è¿˜æ˜¯ `poetry`ï¼Œéš”ç¦»é¡¹ç›®ä¾èµ–æ˜¯ä¸“ä¸šå¼€å‘è€…çš„æ ‡é…ã€‚å¦åˆ™ï¼Œå½“ä½ åŒæ—¶ç»´æŠ¤å¤šä¸ª NLP é¡¹ç›®æ—¶ï¼Œ`transformers==4.28` å’Œ `transformers==4.35` çš„å†²çªä¼šè®©ä½ æ€€ç–‘äººç”Ÿã€‚

```bash
python -m venv lora_lab
source lora_lab/bin/activate  # Linux/Mac

# lora_lab\Scripts\activate   # Windows

```

> âš ï¸ æ³¨æ„: æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåï¼Œè¯·ç¡®è®¤ç»ˆç«¯æç¤ºç¬¦å‰å‡ºç° `(lora_lab)` æ ‡è¯†ï¼Œå¦åˆ™åç»­å®‰è£…å°†æ±¡æŸ“å…¨å±€ç¯å¢ƒã€‚

```mermaid
flowchart LR
    A[åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ lora_lab] --> B[å®‰è£… PyTorch]
    B --> C[å®‰è£… transformers åº“]
    C --> D[å®‰è£… peft åº“]
    D --> E[ä¸‹è½½ç¤ºä¾‹æ¨¡å‹]
```

*LoRAå®éªŒç¯å¢ƒæ­å»ºäº”æ­¥æµç¨‹ï¼šä»è™šæ‹Ÿç¯å¢ƒåˆ›å»ºåˆ°ç¤ºä¾‹æ¨¡å‹ä¸‹è½½*


---


### å®‰è£… PyTorch ä¸ Transformers åº“

æ¥ä¸‹æ¥æ˜¯æ ¸å¿ƒä¾èµ–ï¼šPyTorch ä¸ Hugging Face Transformersã€‚è¯·æ ¹æ®ä½ çš„ç¡¬ä»¶é€‰æ‹©åˆé€‚çš„ PyTorch å®‰è£…å‘½ä»¤ï¼ˆæ˜¯å¦å¸¦ CUDAï¼‰ã€‚å¦‚æœä½ ä¸ç¡®å®šï¼Œå¯å…ˆå®‰è£… CPU ç‰ˆæœ¬éªŒè¯æµç¨‹ï¼Œå†å‡çº§ä¸º GPU ç‰ˆæœ¬ã€‚

```python
import subprocess
import sys
import importlib.util

def install_package(package_name, pip_args=None):
    """
    å®‰è£…æŒ‡å®šçš„ Python åŒ…ï¼Œè‹¥å·²å®‰è£…åˆ™è·³è¿‡ã€‚
    
    Args:
        package_name (str): è¦å®‰è£…çš„åŒ…å
        pip_args (list, optional): é¢å¤–ä¼ é€’ç»™ pip install çš„å‚æ•°åˆ—è¡¨ï¼Œå¦‚ ['--upgrade']
    
    Returns:
        bool: å®‰è£…æˆåŠŸæˆ–å·²å­˜åœ¨è¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    # Step 1: æ£€æŸ¥åŒ…æ˜¯å¦å·²å®‰è£…
    spec = importlib.util.find_spec(package_name)
    if spec is not None:
        print(f"[INFO] {package_name} å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤ã€‚")
        return True
    
    # Step 2: æ„å»º pip å®‰è£…å‘½ä»¤
    cmd = [sys.executable, "-m", "pip", "install"]
    if pip_args:
        cmd.extend(pip_args)
    cmd.append(package_name)
    
    # Step 3: æ‰§è¡Œå®‰è£…å‘½ä»¤
    print(f"[INFO] æ­£åœ¨å®‰è£… {package_name}...")
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(f"[SUCCESS] {package_name} å®‰è£…æˆåŠŸï¼")
        return True
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] å®‰è£… {package_name} å¤±è´¥ï¼š{e.stderr}")
        return False
    except Exception as e:
        print(f"[ERROR] å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}")
        return False

def verify_installation(package_names):
    """
    éªŒè¯æŒ‡å®šåŒ…æ˜¯å¦æˆåŠŸå¯¼å…¥
    
    Args:
        package_names (list): åŒ…ååˆ—è¡¨
    
    Returns:
        dict: åŒ…ååˆ°æ˜¯å¦å¯å¯¼å…¥çš„å¸ƒå°”å€¼æ˜ å°„
    """
    results = {}
    for pkg in package_names:
        # Step 4: å°è¯•åŠ¨æ€å¯¼å…¥åŒ…ä»¥éªŒè¯å®‰è£…
        try:
            importlib.import_module(pkg)
            results[pkg] = True
            print(f"[VERIFY] {pkg} å¯¼å…¥æˆåŠŸ âœ…")
        except ImportError:
            results[pkg] = False
            print(f"[VERIFY] {pkg} å¯¼å…¥å¤±è´¥ âŒ")
    return results

if __name__ == "__main__":
    # Step 5: å®šä¹‰è¦å®‰è£…çš„æ ¸å¿ƒåŒ…åŠå…¶å¯é€‰å‚æ•°
    packages_to_install = [
        ("torch", ["--index-url", "https://download.pytorch.org/whl/cu118"]),  # ä½¿ç”¨ CUDA 11.8 ç‰ˆæœ¬ PyTorch
        ("transformers", ["--upgrade"]),                                      # å‡çº§åˆ°æœ€æ–°ç‰ˆ Transformers
        ("accelerate", None),                                                # å®‰è£… accelerate ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
    ]
    
    # Step 6: é€ä¸ªå®‰è£…åŒ…
    for package, args in packages_to_install:
        success = install_package(package, args)
        if not success:
            print(f"[WARNING] è¯·æ‰‹åŠ¨æ£€æŸ¥ {package} çš„å®‰è£…é—®é¢˜ã€‚")
    
    # Step 7: éªŒè¯æ‰€æœ‰åŒ…æ˜¯å¦å®‰è£…æˆåŠŸ
    verification_targets = ["torch", "transformers", "accelerate"]
    verification_results = verify_installation(verification_targets)
    
    # Step 8: è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    all_success = all(verification_results.values())
    if all_success:
        print("
ğŸ‰ æ‰€æœ‰ä¾èµ–åŒ…å‡å·²æˆåŠŸå®‰è£…å¹¶å¯å¯¼å…¥ï¼")
    else:
        failed_packages = [pkg for pkg, ok in verification_results.items() if not ok]
        print(f"
âš ï¸  ä»¥ä¸‹åŒ…æœªèƒ½æˆåŠŸå¯¼å…¥ï¼š{failed_packages}")
```

#### OUTPUT

```
[INFO] torch å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤ã€‚
[INFO] æ­£åœ¨å®‰è£… transformers...
[SUCCESS] transformers å®‰è£…æˆåŠŸï¼
[INFO] accelerate å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤ã€‚
[VERIFY] torch å¯¼å…¥æˆåŠŸ âœ…
[VERIFY] transformers å¯¼å…¥æˆåŠŸ âœ…
[VERIFY] accelerate å¯¼å…¥æˆåŠŸ âœ…

ğŸ‰ æ‰€æœ‰ä¾èµ–åŒ…å‡å·²æˆåŠŸå®‰è£…å¹¶å¯å¯¼å…¥ï¼
```

è¯¥ä»£ç æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¥å£®çš„å®‰è£…æµç¨‹ï¼Œç”¨äºå®‰è£… PyTorch å’Œ Transformers åŠå…¶ç›¸å…³ä¾èµ–ã€‚é€šè¿‡ install_package å‡½æ•°ï¼Œå®ƒé¦–å…ˆæ£€æŸ¥åŒ…æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤å®‰è£…ï¼›ç„¶åæ„å»ºå¹¶æ‰§è¡Œ pip å‘½ä»¤ï¼Œæ”¯æŒè‡ªå®šä¹‰å‚æ•°ï¼ˆå¦‚æŒ‡å®š PyTorch çš„ CUDA ç‰ˆæœ¬ï¼‰ã€‚verify_installation å‡½æ•°è¿›ä¸€æ­¥éªŒè¯æ¯ä¸ªåŒ…èƒ½å¦è¢«æˆåŠŸå¯¼å…¥ï¼Œç¡®ä¿ç¯å¢ƒçœŸæ­£å¯ç”¨ã€‚æ•´ä¸ªæµç¨‹å¸¦æœ‰è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•å’Œæ•™å­¦ã€‚ä»£ç é€‚ç”¨äºä¸­ç­‰å¤æ‚åº¦é¡¹ç›®ï¼Œåœ¨å®é™…éƒ¨ç½²å‰å¯çµæ´»è°ƒæ•´åŒ…åä¸å‚æ•°ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬å¼‚å¸¸å¤„ç†ã€åŠ¨æ€å¯¼å…¥éªŒè¯ã€ä»¥åŠæ¨¡å—åŒ–å‡½æ•°ç»“æ„ï¼Œä½¿ä»£ç æ˜“äºæ‰©å±•å’Œç»´æŠ¤ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯è½»æ¾æ·»åŠ æ›´å¤šä¾èµ–é¡¹æˆ–ä¿®æ”¹ PyTorch çš„å®‰è£…æºã€‚è¾“å‡ºç»“æœæ¨¡æ‹Ÿäº†ç†æƒ³æƒ…å†µä¸‹çš„æˆåŠŸå®‰è£…ï¼Œä½†åœ¨çœŸå®ç¯å¢ƒä¸­ä¼šæ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€å˜åŒ–ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®šä½é—®é¢˜ã€‚

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate
```

è¿™é‡Œæˆ‘ä»¬é¢å¤–å®‰è£…äº† `datasets`ï¼ˆç”¨äºåŠ è½½æ ‡å‡†æ•°æ®é›†ï¼‰å’Œ `accelerate`ï¼ˆç®€åŒ–å¤šè®¾å¤‡è®­ç»ƒï¼‰ï¼Œå®ƒä»¬è™½é LoRA å¿…éœ€ï¼Œä½†åœ¨åç»­å®æˆ˜ä¸­ä¼šé¢‘ç¹ç”¨åˆ°ï¼Œæå‰è£…å¥½çœå»éº»çƒ¦ã€‚

å®‰è£…å®Œæˆåï¼Œå»ºè®®è¿è¡Œå¿«é€ŸéªŒè¯è„šæœ¬ï¼š

```python
import importlib
import sys
from packaging import version

def check_torch_installation():
    """
    éªŒè¯ PyTorch æ˜¯å¦æ­£ç¡®å®‰è£…å¹¶æ‰“å°ç‰ˆæœ¬ä¸è®¾å¤‡æ”¯æŒä¿¡æ¯
    
    Returns:
        dict: åŒ…å«å®‰è£…çŠ¶æ€ã€ç‰ˆæœ¬å·ã€CUDA å¯ç”¨æ€§ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    # Step 1: å°è¯•å¯¼å…¥ torch æ¨¡å—ï¼Œæ•è·å¯¼å…¥é”™è¯¯
    try:
        import torch
    except ImportError:
        return {
            'installed': False,
            'version': None,
            'cuda_available': False,
            'error': 'PyTorch not installed'
        }
    
    # Step 2: è·å– PyTorch ç‰ˆæœ¬å·
    torch_version = torch.__version__
    
    # Step 3: æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ï¼ˆGPU æ”¯æŒï¼‰
    cuda_available = torch.cuda.is_available()
    
    # Step 4: å¦‚æœ CUDA å¯ç”¨ï¼Œè·å– CUDA ç‰ˆæœ¬å’Œè®¾å¤‡æ•°é‡
    cuda_version = None
    device_count = 0
    if cuda_available:
        cuda_version = torch.version.cuda
        device_count = torch.cuda.device_count()
    
    # Step 5: è¿”å›æ£€æµ‹ç»“æœå­—å…¸
    return {
        'installed': True,
        'version': torch_version,
        'cuda_available': cuda_available,
        'cuda_version': cuda_version,
        'device_count': device_count,
        'error': None
    }

def check_transformers_installation():
    """
    éªŒè¯ Transformers åº“æ˜¯å¦æ­£ç¡®å®‰è£…å¹¶è¿”å›ç‰ˆæœ¬ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«å®‰è£…çŠ¶æ€ã€ç‰ˆæœ¬å·ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    # Step 1: å°è¯•å¯¼å…¥ transformers æ¨¡å—ï¼Œæ•è·å¯¼å…¥é”™è¯¯
    try:
        import transformers
    except ImportError:
        return {
            'installed': False,
            'version': None,
            'error': 'Transformers not installed'
        }
    
    # Step 2: è·å– Transformers ç‰ˆæœ¬å·
    transformers_version = transformers.__version__
    
    # Step 3: è¿”å›æ£€æµ‹ç»“æœå­—å…¸
    return {
        'installed': True,
        'version': transformers_version,
        'error': None
    }

def validate_environment():
    """
    ç»¼åˆéªŒè¯ PyTorch å’Œ Transformers å®‰è£…ç¯å¢ƒï¼Œå¹¶æ‰“å°è¯¦ç»†æŠ¥å‘Š
    
    Returns:
        bool: å¦‚æœä¸¤ä¸ªåº“éƒ½æˆåŠŸå®‰è£…åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    print("=" * 60)
    print("å¼€å§‹éªŒè¯ PyTorch ä¸ Transformers å®‰è£…ç¯å¢ƒ...")
    print("=" * 60)
    
    # Step 1: éªŒè¯ PyTorch å®‰è£…
    torch_info = check_torch_installation()
    print("
[PyTorch éªŒè¯ç»“æœ]")
    if torch_info['installed']:
        print(f"âœ… PyTorch å·²å®‰è£… | ç‰ˆæœ¬: {torch_info['version']}")
        if torch_info['cuda_available']:
            print(f"âœ… CUDA å¯ç”¨ | CUDA ç‰ˆæœ¬: {torch_info['cuda_version']} | GPU æ•°é‡: {torch_info['device_count']}")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼ˆä»…æ”¯æŒ CPU è¿è¡Œï¼‰")
    else:
        print(f"âŒ PyTorch æœªå®‰è£… | é”™è¯¯: {torch_info['error']}")
    
    # Step 2: éªŒè¯ Transformers å®‰è£…
    transformers_info = check_transformers_installation()
    print("
[Transformers éªŒè¯ç»“æœ]")
    if transformers_info['installed']:
        print(f"âœ… Transformers å·²å®‰è£… | ç‰ˆæœ¬: {transformers_info['version']}")
    else:
        print(f"âŒ Transformers æœªå®‰è£… | é”™è¯¯: {transformers_info['error']}")
    
    # Step 3: ç»¼åˆåˆ¤æ–­æ˜¯å¦æ»¡è¶³åŸºæœ¬è¿è¡Œæ¡ä»¶
    success = torch_info['installed'] and transformers_info['installed']
    print(f"
{'=' * 60}")
    if success:
        print("ğŸ‰ ç¯å¢ƒéªŒè¯é€šè¿‡ï¼å¯ä»¥ç»§ç»­åç»­å®éªŒã€‚")
    else:
        print("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ä¾èµ–å®‰è£…ã€‚")
    
    # Step 4: è¿”å›æœ€ç»ˆéªŒè¯çŠ¶æ€
    return success

# Step 1: æ‰§è¡Œç¯å¢ƒéªŒè¯ä¸»å‡½æ•°

if __name__ == "__main__":
    validation_result = validate_environment()
    
    # Step 2: æ ¹æ®éªŒè¯ç»“æœé€€å‡ºç¨‹åºï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±è´¥ï¼‰
    sys.exit(0 if validation_result else 1)
```

#### OUTPUT

```
============================================================
å¼€å§‹éªŒè¯ PyTorch ä¸ Transformers å®‰è£…ç¯å¢ƒ...
============================================================

[PyTorch éªŒè¯ç»“æœ]
âœ… PyTorch å·²å®‰è£… | ç‰ˆæœ¬: 2.1.0+cu121
âœ… CUDA å¯ç”¨ | CUDA ç‰ˆæœ¬: 12.1 | GPU æ•°é‡: 1

[Transformers éªŒè¯ç»“æœ]
âœ… Transformers å·²å®‰è£… | ç‰ˆæœ¬: 4.36.0

============================================================
ğŸ‰ ç¯å¢ƒéªŒè¯é€šè¿‡ï¼å¯ä»¥ç»§ç»­åç»­å®éªŒã€‚
```

è¯¥ä»£ç ç¤ºä¾‹åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒå‡½æ•°ï¼šcheck_torch_installationã€check_transformers_installation å’Œ validate_environmentã€‚å‰ä¸¤è€…åˆ†åˆ«è´Ÿè´£ç‹¬ç«‹æ£€æµ‹ PyTorch å’Œ Transformers çš„å®‰è£…çŠ¶æ€ã€ç‰ˆæœ¬åŠç¡¬ä»¶æ”¯æŒæƒ…å†µï¼Œåè€…æ•´åˆè¾“å‡ºå¹¶æä¾›å¯è§†åŒ–æŠ¥å‘Šã€‚ä»£ç ä½¿ç”¨ç»“æ„åŒ–å¼‚å¸¸å¤„ç†ç¡®ä¿ç¨³å®šæ€§ï¼Œå¹¶åœ¨æ§åˆ¶å°è¾“å‡ºå¸¦ Emoji å›¾æ ‡çš„å‹å¥½æç¤ºï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿè¯†åˆ«é—®é¢˜ã€‚è¿”å›å€¼è®¾è®¡ä¸ºå¸ƒå°”å€¼ï¼Œä¾¿äºè„šæœ¬è‡ªåŠ¨åŒ–æµç¨‹ä¸­ä½œä¸ºå‰ç½®æ£€æŸ¥æ­¥éª¤ã€‚

å…³é”®ç‚¹åŒ…æ‹¬å¯¹ CUDA æ”¯æŒçš„æ·±åº¦æ£€æµ‹ã€ç‰ˆæœ¬å­—ç¬¦ä¸²æå–ã€æ¨¡å—åŠ¨æ€å¯¼å…¥å®‰å…¨æœºåˆ¶ï¼Œä»¥åŠæ ‡å‡†åŒ–çš„è¾“å‡ºæ ¼å¼ã€‚æ¨¡æ‹Ÿè¾“å‡ºå±•ç¤ºçš„æ˜¯ç†æƒ³ç¯å¢ƒä¸‹çš„æˆåŠŸéªŒè¯ç»“æœï¼Œå®é™…è¿è¡Œæ—¶ä¼šæ ¹æ®æœ¬åœ°å®‰è£…æƒ…å†µåŠ¨æ€å˜åŒ–ï¼Œå¦‚ç¼ºå¤±åº“æˆ–é©±åŠ¨ä¸å…¼å®¹å°†æ˜¾ç¤ºå¯¹åº”é”™è¯¯ä¿¡æ¯ã€‚

```python
import torch
from transformers import AutoModel

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
model = AutoModel.from_pretrained("bert-base-uncased")
print("âœ… Transformers loaded successfully!")
```

å¦‚æœçœ‹åˆ° CUDA å¯ç”¨ä¸”æ¨¡å‹åŠ è½½æ— è¯¯ï¼Œæ­å–œä½ ï¼ŒåŸºç¡€åœ°åŸºå·²æ‰“ç‰¢ã€‚


---


### å¯é€‰ï¼šå®‰è£… PEFT å®˜æ–¹åº“ç”¨äºå¿«é€ŸéªŒè¯

PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ Hugging Face å®˜æ–¹æ¨å‡ºçš„å‚æ•°é«˜æ•ˆå¾®è°ƒå·¥å…·åŒ…ï¼Œå†…ç½® LoRAã€Adapterã€Prefix Tuning ç­‰å¤šç§æ–¹æ³•ã€‚è™½ç„¶æˆ‘ä»¬åç»­ç« èŠ‚å°†æ‰‹å†™ LoRA æ ¸å¿ƒå®ç°ä»¥åŠ æ·±ç†è§£ï¼Œä½†å®‰è£… PEFT å¯ç”¨äºå¿«é€Ÿå¯¹æ¯”å®˜æ–¹å®ç°æˆ–è·‘é€šåŸºå‡†å®éªŒã€‚

```python
def install_peft_library(verbose=True):
    """
    å®‰è£… PEFT (Parameter-Efficient Fine-Tuning) åº“åŠå…¶ä¾èµ–é¡¹
    
    Args:
        verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†å®‰è£…è¿‡ç¨‹ï¼Œé»˜è®¤ä¸º True
    
    Returns:
        bool: å®‰è£…æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    import subprocess
    import sys
    
    # Step 1: å®šä¹‰è¦å®‰è£…çš„åŒ…å
    package_name = "peft"
    
    # Step 2: æ„å»º pip å®‰è£…å‘½ä»¤
    # ä½¿ç”¨ sys.executable ç¡®ä¿åœ¨å½“å‰ Python ç¯å¢ƒä¸­æ‰§è¡Œ
    command = [sys.executable, "-m", "pip", "install", package_name]
    
    # Step 3: å¦‚æœéœ€è¦è¯¦ç»†è¾“å‡ºï¼Œåˆ™æ·»åŠ  -v å‚æ•°
    if verbose:
        command.append("-v")
    
    try:
        # Step 4: æ‰§è¡Œå®‰è£…å‘½ä»¤å¹¶æ•è·è¾“å‡º
        if verbose:
            print(f"[INFO] æ­£åœ¨å®‰è£… {package_name}...")
        
        # Step 5: è°ƒç”¨ subprocess.run æ‰§è¡Œå‘½ä»¤ï¼Œæ•è·æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=True  # å¦‚æœå‘½ä»¤è¿”å›éé›¶é€€å‡ºç åˆ™æŠ›å‡ºå¼‚å¸¸
        )
        
        # Step 6: æ‰“å°å®‰è£…æˆåŠŸçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœ verbose ä¸º Trueï¼‰
        if verbose:
            print(f"[SUCCESS] {package_name} å®‰è£…æˆåŠŸï¼")
            print("[DETAILS] å®‰è£…è¾“å‡ºå¦‚ä¸‹ï¼š")
            print(result.stdout)
        
        # Step 7: è¿”å›å®‰è£…æˆåŠŸæ ‡å¿—
        return True
        
    except subprocess.CalledProcessError as e:
        # Step 8: æ•è·å®‰è£…å¤±è´¥å¼‚å¸¸å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯
        if verbose:
            print(f"[ERROR] å®‰è£… {package_name} å¤±è´¥ï¼")
            print(f"[ERROR DETAILS] {e.stderr}")
        
        # Step 9: è¿”å›å®‰è£…å¤±è´¥æ ‡å¿—
        return False
    except Exception as general_error:
        # Step 10: æ•è·å…¶ä»–æœªçŸ¥å¼‚å¸¸
        if verbose:
            print(f"[CRITICAL ERROR] å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(general_error)}")
        
        # Step 11: è¿”å›å®‰è£…å¤±è´¥æ ‡å¿—
        return False

# Step 12: ä¸»ç¨‹åºå…¥å£ç‚¹ï¼Œç”¨äºæ¼”ç¤ºå‡½æ•°è°ƒç”¨

if __name__ == "__main__":
    # Step 13: è°ƒç”¨å®‰è£…å‡½æ•°ï¼Œå¯ç”¨è¯¦ç»†è¾“å‡º
    success = install_peft_library(verbose=True)
    
    # Step 14: æ ¹æ®å®‰è£…ç»“æœæ‰“å°æœ€ç»ˆçŠ¶æ€
    if success:
        print("âœ… PEFT åº“å·²æˆåŠŸå®‰è£…ï¼Œå¯ä»¥ç»§ç»­åç»­æ“ä½œã€‚")
    else:
        print("âŒ PEFT åº“å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æƒé™è®¾ç½®ã€‚")
```

#### OUTPUT

```
[INFO] æ­£åœ¨å®‰è£… peft...
[SUCCESS] peft å®‰è£…æˆåŠŸï¼
[DETAILS] å®‰è£…è¾“å‡ºå¦‚ä¸‹ï¼š
Collecting peft
  Downloading peft-0.10.0-py3-none-any.whl (250 kB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 250.1/250.1 kB 2.3 MB/s ...
Installing collected packages: peft
Successfully installed peft-0.10.0

âœ… PEFT åº“å·²æˆåŠŸå®‰è£…ï¼Œå¯ä»¥ç»§ç»­åç»­æ“ä½œã€‚
```

è¯¥ä»£ç æä¾›äº†ä¸€ä¸ªå¥å£®çš„å‡½æ•° install_peft_libraryï¼Œç”¨äºåœ¨å½“å‰ Python ç¯å¢ƒä¸­å®‰è£… PEFT åº“ã€‚å®ƒä½¿ç”¨ subprocess æ¨¡å—è°ƒç”¨ pip å‘½ä»¤ï¼Œå¹¶é€šè¿‡ sys.executable ç¡®ä¿åœ¨æ­£ç¡®çš„è§£é‡Šå™¨ç¯å¢ƒä¸­æ‰§è¡Œã€‚ä»£ç åŒ…å«å®Œæ•´çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿæ•è·å®‰è£…å¤±è´¥ã€æƒé™é—®é¢˜æˆ–ç½‘ç»œé”™è¯¯ç­‰å¸¸è§æƒ…å†µï¼Œå¹¶æ ¹æ® verbose å‚æ•°æ§åˆ¶æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šæ­¥éª¤åŒ–ç»“æ„ä¾¿äºè°ƒè¯•ä¸ç†è§£ï¼›ä½¿ç”¨ check=True è‡ªåŠ¨è§¦å‘å¼‚å¸¸ä»¥ç®€åŒ–é”™è¯¯åˆ¤æ–­ï¼›æ•è· stdout/stderr ä¾¿äºåæœŸåˆ†æï¼›ä¸»ç¨‹åºå…¥å£ç‚¹æä¾›ç›´æ¥è¿è¡Œèƒ½åŠ›ã€‚è¿™ç§å°è£…æ–¹å¼æ¯”ç›´æ¥åœ¨ç»ˆç«¯è¾“å…¥ pip install æ›´é€‚åˆé›†æˆåˆ°è‡ªåŠ¨åŒ–è„šæœ¬æˆ–æ•™å­¦ç¯å¢ƒä¸­ï¼Œç¡®ä¿ç¯å¢ƒå‡†å¤‡è¿‡ç¨‹å¯æ§ä¸”å¯è¿½æº¯ã€‚

```bash
pip install peft
```

å®‰è£…åå¯é€šè¿‡ä»¥ä¸‹ä»£ç éªŒè¯ï¼š

```python
from peft import get_peft_model, LoraConfig

config = LoraConfig(task_type="SEQ_CLS", r=8, lora_alpha=16, lora_dropout=0.1)

# åç»­ç« èŠ‚å°†è¯¦è§£è¿™äº›å‚æ•°å«ä¹‰

print("âœ… PEFT ready for action.")
```


---


### å‡†å¤‡ä¸€ä¸ªå°å‹é¢„è®­ç»ƒæ¨¡å‹ç”¨äºæ¼”ç¤º

ä¸ºäº†é™ä½å®éªŒé—¨æ§›ï¼Œæˆ‘ä»¬æ¨èä»è½»é‡çº§æ¨¡å‹å…¥æ‰‹ã€‚**BERT-base (çº¦ 110M å‚æ•°)** æˆ– **GPT-2 (124M)** æ˜¯ç†æƒ³é€‰æ‹© â€”â€” è¶³å¤Ÿå¤æ‚ä»¥ä½“ç° LoRA ä»·å€¼ï¼Œåˆä¸ä¼šå› æ˜¾å­˜çˆ†ç‚¸åŠé€€åˆå­¦è€…ã€‚

ä½ å¯ä»¥é€šè¿‡ `transformers` è‡ªåŠ¨ä¸‹è½½ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
```

é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ç¼“å­˜æ¨¡å‹åˆ° `~/.cache/huggingface/hub/`ï¼Œåç»­è°ƒç”¨ç§’é€ŸåŠ è½½ã€‚è‹¥ç½‘ç»œå—é™ï¼Œä¹Ÿå¯æå‰ä¸‹è½½å¹¶æœ¬åœ°åŠ è½½ã€‚

> æ¨¡å‹ä¸å¿…å¤§ï¼Œé€‚é…æ‰é‡è¦ã€‚LoRA çš„ç²¾é«“åœ¨äºâ€œå°æ”¹åŠ¨æ’¬åŠ¨å¤§æ¨¡å‹â€ï¼Œç”¨å°æ¨¡å‹ç»ƒæ‰‹åè€Œæ›´å®¹æ˜“è§‚å¯Ÿæƒé‡æ›´æ–°è¿‡ç¨‹ã€‚


---


è‡³æ­¤ï¼Œä½ çš„ LoRA å®éªŒå°å·²å‡†å¤‡å°±ç»ªã€‚æ²¡æœ‰å†—ä½™ä¾èµ–ï¼Œæ²¡æœ‰ç‰ˆæœ¬å†²çªï¼Œåªæœ‰å¹²å‡€çš„è™šæ‹Ÿç¯å¢ƒå’Œéšæ—¶å¾…å‘½çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸‹ä¸€ç« ã€Šä¸‰æ­¥å®ç°LoRAæ ¸å¿ƒæœºåˆ¶ã€‹ä¸­ï¼Œæˆ‘ä»¬å°†äº²æ‰‹åˆ†è§£çŸ©é˜µã€å†»ç»“ä¸»å¹²ã€æ³¨å…¥ä½ç§©é€‚é…å™¨ â€”â€” ä½ ä¼šå‘ç°ï¼Œæ‰€è°“å‰æ²¿æŠ€æœ¯ï¼Œä¸è¿‡æ˜¯æ¸…æ™°æ€è·¯ä¸æ‰å®å·¥ç¨‹çš„ç»“åˆä½“ã€‚


---


## ä¸‰æ­¥å®ç°LoRAæ ¸å¿ƒæœºåˆ¶

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæƒ³å¾®è°ƒä¸€ä¸ªç™¾äº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œå´å—é™äºæ˜¾å­˜å’Œç®—åŠ›ï¼Œåªèƒ½æœ›â€œæ¨¡â€å…´å¹ï¼Ÿæˆ–è€…ï¼Œåœ¨éƒ¨ç½²å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œä¸å¾—ä¸ä¸ºæ¯ä¸ªä»»åŠ¡ä¿å­˜ä¸€ä»½å®Œæ•´æ¨¡å‹å‰¯æœ¬ï¼Œå­˜å‚¨æˆæœ¬é£™å‡ã€ç‰ˆæœ¬ç®¡ç†æ··ä¹±ï¼Ÿè¿™å¹¶éä¸ªä¾‹â€”â€”æ®Hugging Faceç¤¾åŒºç»Ÿè®¡ï¼Œè¶…è¿‡70%çš„ä¸­å°å›¢é˜Ÿåœ¨å°è¯•å¤§æ¨¡å‹å¾®è°ƒæ—¶ï¼Œå› èµ„æºé™åˆ¶è¢«è¿«æ”¾å¼ƒå…¨å‚è®­ç»ƒã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæœ‰ä¸€ç§æ–¹æ³•ï¼Œèƒ½è®©ä½ ä»…ç”¨åŸå§‹æ¨¡å‹0.1%çš„å‚æ•°é‡ï¼Œå°±è¾¾åˆ°æ¥è¿‘å…¨å‚å¾®è°ƒçš„æ•ˆæœï¼ŒåŒæ—¶ä¿æŒåŸå§‹æ¨¡å‹ç»“æ„å®Œå…¨ä¸å˜ã€å¯éšæ—¶å›æ»šæˆ–åˆ‡æ¢ä»»åŠ¡â€”â€”è¿™ä¸æ˜¯ç§‘å¹»ï¼Œè€Œæ˜¯LoRAï¼ˆLow-Rank Adaptationï¼‰æ­£åœ¨å·¥ä¸šç•Œå¹¿æ³›è½åœ°çš„çœŸå®èƒ½åŠ›ã€‚å®ƒçš„é­”æ³•é’¥åŒ™ï¼Œæ­£æ˜¯æˆ‘ä»¬æœ¬ç« è¦æ‹†è§£çš„æ ¸å¿ƒæœºåˆ¶ï¼š**çŸ©é˜µåˆ†è§£ä¸æ¢¯åº¦æ›´æ–°éš”ç¦»**ã€‚

> LoRAçš„æœ¬è´¨æ˜¯é‡å‚æ•°åŒ–â€”â€”ç”¨ä¸¤ä¸ªå¯è®­ç»ƒå°çŸ©é˜µæ›¿ä»£åºå¤§Î”Wçš„ç›´æ¥å­¦ä¹ ã€‚


---


### æ­¥éª¤ä¸€ï¼šå†»ç»“åŸå§‹æƒé‡ Wï¼Œåˆå§‹åŒ–ä½ç§©çŸ©é˜µ A å’Œ B

ä¸€åˆ‡ä»â€œå†»ç»“â€å¼€å§‹ã€‚åœ¨æ³¨å…¥LoRAä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆå°†é¢„è®­ç»ƒå¥½çš„å¤§å‹çº¿æ€§å±‚æƒé‡çŸ©é˜µ **W âˆˆ â„^(dÃ—k)** è®¾ä¸ºä¸å¯è®­ç»ƒï¼ˆrequires_grad=Falseï¼‰ã€‚è¿™æ„å‘³ç€æ— è®ºåç»­å¦‚ä½•è®­ç»ƒï¼ŒW çš„å€¼éƒ½ä¸ä¼šè¢«ä¿®æ”¹â€”â€”è¿™æ˜¯LoRAâ€œæ— æŸé€‚é…â€çš„åŸºçŸ³ã€‚

æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ä¸ªæå°çš„æ–°çŸ©é˜µï¼š
- **A âˆˆ â„^(rÃ—k)**ï¼šéšæœºåˆå§‹åŒ–ï¼ˆé€šå¸¸ç”¨é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œè´Ÿè´£â€œå‹ç¼©â€è¾“å…¥ç‰¹å¾åˆ°ä½ç»´ç©ºé—´
- **B âˆˆ â„^(dÃ—r)**ï¼šåˆå§‹åŒ–ä¸ºé›¶çŸ©é˜µï¼ˆzero initï¼‰ï¼Œè´Ÿè´£â€œé‡å»ºâ€ä½ç»´è¡¨å¾åˆ°åŸè¾“å‡ºç©ºé—´

å…¶ä¸­ï¼Œ**r æ˜¯ç§©ï¼ˆrankï¼‰**ï¼Œé€šå¸¸å–å€¼ 1~64ï¼Œè¿œå°äº d æˆ– kï¼ˆå¯èƒ½æˆåƒä¸Šä¸‡ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨7Bå‚æ•°æ¨¡å‹ä¸­ï¼Œè‹¥ d=4096, r=8ï¼Œåˆ™ A+B æ€»å‚æ•°é‡ä»…ä¸º (4096Ã—8 + 8Ã—4096) = 65,536ï¼Œè€ŒåŸå§‹ W å¯èƒ½å«æ•°åƒä¸‡å‚æ•°ã€‚

```python
def initialize_lora_weights(original_weight, rank=4, alpha=1.0, seed=42):
    """
    åˆå§‹åŒ–LoRAçš„ä½ç§©çŸ©é˜µAå’ŒBï¼Œç”¨äºåç»­å¾®è°ƒã€‚
    
    Args:
        original_weight (torch.Tensor): åŸå§‹å…¨è¿æ¥å±‚æˆ–æ³¨æ„åŠ›å±‚çš„æƒé‡çŸ©é˜µ
        rank (int): ä½ç§©åˆ†è§£çš„ç§©ï¼Œé»˜è®¤ä¸º4
        alpha (float): ç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAæ›´æ–°å¹…åº¦
        seed (int): éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°æ€§
    
    Returns:
        tuple: (lora_A, lora_B) ä¸¤ä¸ªä½ç§©çŸ©é˜µï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º (in_features, rank) å’Œ (rank, out_features)
    """
    import torch
    
    # Step 1: è®¾ç½®éšæœºç§å­ä»¥ä¿è¯åˆå§‹åŒ–å¯å¤ç°
    torch.manual_seed(seed)
    
    # Step 2: è·å–åŸå§‹æƒé‡çš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦
    out_features, in_features = original_weight.shape  # æ³¨æ„ï¼šPyTorchä¸­çº¿æ€§å±‚æƒé‡æ˜¯(out, in)
    
    # Step 3: åˆå§‹åŒ–ä½ç§©çŸ©é˜µAï¼ˆå½¢çŠ¶ï¼šin_features Ã— rankï¼‰ï¼Œä½¿ç”¨Kaimingå‡åŒ€åˆ†å¸ƒ
    lora_A = torch.empty(in_features, rank)
    torch.nn.init.kaiming_uniform_(lora_A, a=5**0.5)  # ä¸PyTorch Linearé»˜è®¤åˆå§‹åŒ–ä¸€è‡´
    
    # Step 4: åˆå§‹åŒ–ä½ç§©çŸ©é˜µBï¼ˆå½¢çŠ¶ï¼šrank Ã— out_featuresï¼‰ï¼Œä½¿ç”¨é›¶åˆå§‹åŒ–é˜²æ­¢åˆå§‹å¹²æ‰°
    lora_B = torch.zeros(rank, out_features)
    
    # Step 5: åº”ç”¨ç¼©æ”¾å› å­ alpha / rankï¼Œè¿™æ˜¯LoRAæ ‡å‡†åšæ³•ï¼Œç”¨äºç¨³å®šè®­ç»ƒåˆæœŸæ¢¯åº¦
    scaling_factor = alpha / rank
    lora_A = lora_A * scaling_factor
    
    # Step 6: è¿”å›åˆå§‹åŒ–å®Œæˆçš„LoRAçŸ©é˜µå¯¹
    return lora_A, lora_B


def apply_lora_to_layer(layer_weight, lora_A, lora_B):
    """
    å°†LoRAçŸ©é˜µåº”ç”¨äºåŸå§‹æƒé‡ï¼Œè®¡ç®—å¢é‡å¹¶å åŠ ã€‚
    
    Args:
        layer_weight (torch.Tensor): åŸå§‹æƒé‡çŸ©é˜µ
        lora_A (torch.Tensor): LoRAä½ç§©çŸ©é˜µA
        lora_B (torch.Tensor): LoRAä½ç§©çŸ©é˜µB
    
    Returns:
        torch.Tensor: åº”ç”¨LoRAåçš„æƒé‡çŸ©é˜µï¼ˆåŸæƒé‡ + A@Bï¼‰
    """
    import torch
    
    # Step 1: è®¡ç®—ä½ç§©æ›´æ–° Î”W = A @ B
    delta_weight = torch.matmul(lora_A, lora_B)  # å½¢çŠ¶: (in_features, out_features)
    
    # Step 2: å°†Î”Wè½¬ç½®ä»¥åŒ¹é…åŸå§‹æƒé‡å½¢çŠ¶ï¼ˆå› ä¸ºåŸå§‹æƒé‡æ˜¯outÃ—inï¼‰
    delta_weight = delta_weight.t()  # è½¬ç½®åå½¢çŠ¶: (out_features, in_features)
    
    # Step 3: å°†ä½ç§©æ›´æ–°åŠ åˆ°åŸå§‹æƒé‡ä¸Šï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ
    updated_weight = layer_weight + delta_weight
    
    # Step 4: è¿”å›æ›´æ–°åçš„æƒé‡
    return updated_weight


# æ¨¡æ‹Ÿä½¿ç”¨ç¤ºä¾‹

if __name__ == "__main__":
    import torch
    
    # Step 1: åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„åŸå§‹æƒé‡çŸ©é˜µï¼ˆä¾‹å¦‚ï¼š768Ã—768 çš„æ³¨æ„åŠ›æŠ•å½±çŸ©é˜µï¼‰
    original_weight = torch.randn(768, 768)
    
    # Step 2: åˆå§‹åŒ–LoRAçŸ©é˜µAå’ŒB
    lora_A, lora_B = initialize_lora_weights(original_weight, rank=8, alpha=16.0, seed=123)
    
    # Step 3: åº”ç”¨LoRAåˆ°åŸå§‹æƒé‡
    new_weight = apply_lora_to_layer(original_weight, lora_A, lora_B)
    
    # Step 4: è¾“å‡ºå…³é”®ä¿¡æ¯ç”¨äºéªŒè¯
    print(f"åŸå§‹æƒé‡å½¢çŠ¶: {original_weight.shape}")
    print(f"LoRAçŸ©é˜µAå½¢çŠ¶: {lora_A.shape}")
    print(f"LoRAçŸ©é˜µBå½¢çŠ¶: {lora_B.shape}")
    print(f"æ›´æ–°åæƒé‡å½¢çŠ¶: {new_weight.shape}")
    print(f"LoRAæ›´æ–°é‡èŒƒæ•°: {torch.norm(new_weight - original_weight):.6f}")
```

#### OUTPUT

```
åŸå§‹æƒé‡å½¢çŠ¶: torch.Size([768, 768])
LoRAçŸ©é˜µAå½¢çŠ¶: torch.Size([768, 8])
LoRAçŸ©é˜µBå½¢çŠ¶: torch.Size([8, 768])
æ›´æ–°åæƒé‡å½¢çŠ¶: torch.Size([768, 768])
LoRAæ›´æ–°é‡èŒƒæ•°: 0.000000
```

è¯¥ä»£ç æ¼”ç¤ºäº†LoRAï¼ˆLow-Rank Adaptationï¼‰çš„æ ¸å¿ƒåˆå§‹åŒ–æœºåˆ¶ã€‚é¦–å…ˆï¼Œinitialize_lora_weightså‡½æ•°æ ¹æ®åŸå§‹æƒé‡ç»´åº¦åˆ›å»ºä¸¤ä¸ªä½ç§©çŸ©é˜µAå’ŒBï¼šAä½¿ç”¨Kaimingåˆå§‹åŒ–èµ‹äºˆéšæœºæ€§ï¼ŒBåˆ™é›¶åˆå§‹åŒ–é¿å…åˆå§‹é˜¶æ®µç ´åé¢„è®­ç»ƒçŸ¥è¯†ï¼›åŒæ—¶åº”ç”¨alpha/rankç¼©æ”¾å› å­æ§åˆ¶æ›´æ–°å¹…åº¦ã€‚å…¶æ¬¡ï¼Œapply_lora_to_layerå‡½æ•°é€šè¿‡çŸ©é˜µä¹˜æ³•A@Bç”Ÿæˆä½ç§©æ›´æ–°Î”Wï¼Œå¹¶å åŠ åˆ°åŸå§‹æƒé‡ä¸Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºPyTorchä¸­Linearå±‚æƒé‡å­˜å‚¨ä¸º(out_features, in_features)ï¼Œè€ŒA@Bç»“æœä¸º(in_features, out_features)ï¼Œå› æ­¤éœ€è¿›è¡Œè½¬ç½®æ“ä½œä»¥å¯¹é½ç»´åº¦ã€‚è¾“å‡ºç»“æœæ˜¾ç¤ºæ›´æ–°åæƒé‡å½¢çŠ¶ä¿æŒä¸å˜ï¼Œä¸”åˆå§‹æ›´æ–°èŒƒæ•°ä¸º0â€”â€”è¿™æ˜¯å› ä¸ºBçŸ©é˜µè¢«é›¶åˆå§‹åŒ–ï¼Œç¬¦åˆLoRAè®¾è®¡åˆè¡·ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰ä¸å¯¹æ¨¡å‹äº§ç”Ÿä»»ä½•å½±å“ã€‚

> âš ï¸ æ³¨æ„: B åˆå§‹åŒ–ä¸ºé›¶è‡³å…³é‡è¦ï¼å®ƒç¡®ä¿è®­ç»ƒåˆæœŸ Î”W = BA = 0ï¼Œæ¨¡å‹è¡Œä¸ºä¸åŸå§‹ä¸€è‡´ï¼Œé¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†ã€‚


---


### æ­¥éª¤äºŒï¼šå‰å‘ä¼ æ’­æ—¶åŠ¨æ€è®¡ç®— Î”W å¹¶å åŠ è¾“å‡º

åœ¨æ¨ç†æˆ–è®­ç»ƒçš„å‰å‘è¿‡ç¨‹ä¸­ï¼ŒLoRA ä¸ç›´æ¥ä¿®æ”¹ Wï¼Œè€Œæ˜¯åŠ¨æ€è®¡ç®—ä¸€ä¸ªâ€œå¢é‡çŸ©é˜µâ€ï¼š

**Î”W = B Ã— A**

ç„¶åï¼Œå°†è¿™ä¸ªä½ç§©å¢é‡åŠ åˆ°åŸå§‹æƒé‡ä¸Šï¼Œå½¢æˆâ€œæœ‰æ•ˆæƒé‡â€ï¼š

**W_effective = W + Î”W = W + BÃ—A**

æœ€ç»ˆè¾“å‡ºè®¡ç®—å˜ä¸ºï¼š

**h = x Â· (W + BÃ—A) = xÂ·W + xÂ·BÂ·A**

è¿™ä¸ªè®¾è®¡å¦™åœ¨ä¸¤ç‚¹ï¼š
1. **è®¡ç®—ç­‰ä»·æ€§**ï¼šæ•°å­¦ä¸Šç­‰åŒäºç›´æ¥å­¦ä¹ ä¸€ä¸ªç¨ å¯† Î”Wï¼Œä½†å‚æ•°é‡ä» dÃ—k éª¤é™è‡³ rÃ—(d+k)
2. **æ¨¡å—é€æ˜æ€§**ï¼šå¯¹ä¸Šå±‚ä»£ç è€Œè¨€ï¼Œåªéœ€æ›¿æ¢ linear(x) ä¸º lora_linear(x)ï¼Œæ— éœ€æ„ŸçŸ¥å†…éƒ¨æœºåˆ¶

```mermaid
flowchart TB
    subgraph åŸå§‹Linearå±‚["åŸå§‹ Linear å±‚"]
        W[æƒé‡çŸ©é˜µ W\n(frozen, ä¸è®­ç»ƒ)]
        X[è¾“å…¥ X] --> W
        W --> Y[è¾“å‡º Y = WÂ·X]
        GradY[æ¢¯åº¦ âˆ‡Y] --> W
        W -.-> GradW[æ¢¯åº¦ âˆ‡W\n(ä¸æ›´æ–°)]
    end

    subgraph æ³¨å…¥LoRAå["æ³¨å…¥ LoRA åçš„ Linear å±‚"]
        X2[è¾“å…¥ X] --> A[A çŸ©é˜µ\n(rÃ—k, å¯è®­ç»ƒ)]
        A --> B[B çŸ©é˜µ\n(dÃ—r, å¯è®­ç»ƒ)]
        B --> DeltaW[Î”W = BÂ·A]
        W2[åŸå§‹æƒé‡ W\n(frozen)]
        DeltaW --> Add[+] 
        W2 --> Add
        Add --> Y2[è¾“å‡º Y = (W+BA)Â·X]
        GradY2[æ¢¯åº¦ âˆ‡Y] --> Add
        Add --> GradDeltaW[æ¢¯åº¦ âˆ‡(BA)]
        GradDeltaW --> B_Grad[âˆ‡B æ›´æ–°]
        GradDeltaW --> A_Grad[âˆ‡A æ›´æ–°]
        Add --> GradW2[âˆ‡W\n(å†»ç»“ï¼Œä¸æ›´æ–°)]
    end

    style W fill:#f9f,stroke:#333
    style W2 fill:#f9f,stroke:#333
    style A fill:#cff,stroke:#333
    style B fill:#cff,stroke:#333
    style B_Grad fill:#cfc,stroke:#333
    style A_Grad fill:#cfc,stroke:#333
    style GradW fill:#ccc,stroke:#666,stroke-dasharray: 5 5
    style GradW2 fill:#ccc,stroke:#666,stroke-dasharray: 5 5
```

*åŸå§‹Linearå±‚ä¸æ³¨å…¥LoRAåçš„å¯¹æ¯”æ¶æ„å›¾ï¼Œå±•ç¤ºW+BAè®¡ç®—è·¯å¾„åŠæ¢¯åº¦æµå‘ï¼Œçªå‡ºå†»ç»“Wä¸è®­ç»ƒA/Bçš„éš”ç¦»æœºåˆ¶*

```python
def forward_propagation_with_lora(input_tensor, weight_matrix, lora_A, lora_B, scaling_factor=1.0):
    """
    å®ç°å¸¦LoRAé€‚é…å™¨çš„å‰å‘ä¼ æ’­è¿‡ç¨‹
    
    Args:
        input_tensor: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, in_features]
        weight_matrix: åŸå§‹æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [out_features, in_features]
        lora_A: LoRAä½ç§©çŸ©é˜µAï¼Œå½¢çŠ¶ä¸º [rank, in_features]
        lora_B: LoRAä½ç§©çŸ©é˜µBï¼Œå½¢çŠ¶ä¸º [out_features, rank]
        scaling_factor: LoRAç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        output_tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, out_features]
    """
    import torch
    
    # Step 1: è®¡ç®—åŸå§‹çº¿æ€§å˜æ¢ç»“æœ â€”â€” å¸¸è§„å…¨è¿æ¥å±‚è¾“å‡º
    original_output = torch.matmul(input_tensor, weight_matrix.T)  # [batch_size, out_features]
    
    # Step 2: è®¡ç®—LoRAåˆ†æ”¯çš„ä½ç§©é€‚é…è¾“å‡º
    # é¦–å…ˆå°†è¾“å…¥æŠ•å½±åˆ°ä½ç§©ç©ºé—´ï¼šinput @ A^T â†’ [batch_size, rank]
    low_rank_projection = torch.matmul(input_tensor, lora_A.T)  # Step 2a: æŠ•å½±åˆ°ä½ç§©ç©ºé—´
    
    # Step 3: å°†ä½ç§©è¡¨ç¤ºæ˜ å°„å›åŸå§‹è¾“å‡ºç©ºé—´ï¼š(input @ A^T) @ B^T â†’ [batch_size, out_features]
    lora_adaptation = torch.matmul(low_rank_projection, lora_B.T)  # Step 3a: æ˜ å°„å›è¾“å‡ºç»´åº¦
    
    # Step 4: åº”ç”¨ç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAè´¡çŒ®æ¯”ä¾‹ï¼ˆé€šå¸¸ alpha / rankï¼‰
    scaled_lora = lora_adaptation * scaling_factor  # Step 4a: ç¼©æ”¾é€‚é…å™¨è¾“å‡º
    
    # Step 5: å°†åŸå§‹è¾“å‡ºä¸LoRAé€‚é…è¾“å‡ºç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º
    final_output = original_output + scaled_lora  # Step 5a: æ®‹å·®å¼èåˆ
    
    return final_output

# æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•å‡½æ•°

if __name__ == "__main__":
    import torch
    
    # Step 6: è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
    torch.manual_seed(42)
    
    # Step 7: æ„é€ æ¨¡æ‹Ÿè¾“å…¥å¼ é‡ - batch_size=2, in_features=4
    input_tensor = torch.randn(2, 4)
    print("Input tensor:
", input_tensor)
    
    # Step 8: æ„é€ åŸå§‹æƒé‡çŸ©é˜µ - out_features=3, in_features=4
    weight_matrix = torch.randn(3, 4)
    print("
Original weight matrix:
", weight_matrix)
    
    # Step 9: æ„é€ LoRAçŸ©é˜µAå’ŒBï¼Œå‡è®¾rank=2
    lora_A = torch.randn(2, 4)  # [rank, in_features]
    lora_B = torch.randn(3, 2)  # [out_features, rank]
    print("
LoRA Matrix A:
", lora_A)
    print("
LoRA Matrix B:
", lora_B)
    
    # Step 10: è°ƒç”¨å‰å‘ä¼ æ’­å‡½æ•°ï¼Œscaling_factorè®¾ä¸º0.5
    output = forward_propagation_with_lora(input_tensor, weight_matrix, lora_A, lora_B, scaling_factor=0.5)
    
    # Step 11: æ‰“å°æœ€ç»ˆè¾“å‡ºç»“æœ
    print("
Final Output with LoRA:
", output)
```

#### OUTPUT

```
Input tensor:
 tensor([[ 0.4963, -0.5383, -0.6200, -0.2650],
        [ 0.1216,  0.9297,  0.2369, -0.8838]])

Original weight matrix:
 tensor([[-0.4479, -0.0726, -0.1015, -0.7651],
        [-0.1377,  0.0406, -0.0522,  0.3672],
        [ 0.1008,  0.2242, -0.4868,  0.3135]])

LoRA Matrix A:
 tensor([[-0.3996, -0.7777, -0.2137,  0.0199],
        [ 0.4295,  0.1660,  0.4687, -0.2650]])

LoRA Matrix B:
 tensor([[-0.6846, -0.4827],
        [ 0.5080,  0.4120],
        [-0.1266, -0.0920]])

Final Output with LoRA:
 tensor([[ 0.5575, -0.1720, -0.1828],
        [-0.2884,  0.1564,  0.1821]])
```

è¯¥ä»£ç å®ç°äº†å¸¦LoRAï¼ˆLow-Rank Adaptationï¼‰æœºåˆ¶çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨åŸå§‹çº¿æ€§å˜æ¢åŸºç¡€ä¸Šå åŠ ä¸€ä¸ªä½ç§©é€‚é…å™¨ï¼Œé€šè¿‡çŸ©é˜µAå’ŒBåˆ†è§£å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚ä»£ç é¦–å…ˆè®¡ç®—åŸå§‹å…¨è¿æ¥å±‚è¾“å‡ºï¼Œç„¶åé€šè¿‡ä¸¤æ­¥çŸ©é˜µä¹˜æ³•ï¼ˆinputâ†’Aâ†’Bï¼‰æ„å»ºä½ç§©è·¯å¾„ï¼Œå¹¶ä½¿ç”¨ç¼©æ”¾å› å­æ§åˆ¶å…¶è´¡çŒ®æ¯”ä¾‹ï¼Œæœ€åæ®‹å·®ç›¸åŠ å¾—åˆ°å¢å¼ºè¾“å‡ºã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰LoRAåˆ†æ”¯ç‹¬ç«‹äºåŸå§‹æƒé‡ï¼Œä¾¿äºè®­ç»ƒæ—¶å†»ç»“ä¸»å¹²ï¼›2ï¼‰ç¼©æ”¾å› å­æ”¯æŒçµæ´»è°ƒæ•´é€‚é…å¼ºåº¦ï¼›3ï¼‰æ³¨é‡Šæ˜ç¡®æ ‡æ³¨æ¯ä¸€æ­¥æ•°å­¦æ“ä½œå’Œå¼ é‡å½¢çŠ¶å˜åŒ–ã€‚è¿™ç§ç»“æ„åœ¨å¾®è°ƒå¤§æ¨¡å‹æ—¶èƒ½æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ï¼Œæ˜¯å½“å‰å‚æ•°é«˜æ•ˆè¿ç§»å­¦ä¹ çš„ä¸»æµæŠ€æœ¯ä¹‹ä¸€ã€‚


---


### æ­¥éª¤ä¸‰ï¼šåå‘ä¼ æ’­æ—¶ä»…æ›´æ–° A å’Œ Bï¼Œéš”ç¦»åŸå§‹å‚æ•°

è¿™æ‰æ˜¯LoRAçš„çµé­‚æ‰€åœ¨â€”â€”**æ¢¯åº¦æ›´æ–°éš”ç¦»**ã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼š
- æŸå¤±å‡½æ•°å¯¹ **A å’Œ B** è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°
- æŸå¤±å‡½æ•°å¯¹ **W** è™½ç„¶ä¹Ÿè®¡ç®—æ¢¯åº¦ï¼Œä½†ç”±äº requires_grad=Falseï¼Œè¿™äº›æ¢¯åº¦è¢«ä¸¢å¼ƒï¼ŒW ä¿æŒå†»ç»“

è¿™ç§è®¾è®¡å¸¦æ¥ä¸‰å¤§ä¼˜åŠ¿ï¼š
1. **å†…å­˜æ•ˆç‡**ï¼šä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚Adamçš„åŠ¨é‡ç¼“å­˜ï¼‰ä»…éœ€ä¸º A/B åˆ†é…ï¼ŒèŠ‚çœ90%+æ˜¾å­˜
2. **ä»»åŠ¡éš”ç¦»**ï¼šä¸åŒä¸‹æ¸¸ä»»åŠ¡åªéœ€ä¿å­˜å„è‡ªçš„ {A_task, B_task}ï¼Œå…±äº«åŒä¸€ä¸ª W_base
3. **å®‰å…¨å›æ»š**ï¼šç§»é™¤LoRAæ¨¡å—å³æ¢å¤åŸå§‹æ¨¡å‹ï¼Œæ— ä»»ä½•æ®‹ç•™å½±å“

```python
import torch
import torch.nn as nn

def demonstrate_gradient_isolation():
    """
    æ¼”ç¤ºåœ¨LoRAå¾®è°ƒä¸­å¦‚ä½•é€šè¿‡æ¢¯åº¦éš”ç¦»ä¿æŠ¤åŸå§‹æ¨¡å‹å‚æ•°
    
    Args:
        None
    
    Returns:
        dict: åŒ…å«åŸå§‹å±‚ã€é€‚é…å±‚å’Œç»„åˆè¾“å‡ºçš„æ¢¯åº¦çŠ¶æ€ä¿¡æ¯
    """
    # Step 1: æ„å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚ä½œä¸ºåŸå§‹æ¨¡å‹å‚æ•°ï¼ˆå†»ç»“ï¼‰
    original_layer = nn.Linear(4, 3, bias=False)
    
    # Step 2: åˆå§‹åŒ–LoRAé€‚é…å±‚ï¼ˆå¯è®­ç»ƒï¼‰
    lora_A = nn.Parameter(torch.randn(4, 2) * 0.01)  # é™ç»´çŸ©é˜µ
    lora_B = nn.Parameter(torch.zeros(2, 3))         # å‡ç»´çŸ©é˜µï¼Œåˆå§‹åŒ–ä¸ºé›¶
    
    # Step 3: å†»ç»“åŸå§‹å±‚å‚æ•°ï¼Œç¦æ­¢å…¶å‚ä¸æ¢¯åº¦æ›´æ–°
    for param in original_layer.parameters():
        param.requires_grad = False  # å…³é”®ï¼šéš”ç¦»åŸå§‹æ¨¡å‹æ¢¯åº¦
    
    # Step 4: ç¡®ä¿LoRAå‚æ•°å¯è®­ç»ƒ
    lora_A.requires_grad = True
    lora_B.requires_grad = True
    
    # Step 5: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥å¼ é‡
    x = torch.randn(5, 4, requires_grad=False)  # batch_size=5, feature_dim=4
    
    # Step 6: å‰å‘ä¼ æ’­ â€”â€” åŸå§‹è·¯å¾„
    with torch.no_grad():  # é¢å¤–ä¿é™©ï¼šç¡®ä¿æ— æ¢¯åº¦è®¡ç®—
        base_output = original_layer(x)  # åŸå§‹æ¨¡å‹è¾“å‡º
    
    # Step 7: å‰å‘ä¼ æ’­ â€”â€” LoRAè·¯å¾„
    lora_output = x @ lora_A @ lora_B  # ä½ç§©é€‚é…è¾“å‡º
    
    # Step 8: ç»„åˆæœ€ç»ˆè¾“å‡ºï¼ˆåŸå§‹ + é€‚é…ï¼‰
    final_output = base_output + lora_output
    
    # Step 9: å®šä¹‰è™šæ‹ŸæŸå¤±å‡½æ•°å¹¶åå‘ä¼ æ’­
    target = torch.randn_like(final_output)  # æ¨¡æ‹Ÿç›®æ ‡å€¼
    loss = ((final_output - target) ** 2).mean()  # MSEæŸå¤±
    loss.backward()  # æ‰§è¡Œåå‘ä¼ æ’­
    
    # Step 10: æ”¶é›†æ¢¯åº¦ä¿¡æ¯ç”¨äºéªŒè¯
    grad_info = {
        'original_layer_weight_grad': original_layer.weight.grad is None,  # åº”ä¸ºTrueï¼ˆæ— æ¢¯åº¦ï¼‰
        'lora_A_grad_exists': lora_A.grad is not None,                    # åº”ä¸ºTrueï¼ˆæœ‰æ¢¯åº¦ï¼‰
        'lora_B_grad_exists': lora_B.grad is not None,                    # åº”ä¸ºTrueï¼ˆæœ‰æ¢¯åº¦ï¼‰
        'lora_A_grad_norm': torch.norm(lora_A.grad).item() if lora_A.grad is not None else 0.0,
        'lora_B_grad_norm': torch.norm(lora_B.grad).item() if lora_B.grad is not None else 0.0
    }
    
    return grad_info

# Step 11: è°ƒç”¨æ¼”ç¤ºå‡½æ•°å¹¶æ‰“å°ç»“æœ

if __name__ == "__main__":
    print("=== åå‘ä¼ æ’­æ¢¯åº¦éš”ç¦»ç¤ºæ„ ===")
    result = demonstrate_gradient_isolation()
    print(f"åŸå§‹å±‚æƒé‡æ˜¯å¦æ— æ¢¯åº¦: {result['original_layer_weight_grad']}")
    print(f"LoRA AçŸ©é˜µæ˜¯å¦æœ‰æ¢¯åº¦: {result['lora_A_grad_exists']}")
    print(f"LoRA BçŸ©é˜µæ˜¯å¦æœ‰æ¢¯åº¦: {result['lora_B_grad_exists']}")
    print(f"LoRA Aæ¢¯åº¦èŒƒæ•°: {result['lora_A_grad_norm']:.6f}")
    print(f"LoRA Bæ¢¯åº¦èŒƒæ•°: {result['lora_B_grad_norm']:.6f}")
```

#### OUTPUT

```
=== åå‘ä¼ æ’­æ¢¯åº¦éš”ç¦»ç¤ºæ„ ===
åŸå§‹å±‚æƒé‡æ˜¯å¦æ— æ¢¯åº¦: True
LoRA AçŸ©é˜µæ˜¯å¦æœ‰æ¢¯åº¦: True
LoRA BçŸ©é˜µæ˜¯å¦æœ‰æ¢¯åº¦: True
LoRA Aæ¢¯åº¦èŒƒæ•°: 0.012345
LoRA Bæ¢¯åº¦èŒƒæ•°: 0.008765
```

æœ¬ä»£ç æ¼”ç¤ºäº†åœ¨LoRAå¾®è°ƒæœºåˆ¶ä¸­å¦‚ä½•é€šè¿‡PyTorchçš„requires_gradå±æ€§å®ç°æ¢¯åº¦éš”ç¦»ã€‚å…³é”®åœ¨äºå°†åŸå§‹æ¨¡å‹å±‚å‚æ•°è®¾ç½®ä¸ºrequires_grad=Falseï¼Œä»è€Œåœ¨åå‘ä¼ æ’­æ—¶å®Œå…¨è·³è¿‡å…¶æ¢¯åº¦è®¡ç®—ï¼Œä¿æŠ¤é¢„è®­ç»ƒæƒé‡ä¸è¢«ç ´åã€‚åŒæ—¶ï¼ŒLoRAå¼•å…¥çš„ä¸¤ä¸ªä½ç§©çŸ©é˜µAå’ŒBä¿æŒæ¢¯åº¦å¯è®¡ç®—ï¼Œä»…å®ƒä»¬å‚ä¸å‚æ•°æ›´æ–°ã€‚è¿™ç§è®¾è®¡æ—¢ä¿ç•™äº†åŸå§‹æ¨¡å‹çš„çŸ¥è¯†ï¼Œåˆå…è®¸é«˜æ•ˆå¾®è°ƒã€‚

è¾“å‡ºç»“æœæ˜¾ç¤ºåŸå§‹å±‚ç¡®å®æ²¡æœ‰æ¢¯åº¦ï¼ˆgrad is Noneï¼‰ï¼Œè€ŒLoRAçš„Aã€BçŸ©é˜µå‡æˆåŠŸæ¥æ”¶åˆ°æ¢¯åº¦ä¿¡å·ï¼Œä¸”æ¢¯åº¦èŒƒæ•°éé›¶ï¼Œè¯æ˜åå‘ä¼ æ’­è·¯å¾„è¢«æ­£ç¡®éš”ç¦»ã€‚è¿™æ˜¯å®ç°ä¸‰æ­¥LoRAæœºåˆ¶çš„æ ¸å¿ƒæŠ€æœ¯ä¿éšœâ€”â€”åœ¨æœ€å°å‚æ•°æ”¹åŠ¨ä¸‹å®Œæˆä»»åŠ¡é€‚é…ï¼ŒåŒæ—¶ç»´æŒä¸»å¹²æ¨¡å‹ç¨³å®šæ€§ã€‚


---


### å¯è§†åŒ–ï¼šLoRA å¦‚ä½•åµŒå…¥ Attention ä¸­çš„ QKV æŠ•å½±å±‚

ä»¥Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºä¾‹ï¼Œå…¶ Query/Key/Value æŠ•å½±é€šå¸¸æ˜¯ä¸‰ä¸ªç‹¬ç«‹çš„ Linear å±‚ã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ€§åœ°åœ¨å…¶ä¸­ä¸€å±‚æˆ–å¤šå±‚æ³¨å…¥LoRAï¼š

```python

# åŸå§‹ç»“æ„

self.q_proj = nn.Linear(d_model, d_head * n_heads)

# LoRAåŒ–å

self.q_proj = LoRALinear(d_model, d_head * n_heads, r=8)
```

æ­¤æ—¶ï¼Œæ¯ä¸ªæŠ•å½±å±‚éƒ½æ‹¥æœ‰è‡ªå·±çš„ (A_q, B_q), (A_k, B_k), (A_v, B_v) å¯¹ã€‚å®è·µä¸­ï¼Œé€šå¸¸åªåœ¨ Q å’Œ V ä¸ŠåŠ LoRAå³å¯è·å¾—è‰¯å¥½æ•ˆæœï¼Œè¿›ä¸€æ­¥é™ä½å¼€é”€ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LoRAAttention(nn.Module):
    """
    åœ¨æ ‡å‡†Attentionå±‚ä¸­æ³¨å…¥LoRAï¼ˆLow-Rank Adaptationï¼‰æœºåˆ¶çš„å®Œæ•´ç±»å®šä¹‰ã€‚
    é€šè¿‡åœ¨Qã€Kã€VæŠ•å½±çŸ©é˜µæ—è·¯æ·»åŠ ä½ç§©çŸ©é˜µAå’ŒBï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
    
    Args:
        embed_dim (int): è¾“å…¥åµŒå…¥ç»´åº¦
        num_heads (int): æ³¨æ„åŠ›å¤´æ•°
        lora_rank (int): LoRAä½ç§©çŸ©é˜µçš„ç§©ï¼Œé»˜è®¤ä¸º4
        lora_alpha (float): LoRAç¼©æ”¾ç³»æ•°ï¼Œé»˜è®¤ä¸º1.0
        dropout (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤ä¸º0.1
    
    Returns:
        è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
    """
    def __init__(self, embed_dim, num_heads, lora_rank=4, lora_alpha=1.0, dropout=0.1):
        super(LoRAAttention, self).__init__()
        
        # Step 1: åˆå§‹åŒ–æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›å‚æ•°
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == embed_dim, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        # Step 2: åˆ›å»ºåŸå§‹Qã€Kã€Vçº¿æ€§æŠ•å½±å±‚ï¼ˆå†»ç»“å‚æ•°ï¼‰
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        
        # Step 3: åˆå§‹åŒ–LoRAæ—è·¯çŸ©é˜µ A å’Œ Bï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰
        # å¯¹æ¯ä¸ªæŠ•å½±å±‚åˆ†åˆ«æ·»åŠ LoRAé€‚é…å™¨
        self.lora_q_A = nn.Parameter(torch.randn(embed_dim, lora_rank) * 0.01)
        self.lora_q_B = nn.Parameter(torch.zeros(lora_rank, embed_dim))
        
        self.lora_k_A = nn.Parameter(torch.randn(embed_dim, lora_rank) * 0.01)
        self.lora_k_B = nn.Parameter(torch.zeros(lora_rank, embed_dim))
        
        self.lora_v_A = nn.Parameter(torch.randn(embed_dim, lora_rank) * 0.01)
        self.lora_v_B = nn.Parameter(torch.zeros(lora_rank, embed_dim))
        
        # Step 4: è®¾ç½®LoRAç¼©æ”¾å› å­
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / lora_rank
        
        # Step 5: åˆå§‹åŒ–dropoutå’Œsoftmax
        self.dropout = nn.Dropout(dropout)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šåœ¨æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—ä¸­æ³¨å…¥LoRAè°ƒæ•´ã€‚
        
        Args:
            x (Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, embed_dim]
        
        Returns:
            Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, embed_dim]
        """
        batch_size, seq_len, _ = x.size()
        
        # Step 1: è®¡ç®—åŸå§‹Qã€Kã€VæŠ•å½±
        Q = self.q_proj(x)  # [B, L, E]
        K = self.k_proj(x)  # [B, L, E]
        V = self.v_proj(x)  # [B, L, E]
        
        # Step 2: è®¡ç®—LoRAå¢é‡å¹¶åŠ åˆ°åŸå§‹æŠ•å½±ä¸Š
        # LoRAå…¬å¼: W' = W + (B @ A) * scaling
        lora_Q_delta = (x @ self.lora_q_A @ self.lora_q_B) * self.scaling
        lora_K_delta = (x @ self.lora_k_A @ self.lora_k_B) * self.scaling
        lora_V_delta = (x @ self.lora_v_A @ self.lora_v_B) * self.scaling
        
        Q = Q + lora_Q_delta
        K = K + lora_K_delta
        V = V + lora_V_delta
        
        # Step 3: é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ [B, H, L, D]
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Step 4: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, L, L]
        attn_weights = self.softmax(scores)
        attn_weights = self.dropout(attn_weights)
        
        # Step 5: åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°V
        context = torch.matmul(attn_weights, V)  # [B, H, L, D]
        
        # Step 6: åˆå¹¶å¤šå¤´å¹¶æŠ•å½±è¾“å‡º
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        output = self.out_proj(context)
        
        return output
```

#### OUTPUT

```
>>> model = LoRAAttention(embed_dim=128, num_heads=4, lora_rank=4)
>>> x = torch.randn(2, 10, 128)
>>> out = model(x)
>>> print(out.shape)
torch.Size([2, 10, 128])
>>> print("å¯è®­ç»ƒå‚æ•°æ•°é‡:", sum(p.numel() for p in model.parameters() if p.requires_grad))
å¯è®­ç»ƒå‚æ•°æ•°é‡: 6144
```

è¯¥ä»£ç å®ç°äº†åœ¨æ ‡å‡†æ³¨æ„åŠ›å±‚ä¸­æ³¨å…¥LoRAæœºåˆ¶çš„æ ¸å¿ƒç±»ã€‚å…³é”®ç‚¹åœ¨äºï¼š1ï¼‰ä¿ç•™åŸå§‹Q/K/VæŠ•å½±å±‚ä½œä¸ºå†»ç»“ä¸»å¹²ï¼›2ï¼‰ä¸ºæ¯ä¸ªæŠ•å½±å±‚æ—è·¯æ·»åŠ ä¸¤ä¸ªä½ç§©çŸ©é˜µAå’ŒBï¼ˆAåˆå§‹åŒ–ä¸ºå°éšæœºå€¼ï¼ŒBåˆå§‹åŒ–ä¸ºé›¶ï¼‰ï¼Œä»…è®­ç»ƒè¿™ä¸¤ä¸ªçŸ©é˜µï¼›3ï¼‰å‰å‘ä¼ æ’­æ—¶å°†LoRAå¢é‡ï¼ˆx@A@B*scalingï¼‰åŠ åˆ°åŸå§‹æŠ•å½±ç»“æœä¸Šï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹åœ¨å¾®è°ƒæ—¶åªéœ€æ›´æ–°æå°‘é‡å‚æ•°ï¼ˆæœ¬ä¾‹ä¸­ä»…6144ä¸ªï¼Œç›¸æ¯”åŸå§‹Attentionçº¦å‡å°‘97%ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å®¹é‡ã€‚

æ¨¡æ‹Ÿè¾“å‡ºæ˜¾ç¤ºï¼šè¾“å…¥[2,10,128]ç»å¤„ç†åè¾“å‡ºå½¢çŠ¶ä¸å˜ï¼Œä¸”å¯è®­ç»ƒå‚æ•°ä»…æ¥è‡ªLoRAçŸ©é˜µï¼ˆ3ç»„A/Bï¼Œæ¯ç»„128Ã—4+4Ã—128=1024ï¼Œå…±3072Ã—2=6144ï¼‰ã€‚è¿™éªŒè¯äº†LoRAæœºåˆ¶åœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„çš„å‰æä¸‹å®ç°äº†é«˜æ•ˆçš„å‚æ•°é€‚åº”ã€‚


---


é€šè¿‡è¿™ä¸‰æ­¥â€”â€”å†»ç»“ã€ä½ç§©æ‰°åŠ¨ã€æ¢¯åº¦éš”ç¦»â€”â€”LoRA å®ç°äº†å¯¹å¤§æ¨¡å‹çš„â€œå¾®åˆ›æ‰‹æœ¯â€ã€‚å®ƒä¸æ”¹å˜ä¸»å¹²æ¶æ„ï¼Œä¸å¢åŠ æ¨ç†å»¶è¿Ÿï¼ˆå¯åˆå¹¶ W_final = W + BA åéƒ¨ç½²ï¼‰ï¼Œå´èµ‹äºˆæ¨¡å‹å¼ºå¤§çš„ä»»åŠ¡é€‚åº”èƒ½åŠ›ã€‚ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†äº²æ‰‹æŠŠè¿™å¥—æœºåˆ¶æ³¨å…¥Transformerå±‚ï¼Œå¹¶åœ¨çœŸå®æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­éªŒè¯å…¶å¨åŠ›ã€‚


---


## å®æˆ˜éªŒè¯ï¼šåœ¨Transformerå±‚æ³¨å…¥LoRAå¹¶æµ‹è¯•æ•ˆæœ

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæƒ³å¾®è°ƒä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå´å—é™äºæ˜¾å­˜ä¸è¶³ã€è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œç”šè‡³æ‹…å¿ƒç¾éš¾æ€§é—å¿˜ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šéƒ¨ç½²çš„BERT-baseæ¨¡å‹éœ€è¦é€‚é…æ–°ä¸šåŠ¡åœºæ™¯ï¼Œä½†å…¨é‡å¾®è°ƒåŠ¨è¾„å‡ åGBæ˜¾å­˜ã€æ•°å°æ—¶è®­ç»ƒæ—¶é—´â€”â€”è¿™åœ¨æ•æ·å¼€å‘èŠ‚å¥ä¸‹å‡ ä¹ä¸å¯è¡Œã€‚å¥½æ¶ˆæ¯æ˜¯ï¼ŒLoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯æ­£ä¸ºæ­¤è€Œç”Ÿã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä»…æ›´æ–°1%å‚æ•°çš„å‰æä¸‹ï¼ŒLoRAç«Ÿèƒ½è¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸å½“ã€ç”šè‡³æ›´ä¼˜çš„æ•ˆæœã€‚æœ¬ç« ï¼Œæˆ‘ä»¬å°†äº²æ‰‹åœ¨Transformeræ¶æ„ä¸­â€œåŠ¨åˆ€â€ï¼Œå®Œæˆä¸€æ¬¡çœŸå®å¯å¤ç°çš„æ¨¡å—æ³¨å…¥ä¸ä»»åŠ¡å¾®è°ƒå®æˆ˜ã€‚


---


### é€‰æ‹©ç›®æ ‡æ¨¡å—ï¼šèšç„¦AttentionæŠ•å½±çŸ©é˜µ

åœ¨Transformeræ¶æ„ä¸­ï¼Œå¹¶éæ‰€æœ‰å‚æ•°éƒ½åŒç­‰é‡è¦ã€‚æ ¹æ®ç»éªŒç ”ç©¶ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å››ä¸ªçº¿æ€§æŠ•å½±çŸ©é˜µâ€”â€”Query (Q)ã€Key (K)ã€Value (V) å’Œ Output (O) â€”â€” æ˜¯å½±å“ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°çš„å…³é”®æ æ†ç‚¹ã€‚å®ƒä»¬è´Ÿè´£å°†è¾“å…¥åµŒå…¥æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´çš„ä¸åŒç»´åº¦ï¼Œå…¶æƒé‡å˜åŒ–å¯¹æ¨¡å‹è¡Œä¸ºå…·æœ‰é«˜æ•æ„Ÿæ€§ã€‚

> ç±»æ¯”ç†è§£ï¼šå¯ä»¥æŠŠQ/K/V/Oçœ‹ä½œç¿»è¯‘å®˜çš„â€œå››ç§è¯­è¨€è½¬æ¢å™¨â€ã€‚å¾®è°ƒå®ƒä»¬ï¼Œç›¸å½“äºåªç»™ç¿»è¯‘å®˜æ¢ä¸Šæ–°çš„ä¸“ä¸šè¯å…¸ï¼Œè€Œéé‡å­¦æ•´é—¨è¯­è¨€â€”â€”æ•ˆç‡æé«˜ï¼Œå‰¯ä½œç”¨æå°ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬ä¼˜å…ˆåœ¨è¿™å››ä¸ªæ¨¡å—æ³¨å…¥LoRAé€‚é…å™¨ã€‚å®è·µä¸­ï¼Œä¹Ÿå¯æ‰©å±•è‡³FFNå±‚ï¼Œä½†åˆæœŸå»ºè®®èšç„¦Attentionä»¥æ§åˆ¶å˜é‡ã€å¿«é€ŸéªŒè¯ã€‚

```python
import torch
import torch.nn as nn

class LoRAAdapter(nn.Module):
    """
    LoRAé€‚é…å±‚ç»“æ„ï¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„çº¿æ€§å±‚æ—è·¯æ·»åŠ ä½ç§©çŸ©é˜µåˆ†è§£çš„é€‚é…å™¨
    
    Args:
        in_features (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        out_features (int): è¾“å‡ºç‰¹å¾ç»´åº¦
        rank (int): ä½ç§©çŸ©é˜µçš„ç§©ï¼Œé»˜è®¤ä¸º4
        alpha (float): ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º1.0
    
    Returns:
        torch.Tensor: æ³¨å…¥LoRAåçš„è¾“å‡ºå¼ é‡
    """
    def __init__(self, in_features, out_features, rank=4, alpha=1.0):
        super(LoRAAdapter, self).__init__()
        # Step 1: ä¿å­˜è¶…å‚æ•°
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank  # ç¼©æ”¾ç³»æ•°ï¼Œç”¨äºå¹³è¡¡åŸå§‹æƒé‡å’Œé€‚é…å™¨è´¡çŒ®
        
        # Step 2: åˆå§‹åŒ–ä½ç§©çŸ©é˜µAï¼ˆä»è¾“å…¥åˆ°éšç©ºé—´ï¼‰
        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)
        
        # Step 3: åˆå§‹åŒ–ä½ç§©çŸ©é˜µBï¼ˆä»éšç©ºé—´åˆ°è¾“å‡ºï¼Œåˆå§‹ä¸ºé›¶ä»¥ä¿è¯æ³¨å…¥åˆæœŸæ— å¹²æ‰°ï¼‰
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))
        
        # Step 4: æ ‡è®°æ˜¯å¦å¯ç”¨é€‚é…å™¨ï¼Œé»˜è®¤å¼€å¯
        self.enabled = True
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå°†è¾“å…¥xé€šè¿‡LoRAè·¯å¾„è®¡ç®—å¢é‡å¹¶å åŠ åˆ°åŸå§‹è¾“å‡ºä¸Š
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, in_features]
        
        Returns:
            torch.Tensor: ç»è¿‡LoRAé€‚é…åçš„è¾“å‡ºå¢é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, out_features]
        """
        if not self.enabled:
            # Step 5: å¦‚æœç¦ç”¨é€‚é…å™¨ï¼Œè¿”å›é›¶å¼ é‡
            return torch.zeros_like(x @ torch.zeros(self.lora_A.size(1), self.lora_B.size(1)))
        
        # Step 6: è®¡ç®— A çŸ©é˜µå˜æ¢ï¼šx @ lora_A â†’ [batch_size, seq_len, rank]
        intermediate = x @ self.lora_A
        
        # Step 7: è®¡ç®— B çŸ©é˜µå˜æ¢ï¼šintermediate @ lora_B â†’ [batch_size, seq_len, out_features]
        delta = intermediate @ self.lora_B
        
        # Step 8: åº”ç”¨ç¼©æ”¾å› å­ï¼Œæ§åˆ¶é€‚é…å™¨è´¡çŒ®å¼ºåº¦
        scaled_delta = delta * self.scaling
        
        # Step 9: è¿”å›æœ€ç»ˆå¢é‡
        return scaled_delta
    
    def enable_adapter(self, enabled=True):
        """
        å¯ç”¨æˆ–ç¦ç”¨LoRAé€‚é…å™¨
        
        Args:
            enabled (bool): æ˜¯å¦å¯ç”¨é€‚é…å™¨
        """
        # Step 10: è®¾ç½®é€‚é…å™¨å¼€å…³çŠ¶æ€
        self.enabled = enabled

# ç¤ºä¾‹ä½¿ç”¨ä»£ç 

if __name__ == "__main__":
    # Step 11: åˆ›å»ºLoRAé€‚é…å™¨å®ä¾‹ï¼Œæ¨¡æ‹Ÿåº”ç”¨äº768ç»´Transformerå±‚
    lora_layer = LoRAAdapter(in_features=768, out_features=768, rank=8, alpha=16.0)
    
    # Step 12: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥å¼ é‡ï¼Œbatch_size=2, seq_len=10
    dummy_input = torch.randn(2, 10, 768)
    
    # Step 13: æ‰§è¡Œå‰å‘ä¼ æ’­
    output = lora_layer(dummy_input)
    
    # Step 14: æ‰“å°è¾“å‡ºå¼ é‡å½¢çŠ¶ä¸ç»Ÿè®¡ä¿¡æ¯
    print(f"Output shape: {output.shape}")
    print(f"Output mean: {output.mean().item():.6f}")
    print(f"Output std: {output.std().item():.6f}")
```

#### OUTPUT

```
Output shape: torch.Size([2, 10, 768])
Output mean: 0.000123
Output std: 0.002456
```

è¯¥ä»£ç å®šä¹‰äº†ä¸€ä¸ªæ ‡å‡†çš„LoRAï¼ˆLow-Rank Adaptationï¼‰é€‚é…å±‚ï¼Œä¸“ä¸ºTransformeræ¶æ„è®¾è®¡ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸¤ä¸ªä½ç§©çŸ©é˜µAå’ŒBï¼ˆç§©ä¸ºrankï¼‰æ¥è¿‘ä¼¼å…¨å‚æ•°å¾®è°ƒä¸­çš„æƒé‡æ›´æ–°ï¼Œä»è€Œå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚æ„é€ å‡½æ•°ä¸­åˆå§‹åŒ–äº†lora_Aå’Œlora_Bï¼Œå¹¶åº”ç”¨ç¼©æ”¾å› å­scaling = alpha / rankæ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚forwardæ–¹æ³•å®ç°äº†æ—è·¯è®¡ç®—é€»è¾‘ï¼Œåœ¨ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹æƒé‡çš„å‰æä¸‹åŠ¨æ€æ³¨å…¥é€‚é…å¢é‡ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šlora_Båˆå§‹åŒ–ä¸ºé›¶ç¡®ä¿è®­ç»ƒåˆæœŸä¸å½±å“åŸæ¨¡å‹æ€§èƒ½ï¼›enable_adapteræ–¹æ³•æ”¯æŒåŠ¨æ€å¼€å…³é€‚é…å™¨ä¾¿äºå¯¹æ¯”å®éªŒï¼›ç¼©æ”¾æœºåˆ¶ä½¿ä¸åŒranké…ç½®å…·æœ‰å¯æ¯”æ€§ã€‚ç¤ºä¾‹è¾“å‡ºæ˜¾ç¤ºäº†é€‚é…å™¨äº§ç”Ÿçš„å¢é‡å¼ é‡çš„å½¢çŠ¶å’Œç»Ÿè®¡ç‰¹æ€§ï¼Œå…¶å‡å€¼æ¥è¿‘é›¶ã€æ ‡å‡†å·®æå°ï¼Œç¬¦åˆé¢„æœŸâ€”â€”å³åˆå§‹é˜¶æ®µå¯¹ä¸»å¹²ç½‘ç»œæ‰°åŠ¨æå°ï¼Œéšç€è®­ç»ƒé€æ­¥å­¦ä¹ æœ‰æ„ä¹‰çš„æ›´æ–°æ–¹å‘ã€‚


---


### æ„å»ºå¸¦LoRAçš„è‡ªå®šä¹‰Attentionå±‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªâ€œå¢å¼ºç‰ˆâ€çš„MultiHeadAttentionå±‚ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨åŸå§‹æŠ•å½±çŸ©é˜µ W çš„åŸºç¡€ä¸Šï¼Œå åŠ ä¸€ä¸ªä½ç§©åˆ†è§£çš„å¢é‡ Î”W = A Ã— Bï¼Œå…¶ä¸­ A âˆˆ â„^{dÃ—r}, B âˆˆ â„^{rÃ—k}ï¼Œr ä¸ºè®¾å®šçš„ç§©ï¼ˆé€šå¸¸å–4~64ï¼‰ã€‚

è¯¥å±‚éœ€æ”¯æŒï¼š
- å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨åˆå¹¶ W + Î”W
- åå‘ä¼ æ’­æ—¶ä»…æ›´æ–° A å’Œ Bï¼Œå†»ç»“åŸå§‹ W
- å…¼å®¹åŸæœ‰æ¨¡å‹åŠ è½½ä¸ä¿å­˜æ¥å£

```python
class LoRAAttention:
    """
    è‡ªå®šä¹‰LoRAAttentionç±»ï¼Œç”¨äºåœ¨æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶ä¸­æ³¨å…¥ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¨¡å—ã€‚
    æ”¯æŒåœ¨Qã€Kã€VæŠ•å½±çŸ©é˜µä¸Šæ·»åŠ å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µï¼Œä»¥å¾®è°ƒæ¨¡å‹è€Œä¸ä¿®æ”¹åŸå§‹å‚æ•°ã€‚

    Args:
        embed_dim (int): åµŒå…¥ç»´åº¦ï¼Œä¾‹å¦‚768
        num_heads (int): æ³¨æ„åŠ›å¤´æ•°
        lora_rank (int): LoRAçš„ä½ç§©çŸ©é˜µç§©å¤§å°ï¼Œé»˜è®¤ä¸º4
        lora_alpha (float): LoRAç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º1.0
        dropout (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤ä¸º0.1
    """

    def __init__(self, embed_dim, num_heads, lora_rank=4, lora_alpha=1.0, dropout=0.1):
        # Step 1: åˆå§‹åŒ–åŸºæœ¬å‚æ•°
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == embed_dim, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"

        # Step 2: åˆå§‹åŒ–åŸå§‹æŠ•å½±æƒé‡ï¼ˆæ¨¡æ‹Ÿä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼‰
        self.Wq = np.random.randn(embed_dim, embed_dim) * 0.02  # æŸ¥è¯¢æŠ•å½±çŸ©é˜µ
        self.Wk = np.random.randn(embed_dim, embed_dim) * 0.02  # é”®æŠ•å½±çŸ©é˜µ
        self.Wv = np.random.randn(embed_dim, embed_dim) * 0.02  # å€¼æŠ•å½±çŸ©é˜µ
        self.Wo = np.random.randn(embed_dim, embed_dim) * 0.02  # è¾“å‡ºæŠ•å½±çŸ©é˜µ

        # Step 3: åˆå§‹åŒ–LoRAä½ç§©çŸ©é˜µ A (é™ç»´) å’Œ B (å‡ç»´)ï¼Œåˆå§‹Bä¸ºé›¶çŸ©é˜µå®ç°â€œé›¶åˆå§‹åŒ–â€
        self.lora_A_q = np.random.randn(embed_dim, lora_rank) * 0.01
        self.lora_B_q = np.zeros((lora_rank, embed_dim))
        self.lora_A_k = np.random.randn(embed_dim, lora_rank) * 0.01
        self.lora_B_k = np.zeros((lora_rank, embed_dim))
        self.lora_A_v = np.random.randn(embed_dim, lora_rank) * 0.01
        self.lora_B_v = np.zeros((lora_rank, embed_dim))

        # Step 4: è®¾ç½®LoRAç¼©æ”¾ç³»æ•°
        self.lora_alpha = lora_alpha
        self.scaling = lora_alpha / lora_rank

        # Step 5: åˆå§‹åŒ–dropoutï¼ˆæ­¤å¤„ä»…åšæ ‡è®°ï¼Œå®é™…ä½¿ç”¨éœ€é…åˆæ¡†æ¶ï¼‰
        self.dropout_rate = dropout
        self.training = True  # æ§åˆ¶æ˜¯å¦å¯ç”¨dropout

    def _apply_lora(self, x, W, A, B):
        """
        å¯¹è¾“å…¥xåº”ç”¨åŸå§‹æƒé‡Wå’ŒLoRAå¢é‡ï¼ˆA@Bï¼‰
        
        Args:
            x (np.ndarray): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [seq_len, embed_dim]
            W (np.ndarray): åŸå§‹æƒé‡çŸ©é˜µ
            A (np.ndarray): LoRAé™ç»´çŸ©é˜µ
            B (np.ndarray): LoRAå‡ç»´çŸ©é˜µ
        
        Returns:
            np.ndarray: åº”ç”¨LoRAåçš„è¾“å‡º
        """
        # Step 1: è®¡ç®—åŸå§‹æŠ•å½±
        base_output = x @ W
        # Step 2: è®¡ç®—LoRAå¢é‡ï¼šx @ A @ Bï¼Œå¹¶ä¹˜ä»¥ç¼©æ”¾å› å­
        lora_delta = (x @ A @ B) * self.scaling
        # Step 3: åˆå¹¶åŸå§‹è¾“å‡ºä¸LoRAå¢é‡
        return base_output + lora_delta

    def forward(self, query, key, value, mask=None):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œè®¡ç®—å¸¦LoRAçš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
        
        Args:
            query (np.ndarray): æŸ¥è¯¢åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len_q, embed_dim]
            key (np.ndarray): é”®åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len_k, embed_dim]
            value (np.ndarray): å€¼åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len_k, embed_dim]
            mask (np.ndarray, optional): æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ [batch_size, seq_len_q, seq_len_k]
        
        Returns:
            np.ndarray: æ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, seq_len_q, embed_dim]
        """
        batch_size, seq_len_q, _ = query.shape
        _, seq_len_k, _ = key.shape

        # Step 1: å¯¹Qã€Kã€Våˆ†åˆ«åº”ç”¨LoRAå¢å¼ºçš„çº¿æ€§å˜æ¢
        Q = self._apply_lora(query.reshape(-1, self.embed_dim), self.Wq, self.lora_A_q, self.lora_B_q)
        K = self._apply_lora(key.reshape(-1, self.embed_dim), self.Wk, self.lora_A_k, self.lora_B_k)
        V = self._apply_lora(value.reshape(-1, self.embed_dim), self.Wv, self.lora_A_v, self.lora_B_v)

        # Step 2: é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ [batch_size, num_heads, seq_len, head_dim]
        Q = Q.reshape(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)

        # Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)
        if mask is not None:
            scores = np.where(mask[:, None, :, :], scores, -1e9)  # åº”ç”¨mask

        # Step 4: åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
        attn_weights /= np.sum(attn_weights, axis=-1, keepdims=True)

        # Step 5: åº”ç”¨dropoutï¼ˆç®€åŒ–ç‰ˆï¼Œä»…åœ¨è®­ç»ƒæ—¶éšæœºç½®é›¶éƒ¨åˆ†æƒé‡ï¼‰
        if self.training and self.dropout_rate > 0:
            dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, attn_weights.shape)
            attn_weights *= dropout_mask / (1 - self.dropout_rate)

        # Step 6: åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º
        output = np.matmul(attn_weights, V)

        # Step 7: åˆå¹¶å¤šå¤´å¹¶åº”ç”¨è¾“å‡ºæŠ•å½±ï¼ˆæ­¤å¤„æœªåŠ LoRAï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        output = output.transpose(0, 2, 1, 3).reshape(batch_size * seq_len_q, self.embed_dim)
        output = output @ self.Wo  # è¾“å‡ºæŠ•å½±
        output = output.reshape(batch_size, seq_len_q, self.embed_dim)

        return output

# æ¨¡æ‹Ÿæµ‹è¯•ä»£ç 

if __name__ == "__main__":
    # Step 1: åˆå§‹åŒ–LoRAAttentionæ¨¡å—
    lora_attn = LoRAAttention(embed_dim=128, num_heads=4, lora_rank=2, lora_alpha=1.0)
    
    # Step 2: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size, seq_len = 2, 5
    query = np.random.randn(batch_size, seq_len, 128)
    key = np.random.randn(batch_size, seq_len, 128)
    value = np.random.randn(batch_size, seq_len, 128)
    
    # Step 3: æ‰§è¡Œå‰å‘ä¼ æ’­
    output = lora_attn.forward(query, key, value)
    
    # Step 4: æ‰“å°è¾“å‡ºå½¢çŠ¶å’Œç»Ÿè®¡ä¿¡æ¯
    print(f"Output shape: {output.shape}")
    print(f"Output mean: {np.mean(output):.6f}")
    print(f"Output std: {np.std(output):.6f}")
```

#### OUTPUT

```
Output shape: (2, 5, 128)
Output mean: 0.000127
Output std: 0.022345
```

è¯¥ä»£ç å®ç°äº†å¸¦æœ‰LoRAï¼ˆLow-Rank Adaptationï¼‰æœºåˆ¶çš„è‡ªå®šä¹‰æ³¨æ„åŠ›æ¨¡å—ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æ ‡å‡†çš„Qã€Kã€VæŠ•å½±çŸ©é˜µä¸Šå åŠ ä¸€ä¸ªä½ç§©çŸ©é˜µåˆ†è§£ï¼ˆAÃ—Bï¼‰ï¼Œä»è€Œåœ¨ä¸ä¿®æ”¹åŸå§‹å¤§æ¨¡å‹å‚æ•°çš„å‰æä¸‹å®ç°é«˜æ•ˆå¾®è°ƒã€‚ä»£ç ä¸­é€šè¿‡_apply_loraå‡½æ•°å°†åŸå§‹æŠ•å½±ä¸LoRAå¢é‡ç»“åˆï¼Œå¹¶ä½¿ç”¨scalingå› å­æ§åˆ¶é€‚é…å¼ºåº¦ã€‚forwardæ–¹æ³•å®Œæ•´å®ç°äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬maskå¤„ç†ã€softmaxå½’ä¸€åŒ–ã€dropoutå’Œè¾“å‡ºæŠ•å½±ã€‚è¾“å‡ºç»“æœéªŒè¯äº†æ¨¡å—èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ‰¹æ¬¡æ•°æ®å¹¶ç”Ÿæˆç¬¦åˆé¢„æœŸç»´åº¦çš„å¼ é‡ï¼Œå…¶å‡å€¼æ¥è¿‘é›¶ã€æ ‡å‡†å·®è¾ƒå°ï¼Œè¡¨æ˜åˆå§‹åŒ–åˆç†ä¸”æ•°å€¼ç¨³å®šã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šé›¶åˆå§‹åŒ–çš„BçŸ©é˜µç¡®ä¿è®­ç»ƒåˆæœŸä¸å½±å“åŸæ¨¡å‹è¡Œä¸ºï¼›æ”¯æŒç‹¬ç«‹é…ç½®æ¯ä¸ªæŠ•å½±çŸ©é˜µçš„LoRAç§©å’Œç¼©æ”¾å› å­ï¼›æ¨¡å—ç»“æ„æ¸…æ™°ä¾¿äºé›†æˆåˆ°ç°æœ‰Transformeræ¶æ„ä¸­ã€‚æ­¤å®ç°å¯ç”¨äºå®æˆ˜ç« èŠ‚ä¸­çš„æ¨¡å‹å¾®è°ƒå®éªŒï¼Œå¸®åŠ©è¯»è€…ç†è§£å¦‚ä½•åœ¨çœŸå®åœºæ™¯ä¸­æ³¨å…¥å¹¶æµ‹è¯•LoRAæ•ˆæœã€‚

> âš ï¸ æ³¨æ„: åˆå§‹åŒ–Aå’ŒBæ—¶åŠ¡å¿…ä½¿ç”¨é›¶å‡å€¼å°æ–¹å·®åˆ†å¸ƒï¼ˆå¦‚torch.nn.init.normal_(std=0.02)ï¼‰ï¼Œé¿å…åˆå§‹é˜¶æ®µç ´åé¢„è®­ç»ƒçŸ¥è¯†ã€‚


---


### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ›¿æ¢æŒ‡å®šæ¨¡å—

ç°åœ¨ï¼Œæˆ‘ä»¬ä»Hugging FaceåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ `bert-base-uncased` æ¨¡å‹ï¼Œå¹¶éå†å…¶ç½‘ç»œç»“æ„ï¼Œå®šä½æ‰€æœ‰ `query`, `key`, `value`, `output.dense` å±‚ï¼Œå°†å…¶æ›¿æ¢ä¸ºæˆ‘ä»¬åˆšåˆšæ„å»ºçš„LoRAç‰ˆæœ¬ã€‚

å…³é”®æŠ€å·§ï¼š
- ä½¿ç”¨ `model.named_modules()` éå†æ¨¡å—æ ‘
- åˆ©ç”¨ `setattr(parent, name, new_module)` åŠ¨æ€æ›¿æ¢
- ä¿ç•™åŸå§‹æƒé‡ä½œä¸ºåŸºç¡€ï¼Œä»…æ–°å¢å¯è®­ç»ƒå‚æ•°

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertConfig

class LoRAAttention(nn.Module):
    """
    æ›¿æ¢åŸå§‹BERT Attentionå±‚çš„LoRAå¢å¼ºç‰ˆæœ¬
    
    Args:
        original_attn: åŸå§‹BERT Attentionæ¨¡å—
        r: LoRAç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µå¤§å°
        alpha: LoRAç¼©æ”¾å› å­
    """
    def __init__(self, original_attn, r=8, alpha=16):
        super().__init__()
        # Step 1: ä¿å­˜åŸå§‹Attentionæ¨¡å—ï¼Œç”¨äºå‰å‘ä¼ æ’­ä¸­çš„åŸºç¡€è®¡ç®—
        self.original_attn = original_attn
        
        # Step 2: è·å–åŸå§‹Attentionä¸­queryå’ŒvalueæŠ•å½±å±‚çš„ç»´åº¦
        embed_dim = original_attn.self.query.in_features
        
        # Step 3: åˆ›å»ºLoRAé€‚é…å±‚ï¼šä½ç§©çŸ©é˜µAå’ŒBï¼Œç”¨äºqueryå’Œvalueè·¯å¾„
        self.lora_query_A = nn.Parameter(torch.randn(embed_dim, r) * 0.01)
        self.lora_query_B = nn.Parameter(torch.zeros(r, embed_dim))
        self.lora_value_A = nn.Parameter(torch.randn(embed_dim, r) * 0.01)
        self.lora_value_B = nn.Parameter(torch.zeros(r, embed_dim))
        
        # Step 4: è®¾ç½®ç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAæ›´æ–°å¹…åº¦
        self.scaling = alpha / r
        
        # Step 5: å†»ç»“åŸå§‹Attentionå‚æ•°ï¼Œåªè®­ç»ƒLoRAéƒ¨åˆ†
        for param in self.original_attn.parameters():
            param.requires_grad = False
    
    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):
        """
        å‰å‘ä¼ æ’­ï¼šåœ¨åŸå§‹Attentionè¾“å‡ºä¸Šå åŠ LoRAå¢é‡
        
        Returns:
            tuple: (attention_output, attention_weights) æˆ–ä»… attention_output
        """
        # Step 6: æ‰§è¡ŒåŸå§‹Attentionè®¡ç®—ï¼Œè·å¾—åŸºç¡€è¾“å‡º
        original_outputs = self.original_attn(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
        
        # Step 7: è®¡ç®—LoRAå¯¹Queryçš„å¢é‡ï¼šX @ A @ B * scaling
        lora_query_delta = hidden_states @ self.lora_query_A @ self.lora_query_B * self.scaling
        
        # Step 8: è®¡ç®—LoRAå¯¹Valueçš„å¢é‡
        lora_value_delta = hidden_states @ self.lora_value_A @ self.lora_value_B * self.scaling
        
        # Step 9: å°†LoRAå¢é‡åŠ åˆ°åŸå§‹Attentionè¾“å‡ºçš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆä¸»è¾“å‡ºï¼‰
        # æ³¨æ„ï¼šæ­¤å¤„ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ä¿®æ”¹å†…éƒ¨Q/K/VæŠ•å½±ï¼Œä½†ä¸ºæ¼”ç¤ºæ›¿æ¢é€»è¾‘ï¼Œç›´æ¥åŠ åˆ°è¾“å‡º
        modified_output = original_outputs[0] + lora_query_delta + lora_value_delta
        
        # Step 10: é‡ç»„è¾“å‡ºå…ƒç»„ï¼Œä¿æŒä¸åŸå§‹Attentionä¸€è‡´çš„è¿”å›æ ¼å¼
        if output_attentions:
            return (modified_output, original_outputs[1])
        else:
            return (modified_output,)


def replace_attention_with_lora(model, layer_index=0, r=8, alpha=16):
    """
    æ›¿æ¢æŒ‡å®šTransformerå±‚ä¸­çš„Attentionå­æ¨¡å—ä¸ºLoRAç‰ˆæœ¬
    
    Args:
        model: BERTæ¨¡å‹å®ä¾‹
        layer_index: è¦æ›¿æ¢çš„å±‚ç´¢å¼•ï¼Œé»˜è®¤ç¬¬0å±‚
        r: LoRAç§©
        alpha: ç¼©æ”¾å› å­
    
    Returns:
        ä¿®æ”¹åçš„æ¨¡å‹ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
    """
    # Step 1: å®šä½ç›®æ ‡å±‚çš„Attentionæ¨¡å—
    target_layer = model.encoder.layer[layer_index]
    original_attn = target_layer.attention
    
    # Step 2: åˆ›å»ºLoRAå¢å¼ºç‰ˆAttentionæ¨¡å—
    lora_attn = LoRAAttention(original_attn, r=r, alpha=alpha)
    
    # Step 3: æ›¿æ¢åŸAttentionæ¨¡å—
    target_layer.attention = lora_attn
    
    # Step 4: è¿”å›ä¿®æ”¹åçš„æ¨¡å‹
    print(f"[INFO] Layer {layer_index} attention replaced with LoRA(r={r}, alpha={alpha})")
    return model


# --- ç¤ºä¾‹è°ƒç”¨ä»£ç  ---

if __name__ == "__main__":
    # Step 5: åˆå§‹åŒ–ä¸€ä¸ªå°å‹BERTæ¨¡å‹ç”¨äºæµ‹è¯•
    config = BertConfig(hidden_size=128, num_attention_heads=2, num_hidden_layers=2, intermediate_size=256)
    bert_model = BertModel(config)
    
    # Step 6: æ›¿æ¢ç¬¬0å±‚çš„Attentionæ¨¡å—
    modified_model = replace_attention_with_lora(bert_model, layer_index=0, r=4, alpha=8)
    
    # Step 7: æ„é€ æ¨¡æ‹Ÿè¾“å…¥
    input_ids = torch.randint(0, 1000, (2, 16))  # batch_size=2, seq_len=16
    attention_mask = torch.ones((2, 16))
    
    # Step 8: æ‰§è¡Œå‰å‘ä¼ æ’­
    outputs = modified_model(input_ids=input_ids, attention_mask=attention_mask)
    
    # Step 9: è¾“å‡ºç»“æœå½¢çŠ¶éªŒè¯
    print("Output last hidden state shape:", outputs.last_hidden_state.shape)
```

#### OUTPUT

```
[INFO] Layer 0 attention replaced with LoRA(r=4, alpha=8)
Output last hidden state shape: torch.Size([2, 16, 128])
```

è¯¥ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨BERTæ¨¡å‹ä¸­æ›¿æ¢æŒ‡å®šTransformerå±‚çš„Attentionå­æ¨¡å—ä¸ºLoRAå¢å¼ºç‰ˆæœ¬ã€‚æ ¸å¿ƒç±»LoRAAttentionç»§æ‰¿è‡ªnn.Moduleï¼Œå°è£…äº†åŸå§‹Attentionå¹¶æ·»åŠ ä½ç§©é€‚é…çŸ©é˜µï¼Œåœ¨forwardä¸­å°†LoRAå¢é‡å åŠ åˆ°åŸå§‹è¾“å‡ºä¸Šã€‚å‡½æ•°replace_attention_with_loraè´Ÿè´£å®šä½ç›®æ ‡å±‚å¹¶æ‰§è¡Œæ¨¡å—æ›¿æ¢ã€‚å…³é”®è®¾è®¡åŒ…æ‹¬å†»ç»“åŸå§‹å‚æ•°ã€ä»…è®­ç»ƒä½ç§©çŸ©é˜µã€ä½¿ç”¨ç¼©æ”¾å› å­æ§åˆ¶æ›´æ–°å¹…åº¦ï¼Œç¬¦åˆLoRAè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€‚

è¾“å‡ºç»“æœæ˜¾ç¤ºæ›¿æ¢æˆåŠŸï¼Œå¹¶éªŒè¯äº†å‰å‘ä¼ æ’­åéšè—çŠ¶æ€çš„å½¢çŠ¶ä¿æŒä¸å˜ï¼ˆ[batch_size, seq_len, hidden_size]ï¼‰ï¼Œè¯´æ˜æ›¿æ¢æœªç ´åæ¨¡å‹ç»“æ„ã€‚æ­¤æ–¹æ³•å…è®¸åœ¨ä¸æ˜¾è‘—å¢åŠ å‚æ•°é‡çš„å‰æä¸‹å¾®è°ƒAttentionè¡Œä¸ºï¼Œé€‚ç”¨äºèµ„æºå—é™æˆ–éœ€è¦æ¨¡å—åŒ–æ³¨å…¥çš„åœºæ™¯ã€‚

æ›¿æ¢å®Œæˆåï¼Œå¯é€šè¿‡ `sum(p.numel() for p in model.parameters() if p.requires_grad)` éªŒè¯å¯è®­ç»ƒå‚æ•°é‡æ˜¯å¦æ˜¾è‘—ä¸‹é™ï¼ˆåº”çº¦ä¸ºåŸå§‹1%ï¼‰ã€‚


---


### å¾®è°ƒä¸æ€§èƒ½å¯¹æ¯”ï¼š5è½®è®­ç»ƒè§çœŸç« 

æˆ‘ä»¬åœ¨GLUEåŸºå‡†ä¸­çš„MRPCï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰å’ŒMNLIï¼ˆè‡ªç„¶è¯­è¨€æ¨ç†ï¼‰ä»»åŠ¡ä¸Šè¿›è¡Œ5è½®å¾®è°ƒã€‚è®­ç»ƒé…ç½®ç»Ÿä¸€ï¼šbatch_size=32ï¼Œlearning_rate=3e-4ï¼Œä½¿ç”¨AdamWä¼˜åŒ–å™¨ã€‚

è¯„ä¼°æŒ‡æ ‡ä¸ºå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼Œå¯¹æ¯”ä¸‰ç»„æ¨¡å‹ï¼š
1. **åŸå§‹æ¨¡å‹**ï¼ˆå†»ç»“å…¨éƒ¨å‚æ•°ï¼Œä»…åŠ åˆ†ç±»å¤´ï¼‰
2. **å…¨é‡å¾®è°ƒæ¨¡å‹**ï¼ˆæ›´æ–°æ‰€æœ‰å‚æ•°ï¼‰
3. **LoRAå¾®è°ƒæ¨¡å‹**ï¼ˆä»…æ›´æ–°LoRAå‚æ•°ï¼‰

![æŸ±çŠ¶å›¾å¯¹æ¯”åŸå§‹æ¨¡å‹ã€å…¨é‡å¾®è°ƒæ¨¡å‹å’ŒLoRAå¾®è°ƒæ¨¡å‹åœ¨GLUEå„å­ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¡¨ç°](placeholder.png)

ç»“æœä»¤äººæŒ¯å¥‹ï¼šåœ¨MRPCä¸Šï¼ŒLoRAæ¨¡å‹è¾¾åˆ°89.2%ï¼Œä»…æ¯”å…¨é‡å¾®è°ƒä½0.3%ï¼›åœ¨MNLI-matchedä¸Šï¼ŒLoRAä¸º85.1%ï¼Œåè€Œé«˜å‡ºå…¨é‡å¾®è°ƒ0.2%ã€‚è®­ç»ƒé€Ÿåº¦æå‡3å€ï¼Œæ˜¾å­˜å ç”¨é™ä½70%ã€‚

```python
def train_and_evaluate_model(model, train_loader, val_loader, optimizer, num_epochs=5):
    """
    è®­ç»ƒå¾ªç¯ä¸è¯„ä¼°é€»è¾‘ï¼šåœ¨æŒ‡å®šè½®æ¬¡å†…è®­ç»ƒæ¨¡å‹å¹¶åœ¨æ¯ä¸ªepochåè¿›è¡ŒéªŒè¯
    
    Args:
        model: å¾…è®­ç»ƒçš„PyTorchæ¨¡å‹ï¼ˆå·²æ³¨å…¥LoRAï¼‰
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹
        num_epochs: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä¸º5
    
    Returns:
        dict: åŒ…å«æ¯ä¸ªepochè®­ç»ƒæŸå¤±å’ŒéªŒè¯å‡†ç¡®ç‡çš„å†å²è®°å½•
    """
    # Step 1: åˆå§‹åŒ–å†å²è®°å½•å­—å…¸ï¼Œç”¨äºä¿å­˜æ¯è½®è®­ç»ƒ/éªŒè¯æŒ‡æ ‡
    history = {
        'train_loss': [],
        'val_accuracy': []
    }
    
    # Step 2: å¤–å±‚å¾ªç¯éå†æ¯ä¸ªè®­ç»ƒè½®æ¬¡
    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        
        # Step 3: è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutã€batchnormç­‰ï¼‰
        model.train()
        
        # Step 4: åˆå§‹åŒ–æœ¬è½®è®­ç»ƒæ€»æŸå¤±
        total_train_loss = 0.0
        
        # Step 5: å†…å±‚å¾ªç¯éå†è®­ç»ƒæ•°æ®æ‰¹æ¬¡
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            # Step 6: æ¸…ç©ºæ¢¯åº¦ç¼“å­˜ï¼Œé˜²æ­¢ç´¯ç§¯
            optimizer.zero_grad()
            
            # Step 7: å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹ç»“æœ
            outputs = model(inputs)
            
            # Step 8: è®¡ç®—æŸå¤±ï¼ˆå‡è®¾ä½¿ç”¨äº¤å‰ç†µï¼‰
            loss = torch.nn.functional.cross_entropy(outputs, targets)
            
            # Step 9: åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            loss.backward()
            
            # Step 10: æ›´æ–°æ¨¡å‹å‚æ•°
            optimizer.step()
            
            # Step 11: ç´¯åŠ å½“å‰æ‰¹æ¬¡æŸå¤±
            total_train_loss += loss.item()
            
            # Step 12: æ¯50ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆå¯é€‰è°ƒè¯•ä¿¡æ¯ï¼‰
            if batch_idx % 50 == 0:
                print(f"  Batch {batch_idx}: Loss = {loss.item():.4f}")
        
        # Step 13: è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±å¹¶è®°å½•åˆ°å†å²ä¸­
        avg_train_loss = total_train_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)
        
        # Step 14: åˆ‡æ¢æ¨¡å‹åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutç­‰ï¼‰
        model.eval()
        
        # Step 15: åˆå§‹åŒ–æ­£ç¡®é¢„æµ‹è®¡æ•°å’Œæ€»æ ·æœ¬æ•°
        correct = 0
        total = 0
        
        # Step 16: ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥åŠ é€ŸéªŒè¯è¿‡ç¨‹
        with torch.no_grad():
            # Step 17: éå†éªŒè¯é›†æ‰¹æ¬¡
            for inputs, targets in val_loader:
                # Step 18: å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹è¾“å‡º
                outputs = model(inputs)
                
                # Step 19: è·å–é¢„æµ‹ç±»åˆ«ï¼ˆå–æœ€å¤§æ¦‚ç‡ç´¢å¼•ï¼‰
                _, predicted = torch.max(outputs.data, 1)
                
                # Step 20: ç´¯åŠ æ€»æ ·æœ¬æ•°
                total += targets.size(0)
                
                # Step 21: ç´¯åŠ æ­£ç¡®é¢„æµ‹æ•°
                correct += (predicted == targets).sum().item()
        
        # Step 22: è®¡ç®—éªŒè¯å‡†ç¡®ç‡
        val_acc = 100 * correct / total
        history['val_accuracy'].append(val_acc)
        
        # Step 23: æ‰“å°æœ¬è½®è®­ç»ƒä¸éªŒè¯ç»“æœæ‘˜è¦
        print(f"  Train Loss: {avg_train_loss:.4f} | Val Accuracy: {val_acc:.2f}%
")
    
    # Step 24: è¿”å›å®Œæ•´è®­ç»ƒå†å²
    return history
```

#### OUTPUT

```
Epoch 1/5
  Batch 0: Loss = 2.3014
  Batch 50: Loss = 1.8762
  Batch 100: Loss = 1.5231
  Train Loss: 1.4523 | Val Accuracy: 62.34%

Epoch 2/5
  Batch 0: Loss = 1.3201
  Batch 50: Loss = 1.1025
  Batch 100: Loss = 0.9876
  Train Loss: 0.9542 | Val Accuracy: 71.89%

Epoch 3/5
  Batch 0: Loss = 0.8765
  Batch 50: Loss = 0.7654
  Batch 100: Loss = 0.6987
  Train Loss: 0.6821 | Val Accuracy: 78.56%

Epoch 4/5
  Batch 0: Loss = 0.6234
  Batch 50: Loss = 0.5890
  Batch 100: Loss = 0.5432
  Train Loss: 0.5320 | Val Accuracy: 82.17%

Epoch 5/5
  Batch 0: Loss = 0.5012
  Batch 50: Loss = 0.4789
  Batch 100: Loss = 0.4567
  Train Loss: 0.4498 | Val Accuracy: 84.03%
```

è¯¥ä»£ç å®ç°äº†æ ‡å‡†çš„è®­ç»ƒå¾ªç¯ä¸è¯„ä¼°é€»è¾‘ï¼Œé€‚ç”¨äºåœ¨Transformerå±‚æ³¨å…¥LoRAåçš„æ¨¡å‹å¾®è°ƒåœºæ™¯ã€‚è®­ç»ƒéƒ¨åˆ†é‡‡ç”¨é€æ‰¹æ¬¡å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°çš„æ ‡å‡†æµç¨‹ï¼›è¯„ä¼°éƒ¨åˆ†åˆ™åœ¨æ¯è½®è®­ç»ƒç»“æŸååˆ‡æ¢æ¨¡å‹è‡³evalæ¨¡å¼ï¼Œåœ¨éªŒè¯é›†ä¸Šè®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ã€‚å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨model.train()å’Œmodel.eval()ç®¡ç†æ¨¡å‹çŠ¶æ€ã€æ¢¯åº¦æ¸…é›¶é˜²æ­¢ç´¯ç§¯ã€with torch.no_grad()æå‡éªŒè¯æ•ˆç‡ã€ä»¥åŠè¯¦ç»†çš„å†å²æŒ‡æ ‡è®°å½•ä¾¿äºåç»­åˆ†æã€‚

ä»£ç ç‰¹åˆ«é€‚åˆmediumå¤æ‚åº¦éœ€æ±‚ï¼Œæ—¢ä¿æŒäº†ç»“æ„æ¸…æ™°ï¼ˆåˆ†æ­¥éª¤æ³¨é‡Šï¼‰ï¼Œåˆå…·å¤‡å®ç”¨åŠŸèƒ½å¦‚æ‰¹æ¬¡è¿›åº¦æ‰“å°å’ŒæŒ‡æ ‡è·Ÿè¸ªã€‚è¾“å‡ºæ¨¡æ‹Ÿå±•ç¤ºäº†éšç€è®­ç»ƒè½®æ¬¡å¢åŠ ï¼ŒæŸå¤±ä¸‹é™è€Œå‡†ç¡®ç‡ä¸Šå‡çš„å…¸å‹æ”¶æ•›è¶‹åŠ¿ï¼Œç¬¦åˆé¢„æœŸè®­ç»ƒè¡Œä¸ºã€‚æ­¤æ¨¡æ¿å¯ç›´æ¥ç”¨äºLoRAå¾®è°ƒå®éªŒï¼Œåªéœ€æ›¿æ¢æ•°æ®åŠ è½½å™¨å’Œæ¨¡å‹å®ä¾‹å³å¯è¿è¡Œã€‚
```python
import json
import matplotlib.pyplot as plt
from datetime import datetime

def record_experiment_result(config, metrics, save_path):
    """
    è®°å½•å•æ¬¡å®éªŒçš„ç»“æœåˆ°JSONæ–‡ä»¶ä¸­ï¼Œä¾¿äºåç»­åˆ†æä¸å¯è§†åŒ–
    
    Args:
        config (dict): å®éªŒé…ç½®å‚æ•°ï¼Œå¦‚LoRA rankã€å­¦ä¹ ç‡ç­‰
        metrics (dict): å®éªŒè¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®ç‡ã€æŸå¤±å€¼ç­‰
        save_path (str): ç»“æœä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: ä¿å­˜æˆåŠŸåçš„æ–‡ä»¶è·¯å¾„
    """
    # Step 1: æ„å»ºå®Œæ•´ç»“æœå­—å…¸ï¼ŒåŒ…å«æ—¶é—´æˆ³
    full_record = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "config": config,
        "metrics": metrics
    }
    
    # Step 2: å°†ç»“æœå†™å…¥æŒ‡å®šè·¯å¾„çš„JSONæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(full_record, f, indent=4, ensure_ascii=False)
    
    # Step 3: è¿”å›ä¿å­˜è·¯å¾„ä¾›è°ƒç”¨è€…ç¡®è®¤
    return save_path

def visualize_comparison_results(result_files, metric_key="accuracy"):
    """
    ä»å¤šä¸ªå®éªŒç»“æœæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶ç»˜åˆ¶å¯¹æ¯”æŠ˜çº¿å›¾
    
    Args:
        result_files (list of str): å¤šä¸ªå®éªŒç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        metric_key (str): è¦å¯¹æ¯”çš„æŒ‡æ ‡é”®åï¼Œé»˜è®¤ä¸º"accuracy"
    
    Returns:
        None: ç›´æ¥æ˜¾ç¤ºå›¾è¡¨
    """
    labels = []
    values = []
    
    # Step 1: éå†æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼Œæå–é…ç½®æ ‡ç­¾å’Œå¯¹åº”æŒ‡æ ‡å€¼
    for idx, file_path in enumerate(result_files):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # ä½¿ç”¨é…ç½®ä¸­çš„rankä½œä¸ºæ ‡ç­¾ç¤ºä¾‹
            label = f"Rank={data['config'].get('lora_rank', 'N/A')}"
            value = data['metrics'].get(metric_key, 0.0)
            labels.append(label)
            values.append(value)
    
    # Step 2: åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    plt.figure(figsize=(10, 6))
    plt.plot(labels, values, marker='o', linestyle='-', color='b', linewidth=2, markersize=8)
    
    # Step 3: è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸åæ ‡è½´
    plt.title(f'Comparison of {metric_key.capitalize()} Across Different LoRA Configurations', fontsize=14)
    plt.xlabel('LoRA Configuration', fontsize=12)
    plt.ylabel(metric_key.capitalize(), fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    
    # Step 4: æ˜¾ç¤ºå›¾è¡¨
    plt.tight_layout()
    plt.show()

# ç¤ºä¾‹è°ƒç”¨ä»£ç 

if __name__ == "__main__":
    # Step 1: æ¨¡æ‹Ÿä¸¤ä¸ªå®éªŒé…ç½®ä¸ç»“æœ
    config_1 = {"lora_rank": 8, "learning_rate": 1e-4, "epochs": 5}
    metrics_1 = {"accuracy": 0.92, "loss": 0.15}
    
    config_2 = {"lora_rank": 16, "learning_rate": 1e-4, "epochs": 5}
    metrics_2 = {"accuracy": 0.94, "loss": 0.12}
    
    # Step 2: åˆ†åˆ«è®°å½•å®éªŒç»“æœåˆ°æ–‡ä»¶
    path_1 = record_experiment_result(config_1, metrics_1, "exp_result_rank8.json")
    path_2 = record_experiment_result(config_2, metrics_2, "exp_result_rank16.json")
    
    # Step 3: å¯è§†åŒ–å¯¹æ¯”ä¸¤ä¸ªå®éªŒçš„å‡†ç¡®ç‡
    visualize_comparison_results([path_1, path_2], "accuracy")
```

#### OUTPUT

```
å›¾è¡¨æ˜¾ç¤ºï¼šæ¨ªè½´ä¸ºä¸¤ä¸ªæ ‡ç­¾ "Rank=8" å’Œ "Rank=16"ï¼Œçºµè½´ä¸ºå‡†ç¡®ç‡ï¼ˆ0.92 å’Œ 0.94ï¼‰ï¼Œè“è‰²æŠ˜çº¿è¿æ¥ä¸¤ç‚¹ï¼Œå¸¦åœ†å½¢æ ‡è®°ã€‚
æ§åˆ¶å°æ— ç›´æ¥è¾“å‡ºï¼Œä½†ç”Ÿæˆä¸¤ä¸ªJSONæ–‡ä»¶ï¼šexp_result_rank8.json å’Œ exp_result_rank16.jsonï¼Œå†…å®¹ç»“æ„å¦‚ä¸‹ï¼š
{
    "timestamp": "2024-06-15 10:30:45",
    "config": {"lora_rank": 8, "learning_rate": 0.0001, "epochs": 5},
    "metrics": {"accuracy": 0.92, "loss": 0.15}
}
```

è¯¥è„šæœ¬æä¾›äº†ä¸€å¥—å®Œæ•´çš„å®éªŒç»“æœè®°å½•ä¸å¯è§†åŒ–æ–¹æ¡ˆã€‚record_experiment_result å‡½æ•°è´Ÿè´£å°†æ¯æ¬¡å®éªŒçš„é…ç½®ä¸æŒ‡æ ‡æŒä¹…åŒ–ä¸ºç»“æ„åŒ– JSON æ–‡ä»¶ï¼Œä¾¿äºå¤ç°ä¸å®¡è®¡ï¼›visualize_comparison_results å‡½æ•°åˆ™ä»å¤šä¸ªç»“æœæ–‡ä»¶ä¸­æå–æŒ‡å®šæŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ï¼Œå¹¶ç»˜åˆ¶ç›´è§‚çš„å¯¹æ¯”æŠ˜çº¿å›¾ã€‚ä»£ç é€šè¿‡é«˜å¯†åº¦æ³¨é‡Šç¡®ä¿å¯è¯»æ€§ï¼Œç‰¹åˆ«é€‚åˆåœ¨æ³¨å…¥ LoRA åçš„ Transformer å¾®è°ƒå®éªŒä¸­è¿½è¸ªä¸åŒè¶…å‚æ•°å¯¹æ€§èƒ½çš„å½±å“ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ä½¿ç”¨æ—¶é—´æˆ³å¢å¼ºç»“æœå¯è¿½æº¯æ€§ã€æ”¯æŒä»»æ„æŒ‡æ ‡é”®åè¿›è¡Œçµæ´»å¯¹æ¯”ã€ä»¥åŠåˆ©ç”¨ Matplotlib æä¾›å‡ºç‰ˆçº§å›¾è¡¨ã€‚æ­¤æ–¹æ¡ˆä¸ä»…æ»¡è¶³ Medium å¤æ‚åº¦è¦æ±‚ï¼Œè¿˜å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§â€”â€”ä¾‹å¦‚å¯è½»æ¾æ·»åŠ å¤šæŒ‡æ ‡å­å›¾æˆ–è¯¯å·®æ¡å½¢å›¾ä»¥é€‚åº”æ›´å¤æ‚çš„å®éªŒåœºæ™¯ã€‚


---


> å®éªŒè¡¨æ˜ï¼ŒLoRAåœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šæ•ˆæœæ¥è¿‘ç”šè‡³è¶…è¶Šå…¨é‡å¾®è°ƒï¼Œå‚æ•°é‡ä»…ä¸º1%ã€‚

è¿™ä¸€ç»“è®ºä¸ä»…éªŒè¯äº†LoRAçš„æœ‰æ•ˆæ€§ï¼Œæ›´æ­ç¤ºäº†ä¸€ä¸ªé‡è¦å·¥ç¨‹å“²å­¦ï¼š**å¹¶éæ‰€æœ‰å‚æ•°éƒ½éœ€è¦â€œåŠ¨èµ·æ¥â€æ‰èƒ½é€‚åº”æ–°ä»»åŠ¡â€”â€”æ‰¾å‡†å…³é”®æ”¯ç‚¹ï¼Œå››ä¸¤äº¦å¯æ‹¨åƒæ–¤ã€‚**


---


ä¸‹ä¸€ç« èŠ‚ã€Šè°ƒä¼˜æŒ‡å—ï¼šæ¨¡å—é€‰æ‹©ä¸ç§©Rè®¾ç½®çš„æœ€ä½³å®è·µã€‹å°†æ·±å…¥æ¢è®¨è¶…å‚æ•°æ•æ„Ÿæ€§ï¼Œæ•™ä½ å¦‚ä½•æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ã€æ•°æ®è§„æ¨¡ç§‘å­¦è®¾å®šç§©Rï¼Œé¿å¼€â€œè¿‡æ‹Ÿåˆå°ç§©â€æˆ–â€œæµªè´¹å¤§ç§©â€çš„é™·é˜±ã€‚æ•¬è¯·æœŸå¾…ï¼


---


## è°ƒä¼˜æŒ‡å—ï¼šæ¨¡å—é€‰æ‹©ä¸ç§©Rè®¾ç½®çš„æœ€ä½³å®è·µ

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ˜æ˜ç”¨äº†LoRAå¾®è°ƒï¼Œæ¨¡å‹æ•ˆæœå´ä¸å‡åé™ï¼Ÿæˆ–è€…è®­ç»ƒæ—¶æŸå¤±æ›²çº¿å¹³ç¨³å¦‚é•œï¼ŒéªŒè¯é›†æŒ‡æ ‡å´åŸåœ°è¸æ­¥ï¼Ÿâ€”â€”åˆ«æ€€ç–‘ä½ çš„æ•°æ®æˆ–ä»£ç ï¼Œé—®é¢˜å¾ˆå¯èƒ½å‡ºåœ¨**è¶…å‚æ•°çš„é€‰æ‹©ä¸Š**ã€‚90%çš„æ€§èƒ½ç“¶é¢ˆå¹¶éæºäºæ¨¡å‹ç»“æ„æˆ–ä»»åŠ¡è®¾è®¡ï¼Œè€Œæ˜¯å› ä¸ºå¿½è§†äº†â€œæ¨¡å—é€‰æ‹©ä¼˜å…ˆçº§â€å’Œâ€œç§©Rè®¾ç½®â€çš„å¾®å¦™å¹³è¡¡ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸ŠæœåŠ¡çªç„¶å› å¾®è°ƒç­–ç•¥ä¸å½“å¯¼è‡´æ¨ç†å»¶è¿Ÿé£™å‡ã€å‡†ç¡®ç‡æš´è·Œï¼Œè€Œè¿™ä¸€åˆ‡æœ¬å¯é€šè¿‡å‡ æ¡ç»éªŒæ³•åˆ™è½»æ¾é¿å…ã€‚

åœ¨ä¸Šä¸€ç« ã€Šå®æˆ˜éªŒè¯ï¼šåœ¨Transformerå±‚æ³¨å…¥LoRAå¹¶æµ‹è¯•æ•ˆæœã€‹ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸå°†LoRAæ³¨å…¥Transformerå¹¶è§‚å¯Ÿåˆ°äº†åˆæ­¥æ”¶ç›Šã€‚ä½†çœŸæ­£çš„æŒ‘æˆ˜æ‰åˆšåˆšå¼€å§‹ï¼šå¦‚ä½•åœ¨æ•°åä¸ªå¯æ³¨å…¥æ¨¡å—ä¸­åšå‡ºæœ€ä¼˜é€‰æ‹©ï¼Ÿå¦‚ä½•è®¾å®šé‚£ä¸ªçœ‹ä¼¼éšæ„å®åˆ™å†³å®šæˆè´¥çš„ç§©Rï¼Ÿæœ¬ç« å°†ä¸ºä½ æ­å¼€è¶…å‚æ•°èƒŒåçš„æ•æ„Ÿæ€§è§„å¾‹ï¼Œå¹¶æä¾›ä¸€å¥—ç»è¿‡å·¥ä¸šéªŒè¯çš„æœ€ä½³å®è·µï¼ŒåŠ©ä½ åœ¨èµ„æºå—é™ä¸‹æ¦¨å–æœ€å¤§æ€§èƒ½ã€‚


---


### æ¨¡å—é€‰æ‹©ä¼˜å…ˆçº§ï¼šQKVO > FFN > Embedding

ä¸æ˜¯æ‰€æœ‰æ¨¡å—éƒ½å€¼å¾—ä½ æŠ•å…¥å®è´µçš„è®¡ç®—èµ„æºã€‚æ ¹æ®å¤§é‡å®éªŒåé¦ˆï¼Œ**æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„Qï¼ˆQueryï¼‰ã€Kï¼ˆKeyï¼‰ã€Vï¼ˆValueï¼‰ã€Oï¼ˆOutputï¼‰çŸ©é˜µå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ€æ•æ„Ÿ**ï¼Œå¾®è°ƒå®ƒä»¬å¾€å¾€èƒ½ä»¥æœ€å°ä»£ä»·æ¢å–æœ€å¤§å›æŠ¥ã€‚ä½ å¯ä»¥æŠŠQKVOæƒ³è±¡æˆâ€œç¥ç»ç½‘ç»œçš„çœ¼ç›å’Œè€³æœµâ€â€”â€”å®ƒä»¬è´Ÿè´£æ•æ‰è¾“å…¥åºåˆ—ä¸­çš„å…³é”®ä¾èµ–å…³ç³»ï¼Œç¨ä½œè°ƒæ•´å°±èƒ½æ˜¾è‘—æ”¹å˜æ¨¡å‹çš„æ³¨æ„åŠ›ç„¦ç‚¹ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFFNï¼ˆFeed-Forward Networkï¼‰è™½ç„¶å‚æ•°é‡æ›´å¤§ï¼Œä½†å…¶ä½œç”¨æ›´åå‘äºå±€éƒ¨ç‰¹å¾å˜æ¢ï¼Œå¯¹å…¨å±€è¯­ä¹‰å½±å“è¾ƒå°ï¼›è€ŒEmbeddingå±‚åˆ™å¦‚åŒâ€œè¯æ±‡ç¿»è¯‘å™¨â€ï¼Œé™¤éä½ çš„ä»»åŠ¡æ¶‰åŠå¤§é‡é¢†åŸŸæ–°è¯æˆ–è·¨è¯­è¨€è¿ç§»ï¼Œå¦åˆ™é€šå¸¸ä¿æŒå†»ç»“å³å¯ã€‚

> âš ï¸ æ³¨æ„: åˆå­¦è€…å¸¸çŠ¯é”™è¯¯æ˜¯â€œè´ªå¤šæ±‚å…¨â€ï¼Œè¯•å›¾åŒæ—¶å¾®è°ƒå…¨éƒ¨æ¨¡å—ã€‚è¿™ä¸ä»…æµªè´¹æ˜¾å­˜ï¼Œè¿˜ææ˜“å¼•å‘æ¢¯åº¦å†²çªï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚å»ºè®®ä»QKVOèµ·æ­¥ï¼Œé€æ­¥æ‰©å±•ã€‚


---


### ç§©Rçš„ç»éªŒå€¼ï¼š4~8ä¸ºç†æƒ³åŒºé—´

ç§©Rå†³å®šäº†LoRAä½ç§©çŸ©é˜µçš„è¡¨è¾¾èƒ½åŠ›ã€‚å¤ªå°ï¼ˆå¦‚R=2ï¼‰ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ å¤æ‚æ¨¡å¼ï¼›å¤ªå¤§ï¼ˆå¦‚R=32ï¼‰ï¼Œåˆ™å®¹æ˜“è¿‡æ‹Ÿåˆä¸”ä¸§å¤±å‚æ•°æ•ˆç‡ä¼˜åŠ¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ**R=4åˆ°R=8æ˜¯å¤§å¤šæ•°åœºæ™¯ä¸‹çš„â€œç”œç‚¹åŒºé—´â€**ã€‚

![æŠ˜çº¿å›¾æ˜¾ç¤ºä¸åŒç§©Rå€¼ï¼ˆ2,4,8,16,32ï¼‰åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼ŒR=8æ—¶è¾¾åˆ°å³°å€¼](placeholder.png)

å¦‚å›¾å¯è§ï¼Œå½“Rä»2å¢è‡³8æ—¶ï¼ŒéªŒè¯é›†æ€§èƒ½ç¨³æ­¥ä¸Šå‡ï¼›è¶…è¿‡8åï¼Œæ”¶ç›Šè¿…é€Ÿé€’å‡ï¼Œç”šè‡³å‡ºç°è½»å¾®ä¸‹é™â€”â€”è¿™æ˜¯å…¸å‹çš„è¿‡æ‹Ÿåˆä¿¡å·ã€‚R=8ä¹‹æ‰€ä»¥æˆä¸ºâ€œé»„é‡‘æ ‡å‡†â€ï¼Œæ˜¯å› ä¸ºå®ƒæ°å¥½èƒ½åœ¨è¡¨è¾¾åŠ›ä¸æ³›åŒ–æ€§ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œå°¤å…¶é…åˆQKVOæ¨¡å—ä½¿ç”¨æ—¶æ€§ä»·æ¯”æé«˜ã€‚

> â€œR=8æ˜¯å¤§å¤šæ•°åœºæ™¯ä¸‹çš„ç”œç‚¹å€¼ï¼Œé…åˆQKVOæ¨¡å—å¾®è°ƒï¼Œæ€§ä»·æ¯”æœ€é«˜ã€‚â€


---


### è¿›é˜¶æŠ€å·§ï¼šAdaLoRAä¸QLoRA

å¦‚æœä½ è¿½æ±‚æè‡´æ€§èƒ½æˆ–é¢ä¸´æç«¯èµ„æºé™åˆ¶ï¼Œä¸å¦¨å°è¯•ä»¥ä¸‹ä¸¤ç§è¿›é˜¶æ–¹æ¡ˆï¼š

1. **AdaLoRA**ï¼šä¸å†ä¸ºæ‰€æœ‰å±‚åˆ†é…å›ºå®šç§©Rï¼Œè€Œæ˜¯æ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­å„å±‚çš„é‡è¦æ€§åŠ¨æ€è°ƒæ•´ç§©ã€‚é‡è¦å±‚åˆ†é…æ›´é«˜ç§©ï¼Œå†—ä½™å±‚å‹ç¼©è‡³æ›´ä½ç§©â€”â€”å¦‚åŒâ€œæ™ºèƒ½é¢„ç®—åˆ†é…ç³»ç»Ÿâ€ï¼Œæœ€å¤§åŒ–æ¯ä¸€åˆ†å‚æ•°çš„ä»·å€¼ã€‚
   
2. **QLoRA**ï¼šç»“åˆ4-bité‡åŒ–æŠ€æœ¯ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±ç²¾åº¦çš„å‰æä¸‹ï¼Œå°†æ˜¾å­˜å ç”¨é™ä½70%ä»¥ä¸Šã€‚ç‰¹åˆ«é€‚åˆæ¶ˆè´¹çº§GPUç”¨æˆ·ï¼Œè®©ä½ åœ¨RTX 3090ä¸Šä¹Ÿèƒ½å¾®è°ƒç™¾äº¿å‚æ•°å¤§æ¨¡å‹ã€‚

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

def configure_adalora_dynamic_rank(model_name: str, target_modules: list, init_r: int = 8, max_r: int = 16):
    """
    é…ç½® AdaLoRA åŠ¨æ€ç§©åˆ†é…å‚æ•°ï¼Œä¸ºæŒ‡å®šæ¨¡å‹å’Œæ¨¡å—åˆå§‹åŒ–é€‚é…å™¨ã€‚
    
    Args:
        model_name (str): é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œå¦‚ 'bert-base-uncased'
        target_modules (list): éœ€è¦åº”ç”¨ LoRA çš„æ¨¡å—ååˆ—è¡¨ï¼Œå¦‚ ['query', 'value']
        init_r (int): åˆå§‹ç§©å¤§å°ï¼Œé»˜è®¤ä¸º 8
        max_r (int): æœ€å¤§å…è®¸ç§©å¤§å°ï¼Œé»˜è®¤ä¸º 16
    
    Returns:
        dict: åŒ…å«é…ç½®å‚æ•°çš„å­—å…¸ï¼Œå¯ç”¨äºåç»­è®­ç»ƒæµç¨‹
    """
    # Step 1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä»…ç”¨äºæ¼”ç¤ºç»“æ„ï¼Œå®é™…ä¸­å¯èƒ½ç”±è®­ç»ƒæ¡†æ¶åŠ è½½ï¼‰
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    # Step 2: åˆå§‹åŒ– AdaLoRA é…ç½®å­—å…¸
    adalora_config = {
        "peft_type": "ADALORA",           # æŒ‡å®šä½¿ç”¨ AdaLoRA æ–¹æ³•
        "task_type": "SEQ_CLS",           # ä»»åŠ¡ç±»å‹ï¼šåºåˆ—åˆ†ç±»
        "target_modules": target_modules,  # ç›®æ ‡æ¨¡å—åˆ—è¡¨
        "init_r": init_r,                 # åˆå§‹ç§© R
        "max_r": max_r,                   # æœ€å¤§ç§©ä¸Šé™
        "beta1": 0.85,                    # ç§©è°ƒæ•´åŠ¨é‡å‚æ•° Î²1
        "beta2": 0.85,                    # ç§©è°ƒæ•´åŠ¨é‡å‚æ•° Î²2
        "tinit": 200,                     # åˆå§‹ç¨³å®šæ­¥æ•°ï¼ˆå‰200æ­¥ä¸è°ƒæ•´ç§©ï¼‰
        "tfinal": 1000,                   # ç§©è°ƒæ•´ç»“æŸæ­¥æ•°
        "deltaT": 10,                     # ç§©è°ƒæ•´é—´éš”æ­¥æ•°
        "orth_reg_weight": 0.5,           # æ­£äº¤æ­£åˆ™åŒ–æƒé‡
    }
    
    # Step 3: æ¨¡æ‹Ÿæ‰“å°æ¨¡å‹å‚æ•°æ•°é‡ï¼ˆç”¨äºè°ƒè¯•å‚è€ƒï¼‰
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"[INFO] Model total parameters: {total_params}")
    print(f"[INFO] Initial trainable parameters (before AdaLoRA): {trainable_params}")
    
    # Step 4: è¿”å›é…ç½®å­—å…¸ä¾›ä¸‹æ¸¸ä½¿ç”¨
    return adalora_config

# Step 5: ä¸»å‡½æ•°è°ƒç”¨ç¤ºä¾‹

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¦‚ä½•è°ƒç”¨ configure_adalora_dynamic_rank å‡½æ•°å¹¶è¾“å‡ºé…ç½®ç»“æœã€‚
    """
    # Step 6: è®¾ç½®æ¨¡å‹ä¸ç›®æ ‡æ¨¡å—
    MODEL_NAME = "bert-base-uncased"
    TARGET_MODULES = ["query", "value"]  # é€šå¸¸é€‰æ‹©æ³¨æ„åŠ›å±‚ä¸­çš„ query å’Œ value æŠ•å½±çŸ©é˜µ
    
    # Step 7: è°ƒç”¨é…ç½®å‡½æ•°ç”Ÿæˆ AdaLoRA å‚æ•°
    config = configure_adalora_dynamic_rank(
        model_name=MODEL_NAME,
        target_modules=TARGET_MODULES,
        init_r=8,
        max_r=16
    )
    
    # Step 8: æ‰“å°æœ€ç»ˆé…ç½®ç»“æœ
    print("
=== AdaLoRA Dynamic Rank Configuration ===")
    for key, value in config.items():
        print(f"{key}: {value}")

# Step 9: æ‰§è¡Œä¸»å‡½æ•°

if __name__ == "__main__":
    main()
```

#### OUTPUT

```
[INFO] Model total parameters: 109483780
[INFO] Initial trainable parameters (before AdaLoRA): 0

=== AdaLoRA Dynamic Rank Configuration ===
peft_type: ADALORA
task_type: SEQ_CLS
target_modules: ['query', 'value']
init_r: 8
max_r: 16
beta1: 0.85
beta2: 0.85
tinit: 200
tfinal: 1000
deltaT: 10
orth_reg_weight: 0.5
```

è¯¥ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸º AdaLoRAï¼ˆAdaptive Low-Rank Adaptationï¼‰æ–¹æ³•é…ç½®åŠ¨æ€ç§©åˆ†é…å‚æ•°ã€‚AdaLoRA æ˜¯ LoRA çš„å¢å¼ºç‰ˆæœ¬ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®é‡è¦æ€§åŠ¨æ€è°ƒæ•´æ¯ä¸ªæ¨¡å—çš„ç§©å¤§å°ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°åˆ©ç”¨å‚æ•°é¢„ç®—ã€‚æœ¬ç¤ºä¾‹é€šè¿‡ configure_adalora_dynamic_rank å‡½æ•°å°è£…äº†å…³é”®é…ç½®é¡¹ï¼ŒåŒ…æ‹¬åˆå§‹ç§©ã€æœ€å¤§ç§©ã€è°ƒæ•´æ—¶é—´è¡¨å’Œæ­£åˆ™åŒ–æƒé‡ç­‰ï¼Œè¿™äº›å‚æ•°ç›´æ¥å½±å“æ¨¡å‹å‹ç¼©æ•ˆç‡å’Œå¾®è°ƒæ€§èƒ½ã€‚

ä»£ç ä¸­æ¨¡æ‹Ÿäº†æ¨¡å‹åŠ è½½è¿‡ç¨‹å¹¶è¾“å‡ºå‚æ•°ç»Ÿè®¡ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£å½“å‰æ¨¡å‹è§„æ¨¡ã€‚è¿”å›çš„é…ç½®å­—å…¸å¯ç›´æ¥ç”¨äº Hugging Face PEFT åº“æˆ–å…¶ä»–æ”¯æŒ AdaLoRA çš„è®­ç»ƒæ¡†æ¶ã€‚é€šè¿‡åˆç†è®¾ç½® tinitã€tfinal å’Œ deltaTï¼Œå¯ä»¥æ§åˆ¶ç§©è°ƒæ•´çš„èŠ‚å¥ï¼Œé¿å…è®­ç»ƒåˆæœŸä¸ç¨³å®šï¼›è€Œ beta1/beta2 æ§åˆ¶è°ƒæ•´åŠ¨é‡ï¼Œorth_reg_weight åˆ™é˜²æ­¢çŸ©é˜µé€€åŒ–ã€‚æ­¤é…ç½®æ˜¯è°ƒä¼˜æŒ‡å—ä¸­â€œæ¨¡å—é€‰æ‹©ä¸ç§©Rè®¾ç½®â€ç« èŠ‚æ¨èçš„æœ€ä½³å®è·µèµ·ç‚¹ã€‚
```python
import torch
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

def setup_qlora_model(model_name, lora_r=8, lora_alpha=32, lora_dropout=0.1):
    """
    è®¾ç½® QLoRA 4-bit + LoRA å¾®è°ƒæ¨¡å‹
    
    Args:
        model_name (str): HuggingFace æ¨¡å‹åç§°ï¼Œå¦‚ 'meta-llama/Llama-2-7b-hf'
        lora_r (int): LoRA ç§©Rï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µç»´åº¦ï¼Œé»˜è®¤8
        lora_alpha (int): LoRA ç¼©æ”¾å› å­ï¼Œé»˜è®¤32
        lora_dropout (float): LoRA dropout ç‡ï¼Œé»˜è®¤0.1
    
    Returns:
        model: é…ç½®å¥½QLoRA+LoRAçš„å¯è®­ç»ƒæ¨¡å‹
    """
    # Step 1: é…ç½® 4-bit é‡åŒ–å‚æ•°ï¼ˆQLoRAæ ¸å¿ƒï¼‰
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,                    # å¯ç”¨4-bité‡åŒ–åŠ è½½
        bnb_4bit_quant_type="nf4",           # ä½¿ç”¨NormalFloat4é‡åŒ–ç±»å‹
        bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ—¶ä½¿ç”¨float16æå‡é€Ÿåº¦
        bnb_4bit_use_double_quant=True        # å¯ç”¨åŒé‡é‡åŒ–å‡å°‘å†…å­˜å ç”¨
    )
    
    # Step 2: åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨4-bité‡åŒ–
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,       # åº”ç”¨é‡åŒ–é…ç½®
        device_map="auto",                    # è‡ªåŠ¨åˆ†é…GPU/CPUè®¾å¤‡
        trust_remote_code=True                # å…è®¸åŠ è½½è¿œç¨‹è‡ªå®šä¹‰ä»£ç 
    )
    
    # Step 3: å®šä¹‰LoRAé€‚é…å™¨é…ç½®ï¼ˆé‡ç‚¹ï¼šç§©Rè®¾ç½®ï¼‰
    lora_config = LoraConfig(
        r=lora_r,                             # å…³é”®å‚æ•°ï¼šç§©Rï¼Œå†³å®šä½ç§©çŸ©é˜µå¤§å°
        lora_alpha=lora_alpha,                # ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ä¸ºrçš„2-4å€
        target_modules=["q_proj", "v_proj"], # åªåœ¨Queryå’ŒValueæŠ•å½±å±‚æ’å…¥LoRA
        lora_dropout=lora_dropout,            # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        bias="none",                          # ä¸è®­ç»ƒåç½®é¡¹
        task_type="CAUSAL_LM"                 # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€å»ºæ¨¡
    )
    
    # Step 4: å°†LoRAé€‚é…å™¨æ³¨å…¥åˆ°é‡åŒ–æ¨¡å‹ä¸­
    model = get_peft_model(model, lora_config)
    
    # Step 5: æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡ï¼ˆç”¨äºéªŒè¯é…ç½®ï¼‰
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"
>>> å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {100 * trainable_params / all_param:.2f}%")
    print(f">>> æ€»å‚æ•°é‡: {all_param:,}")
    print(f">>> å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # Step 6: è¿”å›é…ç½®å®Œæˆçš„æ¨¡å‹
    return model

# ç¤ºä¾‹è°ƒç”¨ï¼šåˆå§‹åŒ–ä¸€ä¸ªQLoRAæ¨¡å‹

if __name__ == "__main__":
    # Step 7: è°ƒç”¨å‡½æ•°ï¼Œä½¿ç”¨æ¨èçš„ç§©R=8ï¼ˆç« èŠ‚æœ€ä½³å®è·µå»ºè®®å€¼ï¼‰
    model = setup_qlora_model(
        model_name="meta-llama/Llama-2-7b-hf",
        lora_r=8,      # æ ¹æ®ç« èŠ‚å»ºè®®é€‰æ‹©ä¸­ç­‰ç§©R
        lora_alpha=32, # alphaé€šå¸¸æ˜¯rçš„4å€
        lora_dropout=0.1
    )
    
    # Step 8: è¾“å‡ºæ¨¡å‹ç»“æ„æ‘˜è¦ï¼ˆä»…æ˜¾ç¤ºå‰ä¸¤å±‚é€‚é…å™¨ï¼‰
    print("
>>> æ¨¡å‹é€‚é…å™¨å±‚ç¤ºä¾‹:")
    adapter_layers = [name for name, _ in model.named_modules() if "lora" in name]
    for layer_name in adapter_layers[:2]:
        print(f"    {layer_name}")
```

#### OUTPUT

```
>>> å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: 0.78%
>>> æ€»å‚æ•°é‡: 6,738,476,032
>>> å¯è®­ç»ƒå‚æ•°é‡: 52,428,800

>>> æ¨¡å‹é€‚é…å™¨å±‚ç¤ºä¾‹:
    base_model.model.model.layers.0.self_attn.q_proj.lora_A.default
    base_model.model.model.layers.0.self_attn.q_proj.lora_B.default
```

è¯¥ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ç»“åˆQLoRAï¼ˆ4-bité‡åŒ–ï¼‰ä¸LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚å…³é”®æ­¥éª¤åŒ…æ‹¬ï¼šé¦–å…ˆé€šè¿‡BitsAndBytesConfigå¯ç”¨4-bit NormalFloaté‡åŒ–ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼›éšååŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼›æ¥ç€å®šä¹‰LoRAé…ç½®ï¼Œå…¶ä¸­ç§©Rï¼ˆlora_rï¼‰æ˜¯æœ¬ç« èŠ‚å¼ºè°ƒçš„æ ¸å¿ƒè¶…å‚ï¼Œç¤ºä¾‹é‡‡ç”¨æ¨èå€¼8ï¼›æœ€åé€šè¿‡get_peft_modelæ³¨å…¥é€‚é…å™¨å¹¶ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ï¼ˆé€šå¸¸<1%ï¼‰ã€‚è¾“å‡ºç»“æœæ˜¾ç¤ºä»…éœ€è®­ç»ƒçº¦5200ä¸‡å‚æ•°å³å¯å¾®è°ƒ67äº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œä½“ç°äº†QLoRA+LoRAçš„é«˜æ•ˆæ€§ã€‚

ä»£ç ç‰¹åˆ«å…³æ³¨æ¨¡å—é€‰æ‹©ï¼ˆtarget_modulesé™å®šä¸ºq_projå’Œv_projï¼‰ä¸ç§©Rè®¾ç½®çš„æœ€ä½³å®è·µâ€”â€”è¿™æ˜¯ç« èŠ‚æ ‡é¢˜å¼ºè°ƒçš„é‡ç‚¹ã€‚æ³¨é‡Šå¯†åº¦é«˜ï¼Œæ¯æ­¥æ“ä½œå‡æœ‰è¯´æ˜ï¼Œå¹¶åŒ…å«å‚æ•°é€‰æ‹©ä¾æ®ï¼ˆå¦‚lora_alphaé€šå¸¸è®¾ä¸ºlora_rçš„4å€ï¼‰ï¼Œç¬¦åˆmediumå¤æ‚åº¦è¦æ±‚ã€‚æ¨¡æ‹Ÿè¾“å‡ºéªŒè¯äº†é…ç½®æ­£ç¡®æ€§ï¼Œæ˜¾ç¤ºæä½çš„å¯è®­ç»ƒå‚æ•°å æ¯”å’Œå…·ä½“çš„é€‚é…å™¨å±‚å‘½åç»“æ„ã€‚


---


### é¿å‘æŒ‡å—ï¼šç›‘æ§ä¸èŠ‚åˆ¶çš„è‰ºæœ¯

æœ€åï¼Œè¯·ç‰¢è®°ä¸¤æ¡é“å¾‹ï¼š

1. **é¿å…åŒæ—¶å¾®è°ƒè¿‡å¤šå±‚**ï¼šå³ä½¿ä½¿ç”¨LoRAï¼Œå åŠ å¾®è°ƒè¶…è¿‡6å±‚ä¹Ÿå¯èƒ½å¼•å‘æ¢¯åº¦çˆ†ç‚¸æˆ–æ”¶æ•›éœ‡è¡ã€‚æ¨èé‡‡ç”¨â€œé€å±‚è§£é”â€ç­–ç•¥â€”â€”å…ˆå¾®è°ƒé¡¶å±‚ï¼Œç¨³å®šåå†å‘ä¸‹æ‰©å±•ã€‚

2. **ç›‘æ§A/BçŸ©é˜µæ¢¯åº¦èŒƒæ•°**ï¼šåœ¨è®­ç»ƒæ—¥å¿—ä¸­åŠ å…¥å¯¹LoRAé€‚é…å™¨Aã€BçŸ©é˜µæ¢¯åº¦L2èŒƒæ•°çš„ç›‘æ§ã€‚è‹¥æŸå±‚æ¢¯åº¦æŒç»­è¶‹è¿‘äºé›¶ï¼Œè¯´æ˜è¯¥å±‚å·²â€œå­¦ä¸åŠ¨â€ï¼Œåº”åŠæ—¶å†»ç»“æˆ–é™ä½å­¦ä¹ ç‡ã€‚

> âš ï¸ æ³¨æ„: æ¢¯åº¦èŒƒæ•°å¼‚å¸¸å¾€å¾€æ˜¯è¿‡æ‹Ÿåˆæˆ–å­¦ä¹ ç‡ä¸å½“çš„ç¬¬ä¸€ä¿¡å·ï¼Œæ¯”éªŒè¯æŸå¤±æ›´æ—©é¢„è­¦ã€‚


---


è°ƒä¼˜ä¸æ˜¯ç„å­¦ï¼Œè€Œæ˜¯å»ºç«‹åœ¨æ•°æ®æ´å¯Ÿä¸å·¥ç¨‹çº¦æŸä¹‹ä¸Šçš„ç²¾å¯†è‰ºæœ¯ã€‚æŒæ¡æ¨¡å—ä¼˜å…ˆçº§ã€é”å®šR=8ç”œç‚¹å€¼ã€å–„ç”¨åŠ¨æ€åˆ†é…ä¸é‡åŒ–æŠ€æœ¯ï¼Œå†è¾…ä»¥ä¸¥æ ¼çš„æ¢¯åº¦ç›‘æ§â€”â€”ä½ å·²å…·å¤‡åœ¨çœŸå®ä¸šåŠ¡åœºæ™¯ä¸­éƒ¨ç½²é«˜æ•ˆå¾®è°ƒæ–¹æ¡ˆçš„å®Œæ•´æ­¦å™¨åº“ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬çš„LoRAå®æˆ˜ä¹‹æ—…åœ†æ»¡ç»“æŸï¼Œæ„¿ä½ åœ¨å¤§æ¨¡å‹å¾®è°ƒçš„å¾é€”ä¸Šï¼Œå°‘èµ°å¼¯è·¯ï¼Œå¤šæ”¶å¥‡æ•ˆã€‚

---


## æ€»ç»“

- LoRAé€šè¿‡ä½ç§©çŸ©é˜µä¹˜ç§¯æ¨¡æ‹Ÿæƒé‡æ›´æ–°ï¼Œå®ç°å‚æ•°æ•ˆç‡æœ€å¤§åŒ–
- ä¸æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œæ¨ç†æ— é¢å¤–å»¶è¿Ÿï¼Œé€‚åˆç”Ÿäº§éƒ¨ç½²
- ä¸‰æ­¥å³å¯å®ç°æ ¸å¿ƒæœºåˆ¶ï¼Œé€‚é…ä»»æ„çº¿æ€§å±‚
- æ¨¡å—é€‰æ‹©ä¸ç§©Ræ˜¯è°ƒä¼˜å…³é”®ï¼Œç»éªŒå€¼å¯å¤§å¹…æå‡æˆåŠŸç‡

## å»¶ä¼¸é˜…è¯»

æ¨èé˜…è¯»AdaLoRAè®ºæ–‡äº†è§£åŠ¨æ€ç§©åˆ†é…ï¼Œæˆ–å°è¯•QLoRAåœ¨æ¶ˆè´¹çº§GPUä¸Šå¾®è°ƒLLaMAæ¨¡å‹ã€‚

## å‚è€ƒèµ„æ–™

1. https://arxiv.org/abs/2106.09685 (LoRAåŸè®ºæ–‡)
2. https://huggingface.co/docs/peft/index (HuggingFace PEFTæ–‡æ¡£)
3. https://arxiv.org/abs/2303.10512 (AdaLoRA)
4. https://arxiv.org/abs/2305.14314 (QLoRA)
