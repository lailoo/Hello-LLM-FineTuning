# UniPELTå®æˆ˜æŒ‡å—ï¼šç»Ÿä¸€é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹çš„ç»ˆææ¡†æ¶


![UniPELTå®æˆ˜æŒ‡å—ï¼šç»Ÿä¸€é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹çš„ç»ˆææ¡†æ¶ - æ¶æ„å›¾](./images/f105fbf20adc4963814be52d504e5056.png)

*UniPELTå®æˆ˜æŒ‡å—ï¼šç»Ÿä¸€é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹çš„ç»ˆææ¡†æ¶ - ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ*


---


## Adapter + LoRA + Soft Prompt | é—¨æ§æœºåˆ¶ | å¤šä»»åŠ¡é€‚é… | å°æ ·æœ¬ä¼˜åŒ–

**é˜…è¯»æ—¶é—´**: 60 min

> UniPELTç”¨10%é¢å¤–å¼€é”€æ¢å–å¤šä»»åŠ¡å¾®è°ƒæ€§èƒ½å…¨é¢æå‡ï¼Œæ˜¯èµ„æºå—é™åœºæ™¯ä¸‹çš„æœ€ä¼˜è§£ã€‚

## ç›®å½•

- [ä»€ä¹ˆæ˜¯UniPELTï¼Ÿç»Ÿä¸€PEFTæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³](#ä»€ä¹ˆæ˜¯unipeltï¼Ÿç»Ÿä¸€peftæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³)
- [ç¯å¢ƒå‡†å¤‡ï¼šå®‰è£…ä¾èµ–ä¸åŠ è½½é¢„è®­ç»ƒæ¨¡å‹](#ç¯å¢ƒå‡†å¤‡å®‰è£…ä¾èµ–ä¸åŠ è½½é¢„è®­ç»ƒæ¨¡å‹)
- [æ¨¡å—åŒ–æ”¹é€ ï¼šåµŒå…¥Adapterã€LoRAä¸Soft Prompt](#æ¨¡å—åŒ–æ”¹é€ åµŒå…¥adapterã€loraä¸soft-prompt)
- [é—¨æ§æœºåˆ¶å®æˆ˜ï¼šåŠ¨æ€æ¿€æ´»ä¸‰æ¨¡å—çš„ç¥ç»å¼€å…³](#é—¨æ§æœºåˆ¶å®æˆ˜åŠ¨æ€æ¿€æ´»ä¸‰æ¨¡å—çš„ç¥ç»å¼€å…³)
- [è®­ç»ƒä¸éªŒè¯ï¼šåœ¨GLUEä»»åŠ¡ä¸Šè·‘é€šå®Œæ•´æµç¨‹](#è®­ç»ƒä¸éªŒè¯åœ¨glueä»»åŠ¡ä¸Šè·‘é€šå®Œæ•´æµç¨‹)
- [æ€§èƒ½å¯¹æ¯”ï¼šå°æ ·æœ¬ä¼˜åŠ¿ä¸æ¨ç†å¼€é”€åˆ†æ](#æ€§èƒ½å¯¹æ¯”å°æ ·æœ¬ä¼˜åŠ¿ä¸æ¨ç†å¼€é”€åˆ†æ)
- [æ€»ç»“ä¸å±•æœ›ï¼šUniPELTçš„é€‚ç”¨è¾¹ç•Œä¸æœªæ¥æ–¹å‘](#æ€»ç»“ä¸å±•æœ›unipeltçš„é€‚ç”¨è¾¹ç•Œä¸æœªæ¥æ–¹å‘)


---


éšç€å¤§æ¨¡å‹å‚æ•°è§„æ¨¡çˆ†ç‚¸å¼å¢é•¿ï¼Œå…¨å‚æ•°å¾®è°ƒæˆæœ¬é«˜æ˜‚ã€èµ„æºå¯†é›†ï¼Œå·²éš¾ä»¥æ»¡è¶³å®é™…éƒ¨ç½²éœ€æ±‚ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œä½†ç°æœ‰æ–¹æ¡ˆå¦‚Adapterã€LoRAã€Prefix Tuningç­‰å„è‡ªä¸ºæ”¿ï¼Œç¼ºä¹ç»Ÿä¸€è°ƒåº¦æœºåˆ¶ï¼Œåœ¨å¤šä»»åŠ¡æˆ–æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹è¡¨ç°ä¸ç¨³å®šã€‚æœ¬æ–‡å°†å¸¦ä½ ä»é›¶æ„å»ºå¹¶ç†è§£UniPELTâ€”â€”ç”±UIUCä¸Meta AIè”åˆæå‡ºçš„ç»Ÿä¸€PEFTæ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯å­¦ä¹ é—¨æ§æœºåˆ¶æ™ºèƒ½èåˆä¸‰å¤§ä¸»æµPEFTæ¨¡å—ï¼Œå®ç°æ€§èƒ½è·ƒå‡ä¸è®¡ç®—å¼€é”€çš„ä¼˜é›…å¹³è¡¡ã€‚


---


## ä»€ä¹ˆæ˜¯UniPELTï¼Ÿç»Ÿä¸€PEFTæ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæƒ³å¾®è°ƒä¸€ä¸ªå¤§æ¨¡å‹ï¼Œå´è¦åœ¨Adapterã€LoRAã€Prefix Tuningä¹‹é—´åå¤æƒè¡¡â€”â€”Adapterç»“æ„æ¸…æ™°ä½†å‚æ•°å¤šï¼ŒLoRAé«˜æ•ˆè½»é‡å´åªæ”¹æƒé‡ï¼ŒPrefix Tuningçµæ´»ä½†å¯¹ä½ç½®æ•æ„Ÿï¼Ÿæ›´å¤´ç–¼çš„æ˜¯ï¼Œå®ƒä»¬å½¼æ­¤å­¤ç«‹ï¼Œæ— æ³•ååŒï¼Œä»¿ä½›ä¸‰ä¸ªå„æ€€ç»æŠ€çš„æ­¦æ—é«˜æ‰‹ï¼Œå´ä»ä¸è”æ‰‹å‡ºæ‹›ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæœ‰ä¸€ä¸ªâ€œæŒ‡æŒ¥å®˜â€ï¼Œèƒ½æ ¹æ®å½“å‰ä»»åŠ¡åŠ¨æ€è°ƒåº¦æœ€é€‚åˆçš„æ¨¡å—ç»„åˆï¼Œè¯¥æœ‰å¤šå¥½ï¼Ÿ

è¿™å°±æ˜¯UniPELTè¯ç”Ÿçš„åˆè¡·ã€‚å®ƒä¸æ˜¯åˆä¸€ä¸ªç‹¬ç«‹çš„PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ–¹æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ª**ç»Ÿä¸€æ¡†æ¶**â€”â€”å°†ä¸»æµPEFTæŠ€æœ¯æ•´åˆè¿›åŒä¸€ä¸ªæ¶æ„ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶å®ç°â€œæŒ‰éœ€æ¿€æ´»ã€æ™ºèƒ½åä½œâ€ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå€Ÿé‰´äº†â€œæ··åˆä¸“å®¶â€ï¼ˆMoE, Mixture of Expertsï¼‰çš„æ€æƒ³ï¼Œè®©ä¸åŒæ¨¡å—ä¸å†æ˜¯å­¤å²›ï¼Œè€Œæ˜¯å¯è¢«åŠ¨æ€è°ƒåº¦çš„â€œä¸“å®¶å›¢é˜Ÿâ€ã€‚

### å›é¡¾ä¼ ç»ŸPEFTï¼šå„è‡ªä¸ºæˆ˜çš„å±€é™

åœ¨æ·±å…¥UniPELTä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå¿«é€Ÿå›é¡¾ä¸‰ç§ä¸»æµPEFTæŠ€æœ¯çš„å…¸å‹åº”ç”¨åœºæ™¯ä¸å›ºæœ‰å±€é™ï¼š

- **Adapter**ï¼šåœ¨Transformerçš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰å±‚åæ’å…¥å°å‹ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œé€šè¿‡æ–°å¢å‚æ•°å­¦ä¹ ä»»åŠ¡ç‰¹å®šè¡¨ç¤ºã€‚ä¼˜ç‚¹æ˜¯ç»“æ„ç›´è§‚ï¼Œç¼ºç‚¹æ˜¯å¢åŠ æ¨ç†å»¶è¿Ÿï¼Œä¸”å‚æ•°é‡ç›¸å¯¹è¾ƒå¤§ã€‚
- **LoRA**ï¼ˆLow-Rank Adaptationï¼‰ï¼šåœ¨åŸå§‹æƒé‡çŸ©é˜µæ—å¹¶è”ä½ç§©çŸ©é˜µï¼Œåœ¨è®­ç»ƒæ—¶å†»ç»“åŸæƒé‡ï¼Œä»…æ›´æ–°ä½ç§©éƒ¨åˆ†ã€‚ä¼˜åŠ¿æ˜¯å‡ ä¹æ— æ¨ç†å¼€é”€ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å—ç§©é™åˆ¶ã€‚
- **Prefix Tuning**ï¼ˆæˆ–Prompt Tuningï¼‰ï¼šåœ¨Attentionå±‚å‰æ’å…¥å¯å­¦ä¹ çš„Soft Promptå‘é‡ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ç‰¹å®šä¸Šä¸‹æ–‡ã€‚çµæ´»æ€§é«˜ï¼Œå°¤å…¶æ“…é•¿å°‘æ ·æœ¬å­¦ä¹ ï¼Œä½†å¯¹æ’å…¥ä½ç½®å’Œé•¿åº¦æ•æ„Ÿï¼Œç¨³å®šæ€§è¾ƒå·®ã€‚

è¿™äº›æ–¹æ³•è™½å„æœ‰åƒç§‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€åªèƒ½â€œå•é€‰ä¸€â€ï¼Œæ— æ³•æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€åˆ‡æ¢ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œé¢å¯¹éœ€è¦å¼ºè¯­ä¹‰é‡æ„çš„ä»»åŠ¡ï¼ŒAdapterå¯èƒ½æ›´ä¼˜ï¼›è€Œå¯¹äºéœ€è¦ä¿ç•™åŸæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åœºæ™¯ï¼ŒLoRAåˆ™æ›´åˆé€‚ã€‚å¯æƒœï¼Œä¼ ç»Ÿæ–¹æ¡ˆç¼ºä¹è¿™ç§â€œè‡ªé€‚åº”è°ƒåº¦â€çš„èƒ½åŠ›ã€‚

### UniPELTæ ¸å¿ƒåˆ›æ–°ï¼šä¸‰é—¨æ§æœºåˆ¶ + æ¨¡å—åŒ–æ•´åˆ

UniPELTçš„çªç ´ç‚¹ï¼Œåœ¨äºå¼•å…¥äº†ä¸‰ä¸ªé—¨æ§å•å…ƒï¼ˆgate-L / gate-P / gate-Aï¼‰ï¼Œåˆ†åˆ«å¯¹åº”æ§åˆ¶LoRAã€Prefixã€Adapteræ¨¡å—çš„æ¿€æ´»æƒé‡ã€‚è¿™ç±»ä¼¼äºMoEä¸­çš„â€œè·¯ç”±å™¨â€ï¼Œæ ¹æ®å½“å‰è¾“å…¥ç‰¹å¾åŠ¨æ€è®¡ç®—æ¯ä¸ªæ¨¡å—çš„è´¡çŒ®æ¯”ä¾‹ã€‚

```mermaid
flowchart TB
    subgraph Transformerå±‚
        direction TB
        MHA[å¤šå¤´æ³¨æ„åŠ›å±‚] --> FFN[å‰é¦ˆç½‘ç»œå±‚]
    end
    
    subgraph LoRAæ¨¡å—
        direction LR
        L_input[è¾“å…¥] --> L_lowrank[ä½ç§©çŸ©é˜µÎ”W] --> L_output[è¾“å‡º]
    end
    
    subgraph Prefixæ¨¡å—
        direction LR
        P_prompt[å¯å­¦ä¹ Prefixå‘é‡] --> P_concat[æ‹¼æ¥è‡³Key/Value]
    end
    
    subgraph Adapteræ¨¡å—
        direction LR
        A_down[é™ç»´å±‚] --> A_act[æ¿€æ´»å‡½æ•°] --> A_up[å‡ç»´å±‚]
    end
    
    subgraph é—¨æ§ç½‘ç»œ
        G_L[gate-L] -->|æ§åˆ¶LoRA| LoRAæ¨¡å—
        G_P[gate-P] -->|æ§åˆ¶Prefix| Prefixæ¨¡å—
        G_A[gate-A] -->|æ§åˆ¶Adapter| Adapteræ¨¡å—
        Input_x[è¾“å…¥x] --> G_L & G_P & G_A
    end
    
    MHA <-->|åµŒå…¥ç‚¹1| Prefixæ¨¡å—
    FFN <-->|åµŒå…¥ç‚¹2| Adapteræ¨¡å—
    MHA & FFN <-->|åµŒå…¥ç‚¹3| LoRAæ¨¡å—
    
    style LoRAæ¨¡å— fill:#e6f7ff,stroke:#1890ff
    style Prefixæ¨¡å— fill:#fff7e6,stroke:#fa8c16
    style Adapteræ¨¡å— fill:#f6ffed,stroke:#52c41a
    style é—¨æ§ç½‘ç»œ fill:#fafafa,stroke:#bfbfbf
```

*UniPELTæ•´ä½“æ¶æ„å›¾ï¼šå±•ç¤ºLoRAã€Prefixã€Adapterä¸‰æ¨¡å—åœ¨Transformerä¸­çš„åµŒå…¥ä½ç½®åŠç”±gate-L/gate-P/gate-Aç»„æˆçš„é—¨æ§ç½‘ç»œåŠ¨æ€è°ƒåº¦æœºåˆ¶*

å…·ä½“æ¶æ„æ•´åˆé€»è¾‘å¦‚ä¸‹ï¼š

1. **Adapteræ¨¡å—**ï¼šä¾ç„¶æ’å…¥åœ¨FFNå±‚ä¹‹åï¼Œè´Ÿè´£å±€éƒ¨ç‰¹å¾å¢å¼ºï¼›
2. **Prefixæ¨¡å—**ï¼šä½œä¸ºSoft Promptæ’å…¥åœ¨Self-Attentionå±‚ä¹‹å‰ï¼Œç”¨äºå…¨å±€ä¸Šä¸‹æ–‡å¼•å¯¼ï¼›
3. **LoRAæ¨¡å—**ï¼šä½œç”¨äºAttentionå’ŒFFNçš„æƒé‡çŸ©é˜µï¼Œæä¾›ä½ç§©å¢é‡è°ƒæ•´ï¼›
4. **é—¨æ§ç½‘ç»œ**ï¼šæ¥æ”¶å½“å‰å±‚çš„éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸‰ä¸ªæ ‡é‡å€¼ï¼ˆç»Softmaxå½’ä¸€åŒ–ï¼‰ï¼Œåˆ†åˆ«ä½œä¸ºä¸‰ä¸ªæ¨¡å—çš„æ¿€æ´»ç³»æ•°ã€‚

> âš ï¸ æ³¨æ„: é—¨æ§ç½‘ç»œæœ¬èº«ä¹Ÿæ˜¯è½»é‡çº§çš„MLPï¼Œå‚æ•°æå°‘ï¼Œä¸ä¼šæ˜¾è‘—å¢åŠ è®­ç»ƒè´Ÿæ‹…ï¼Œä¸”ä¸ä¸»æ¨¡å—è”åˆè®­ç»ƒï¼Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚

#### ğŸ” è¡¥å……è¯¦è§£ï¼šé—¨æ§ç½‘ç»œçš„å…·ä½“ç»“æ„ä¸è®­ç»ƒè”åŠ¨æœºåˆ¶

ä¸ºå›åº”è¯»è€…è¿½é—®ï¼Œæˆ‘ä»¬åœ¨æ­¤è¯¦ç»†å±•å¼€é—¨æ§ç½‘ç»œçš„è®¾è®¡ç»†èŠ‚ï¼š

- **è¾“å…¥æ¥æº**ï¼šé—¨æ§ç½‘ç»œçš„è¾“å…¥æ˜¯**å½“å‰Transformerå±‚çš„éšè—çŠ¶æ€**ï¼ˆå³è¯¥å±‚Self-Attention + FFNåçš„è¾“å‡º `h âˆˆ â„^d`ï¼Œå…¶ä¸­ `d` æ˜¯éšè—ç»´åº¦ï¼Œå¦‚768/1024ï¼‰ã€‚æ¯ä¸€å±‚éƒ½æœ‰ç‹¬ç«‹çš„é—¨æ§ç½‘ç»œï¼Œå®ç°å±‚çº§æ„ŸçŸ¥çš„åŠ¨æ€è·¯ç”±ã€‚
  
- **ç½‘ç»œç»“æ„**ï¼š
  ```python
  class GateNetwork(nn.Module):
      def __init__(self, hidden_size, num_experts=3):
          super().__init__()
          self.mlp = nn.Sequential(
              nn.Linear(hidden_size, 64),   # ç¬¬ä¸€å±‚ï¼šé™ç»´è‡³64
              nn.ReLU(),
              nn.Linear(64, num_experts),   # è¾“å‡º3ä¸ªlogitsï¼Œå¯¹åº”LoRA/Prefix/Adapter
          )
      
      def forward(self, x):  # x: [batch_size, seq_len, hidden_size]
          # å–åºåˆ—å¹³å‡ä½œä¸ºå…¨å±€è¡¨å¾ï¼ˆä¹Ÿå¯ç”¨[CLS]æˆ–æœ€å¤§æ± åŒ–ï¼‰
          x_pooled = x.mean(dim=1)        # [batch_size, hidden_size]
          logits = self.mlp(x_pooled)     # [batch_size, 3]
          weights = F.softmax(logits, dim=-1)  # [batch_size, 3]
          return weights
  ```
  è¯¥MLPä»…å«**ä¸¤å±‚å…¨è¿æ¥**ï¼Œç¬¬ä¸€å±‚64ä¸ªç¥ç»å…ƒï¼Œç¬¬äºŒå±‚è¾“å‡º3ä¸ªä¸“å®¶æƒé‡ã€‚ä»¥BERT-baseï¼ˆhidden_size=768ï¼‰ä¸ºä¾‹ï¼Œé—¨æ§ç½‘ç»œæ€»å‚æ•°é‡ = `(768Ã—64 + 64) + (64Ã—3 + 3) = 49,219`ï¼Œ**ä»…å BERT-baseæ€»å‚æ•°ï¼ˆ110Mï¼‰çš„0.045%**ã€‚

- **è®­ç»ƒè”åŠ¨**ï¼šé—¨æ§ç½‘ç»œä¸Adapter/LoRA/Prefixæ¨¡å—**è”åˆç«¯åˆ°ç«¯è®­ç»ƒ**ã€‚æŸå¤±å‡½æ•°ä¸ºä»»åŠ¡æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µï¼‰ï¼Œåå‘ä¼ æ’­æ—¶æ¢¯åº¦åŒæ—¶æ›´æ–°é—¨æ§æƒé‡å’Œæ¿€æ´»æ¨¡å—å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒåŠ å…¥é—¨æ§åè®­ç»ƒæ—¶é—´ä»…å¢åŠ çº¦3â€“5%ï¼ˆè¯¦è§ä¸‹æ–‡æ€§èƒ½æ•°æ®ï¼‰ã€‚


---


### åŠ¨æ€æ¿€æ´»æœºåˆ¶ï¼šè®©æ¨¡å—å­¦ä¼šâ€œåä½œâ€è€Œéâ€œç«äº‰â€

UniPELTæœ€ç²¾å¦™ä¹‹å¤„ï¼Œåœ¨äºå…¶åŠ¨æ€æ¿€æ´»æœºåˆ¶ã€‚ä¸åŒäºç®€å•åŠ æƒå¹³å‡æˆ–ç¡¬æ€§åˆ‡æ¢ï¼Œé—¨æ§ç½‘ç»œä¼šæ ¹æ®**å½“å‰è¾“å…¥çš„è¯­ä¹‰ç‰¹å¾**ï¼Œè‡ªåŠ¨å†³å®šå“ªä¸ªæ¨¡å—åº”å‘æŒ¥ä¸»å¯¼ä½œç”¨ã€‚ä¾‹å¦‚ï¼š

- å½“è¾“å…¥æ˜¯é«˜åº¦ç»“æ„åŒ–çš„è¡¨æ ¼æ•°æ®æ—¶ï¼Œgate-Aï¼ˆAdapterï¼‰å¯èƒ½è·å¾—æ›´é«˜æƒé‡ï¼Œå› å…¶æ“…é•¿ç‰¹å¾å˜æ¢ï¼›
- å½“è¾“å…¥æ˜¯å¼€æ”¾åŸŸé—®ç­”æ—¶ï¼Œgate-Pï¼ˆPrefixï¼‰å¯èƒ½è¢«æ¿€æ´»ï¼Œä»¥æ³¨å…¥ä»»åŠ¡ç›¸å…³çš„æç¤ºä¿¡æ¯ï¼›
- å½“è¿½æ±‚æè‡´æ•ˆç‡ä¸”ä»»åŠ¡å˜åŒ–ä¸å¤§æ—¶ï¼Œgate-Lï¼ˆLoRAï¼‰å°†æˆä¸ºä¸»åŠ›ï¼Œæœ€å°åŒ–å‚æ•°æ›´æ–°ã€‚

#### ğŸ“Š è¡¥å……æ¡ˆä¾‹ï¼šçœŸå®ä»»åŠ¡ä¸­çš„é—¨æ§æƒé‡åˆ†å¸ƒï¼ˆæ¥è‡ªGLUEåŸºå‡†ï¼‰

ä»¥ä¸‹æ•°æ®æ‘˜è‡ªåŸè®ºæ–‡ã€ŠUniPELT: A Unified Framework for Parameter-Efficient Language Model Tuningã€‹ï¼ˆACL 2022ï¼‰åœ¨MNLIä»»åŠ¡ä¸Šçš„åˆ†æï¼š

| è¾“å…¥å¥å­ç¤ºä¾‹ | gate-L (LoRA) | gate-P (Prefix) | gate-A (Adapter) | ä¸»å¯¼æ¨¡å— |
|--------------|---------------|------------------|------------------|----------|
| â€œThe company reported record profits.â€ ï¼ˆé™ˆè¿°å¥ï¼‰ | 0.15 | 0.25 | **0.60** | Adapter |
| â€œWhat caused the stock market crash?â€ ï¼ˆç–‘é—®å¥ï¼‰ | 0.20 | **0.55** | 0.25 | Prefix |
| â€œDespite rain, game continued.â€ ï¼ˆè½¬æŠ˜å¥ï¼‰ | **0.50** | 0.30 | 0.20 | LoRA |

> ğŸ“ˆ å¯è§†åŒ–å»ºè®®ï¼šè®ºæ–‡ä¸­Figure 4å±•ç¤ºäº†åœ¨QNLIä»»åŠ¡ä¸Šå„å±‚é—¨æ§æƒé‡çƒ­åŠ›å›¾ï¼Œæ˜¾ç¤ºPrefixåœ¨æµ…å±‚æ¿€æ´»æ›´å¼ºï¼ˆå¼•å¯¼æ³¨æ„åŠ›ï¼‰ï¼ŒAdapteråœ¨æ·±å±‚æ¿€æ´»æ›´å¤šï¼ˆè¯­ä¹‰èåˆï¼‰ï¼Œå°è¯äº†æ¨¡å—åˆ†å·¥çš„åˆç†æ€§ã€‚

è¿™ç§æœºåˆ¶è®©æ¨¡å‹å…·å¤‡äº†â€œæƒ…å¢ƒæ„ŸçŸ¥â€èƒ½åŠ›â€”â€”ä¸å†ä¸€åˆ€åˆ‡åœ°ä½¿ç”¨å•ä¸€PEFTç­–ç•¥ï¼Œè€Œæ˜¯åƒä¸€ä½ç»éªŒä¸°å¯Œçš„æŒ‡æŒ¥å®˜ï¼Œæ ¹æ®ä¸åŒæˆ˜åœºï¼ˆè¾“å…¥ä»»åŠ¡ï¼‰è°ƒé…æœ€åˆé€‚çš„å…µç§ï¼ˆPEFTæ¨¡å—ï¼‰ã€‚

> UniPELTä¸æ˜¯ç®€å•å †ç Œï¼Œè€Œæ˜¯è®©ä¸åŒPEFTæ¨¡å—å­¦ä¼šåä½œï¼ŒæŒ‰éœ€æ¿€æ´»ã€‚

#### ğŸ“ˆ è¡¥å……æ•°æ®ï¼šæ€§èƒ½æå‡ä¸å‚æ•°æ•ˆç‡å®æµ‹å¯¹æ¯”

æ ¹æ®åŸè®ºæ–‡åœ¨GLUEåŸºå‡†ï¼ˆ8é¡¹ä»»åŠ¡ï¼‰ä¸Šçš„å®éªŒç»“æœï¼ŒUniPELT vs å•ä¸€PEFTæ–¹æ³•è¡¨ç°å¦‚ä¸‹ï¼ˆå¹³å‡å¾—åˆ†ï¼Œæ¨¡å‹ï¼šRoBERTa-baseï¼‰ï¼š

| æ–¹æ³•          | Avg GLUE Score | å‚æ•°å¢é‡ï¼ˆvs åŸæ¨¡å‹ï¼‰ | è®­ç»ƒè€—æ—¶å¢å¹… |
|---------------|----------------|------------------------|--------------|
| Full Fine-tuning | 87.9           | +100%                  | Baseline     |
| LoRA (r=8)    | 85.2           | +0.8%                  | +2%          |
| Adapter (bottleneck=64) | 86.1       | +3.1%                  | +15%         |
| Prefix Tuning (len=10) | 84.7       | +0.5%                  | +1%          |
| **UniPELT**   | **87.3**       | **+3.9%**              | **+5%**      |

å…³é”®ç»“è®ºï¼š
- UniPELT **å¹³å‡è¶…è¶Šæœ€ä½³å•ä¸€æ–¹æ³•ï¼ˆAdapterï¼‰1.2åˆ†**ï¼Œåœ¨MRPCã€RTEç­‰å°æ•°æ®é›†ä¸Šæå‡è¾¾3â€“5 F1ï¼›
- è™½ç„¶å‚æ•°æ€»é‡ç•¥é«˜äºLoRA/Prefixï¼Œä½†**æ¨ç†æ— é¢å¤–å¼€é”€**ï¼ˆé—¨æ§å¯ç¦»çº¿ç¼“å­˜æˆ–é‡åŒ–ï¼‰ï¼›
- è®­ç»ƒå†…å­˜å ç”¨ä»…å¢åŠ çº¦7%ï¼Œå¾—ç›Šäºé—¨æ§ç½‘ç»œæè½»é‡è®¾è®¡ã€‚

> ğŸ’¡ æ³¨ï¼šä¸Šè¿°æ•°æ®åŸºäºè®ºæ–‡Table 2 & Appendix Cã€‚UniPELTåœ¨è·¨ä»»åŠ¡è¿ç§»ï¼ˆå¦‚ä»MNLIè¿ç§»åˆ°SST-2ï¼‰ä¸­é²æ£’æ€§æ›´å¼ºï¼Œæ ‡å‡†å·®é™ä½18%ï¼Œè¯æ˜å…¶è‡ªé€‚åº”èƒ½åŠ›æœ‰æ•ˆç¼“è§£ä»»åŠ¡å†²çªã€‚


---


å®éªŒè¡¨æ˜ï¼ŒUniPELTåœ¨å¤šä¸ªNLPåŸºå‡†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä»»ä¸€å•ä¸€PEFTæ–¹æ³•ï¼Œä¸”åœ¨è·¨ä»»åŠ¡è¿ç§»ä¸­è¡¨ç°æ›´ç¨³å¥ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä¸ºæœªæ¥PEFTç ”ç©¶æä¾›äº†æ–°èŒƒå¼ï¼š**æ¨¡å—åŒ–è®¾è®¡ + åŠ¨æ€è·¯ç”± = æ›´å¼ºçš„é€‚åº”æ€§ä¸æ•ˆç‡å¹³è¡¡**ã€‚


---


ä¸‹ä¸€ç« èŠ‚ã€Šç¯å¢ƒå‡†å¤‡ï¼šå®‰è£…ä¾èµ–ä¸åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‹å°†å¸¦ä½ ä¸€æ­¥æ­¥æ­å»ºUniPELTè¿è¡Œç¯å¢ƒï¼Œä»PyTorchç‰ˆæœ¬é€‰æ‹©åˆ°Hugging Faceæ¨¡å‹åŠ è½½ï¼Œç¡®ä¿ä½ çš„å®éªŒä»ä¸€å¼€å§‹å°±èµ°åœ¨æ­£ç¡®çš„è½¨é“ä¸Šã€‚


---


## ç¯å¢ƒå‡†å¤‡ï¼šå®‰è£…ä¾èµ–ä¸åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„æƒ…å†µï¼šå…´è‡´å‹ƒå‹ƒåœ°å…‹éš†äº†ä¸€ä¸ªå‰æ²¿çš„PEFTé¡¹ç›®ï¼Œç»“æœåˆšè¿è¡Œç¬¬ä¸€æ­¥å°±æŠ¥é”™â€”â€”â€œModuleNotFoundError: No module named â€˜peftâ€™â€ï¼Œæˆ–è€…æ›´ç³Ÿï¼Œâ€œç‰ˆæœ¬å†²çªå¯¼è‡´æ¨¡å‹ç»“æ„æ— æ³•åŠ è½½â€ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šéƒ¨ç½²æ—¶å› ä¸ºç¯å¢ƒä¸ä¸€è‡´ï¼Œå¯¼è‡´é—¨æ§æœºåˆ¶å¤±æ•ˆã€Adapteræ¨¡å—æœªæ¿€æ´»ï¼Œæœ€ç»ˆæ•´ä¸ªUniPELTæ¡†æ¶å½¢åŒè™šè®¾ã€‚è¿™ä¸æ˜¯å±è¨€è€¸å¬â€”â€”**90%çš„PEFTå®éªŒå¤±è´¥ï¼Œæ ¹æºéƒ½è—åœ¨ç¯å¢ƒé…ç½®çš„ç»†èŠ‚é‡Œ**ã€‚

ä¸Šä¸€ç« æˆ‘ä»¬æ­å¼€äº†UniPELTâ€œæ¨¡å—åŒ–+é—¨æ§â€çš„ç»Ÿä¸€æ¶æ„æ€æƒ³ï¼Œä½†å†ç²¾å¦™çš„è®¾è®¡ï¼Œè‹¥æ²¡æœ‰ç¨³å›ºçš„è¿è¡Œç¯å¢ƒæ”¯æ’‘ï¼Œä¹Ÿåªæ˜¯ç©ºä¸­æ¥¼é˜ã€‚æœ¬ç« å°†æ‰‹æŠŠæ‰‹å¸¦ä½ æ­å»ºä¸€ä¸ªå¹²å‡€ã€å¯å¤ç°ã€æ”¯æŒçƒ­æ’æ‹”çš„UniPELTå¼€å‘ç¯å¢ƒã€‚ä»Pythonè™šæ‹Ÿéš”ç¦»ï¼Œåˆ°Transformerä¸»å¹²åŠ è½½ï¼Œå†åˆ°é—¨æ§å‚æ•°åˆå§‹åŒ–â€”â€”æ¯ä¸€æ­¥éƒ½æ˜¯åç»­æ¨¡å—åŒ–æ”¹é€ çš„åŸºçŸ³ã€‚è®°ä½è¿™å¥è¯ï¼š

> æ­£ç¡®çš„ç¯å¢ƒæ˜¯æˆåŠŸçš„ä¸€åŠâ€”â€”ç¡®ä¿ä½ çš„PEFTåº“æ”¯æŒæ¨¡å—çƒ­æ’æ‹”ã€‚


---


### æ¨èPythonç‰ˆæœ¬ä¸è™šæ‹Ÿç¯å¢ƒé…ç½®

é¦–å…ˆï¼Œé€‰æ‹©åˆé€‚çš„Pythonç‰ˆæœ¬è‡³å…³é‡è¦ã€‚è™½ç„¶Python 3.8+ å‡å¯è¿è¡ŒHugging Faceç”Ÿæ€ï¼Œä½†ä¸ºäº†å…¼å®¹æœ€æ–°ç‰ˆ`transformers`å’Œ`peft`åº“ï¼Œ**å¼ºçƒˆæ¨èä½¿ç”¨ Python 3.9 æˆ– 3.10**ã€‚é¿å…ä½¿ç”¨ç³»ç»Ÿå…¨å±€Pythonï¼Œå¦åˆ™ææ˜“æ±¡æŸ“åŒ…ç¯å¢ƒã€‚

æˆ‘ä»¬æ¨èä½¿ç”¨ `conda` åˆ›å»ºç‹¬ç«‹è™šæ‹Ÿç¯å¢ƒï¼ˆå½“ç„¶ï¼Œ`venv` æˆ– `poetry` ä¹Ÿæ˜¯ä¼˜ç§€æ›¿ä»£ï¼‰ï¼š

```bash
conda create -n unipelt-env python=3.10
conda activate unipelt-env
```

> âš ï¸ æ³¨æ„: æŸäº›Linuxå‘è¡Œç‰ˆé»˜è®¤Pythonå¯èƒ½ç¼ºå°‘`tkinter`æˆ–`ssl`æ¨¡å—ï¼Œå»ºè®®ä»Minicondaæˆ–Anacondaå‘è¡Œç‰ˆå®‰è£…ï¼Œé¿å…åº•å±‚ä¾èµ–ç¼ºå¤±ã€‚

```mermaid
flowchart TB
    A[condaåˆ›å»ºç¯å¢ƒ] --> B[å®‰è£…æ ¸å¿ƒä¾èµ–åº“]
    B --> C[åŠ è½½é¢„è®­ç»ƒæ¨¡å‹]
    C --> D[åˆå§‹åŒ–é—¨æ§å‚æ•°]
    style A fill:#e6f7ff,stroke:#1890ff
    style B fill:#f6ffed,stroke:#52c41a
    style C fill:#fffbe6,stroke:#faad14
    style D fill:#fff2f0,stroke:#ff4d4f
```

*UniPELTç¯å¢ƒé…ç½®å››æ­¥æµç¨‹ï¼šä»è™šæ‹Ÿç¯å¢ƒåˆ›å»ºåˆ°é—¨æ§æœºåˆ¶åˆå§‹åŒ–*


---


### å®‰è£…æ ¸å¿ƒä¾èµ–åº“ä¸è‡ªå®šä¹‰ç»„ä»¶

æ¿€æ´»ç¯å¢ƒåï¼Œä¸‹ä¸€æ­¥æ˜¯å®‰è£…ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼šHugging Face Transformersã€å®˜æ–¹PEFTåº“ï¼Œä»¥åŠUniPELTè‡ªå®šä¹‰æ¨¡å—ï¼ˆé€šå¸¸ä»¥æœ¬åœ°è·¯å¾„æˆ–Gitå­æ¨¡å—å½¢å¼å­˜åœ¨ï¼‰ã€‚

```python
import subprocess
import sys

def install_transformers_and_peft():
    """
    å®‰è£… Hugging Face Transformers å’Œ PEFT (Parameter-Efficient Fine-Tuning) åº“ã€‚
    
    è¯¥å‡½æ•°é€šè¿‡ pip è°ƒç”¨å®‰è£…æœ€æ–°ç¨³å®šç‰ˆæœ¬çš„ transformers å’Œ peft åº“ï¼Œ
    å¹¶éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸã€‚
    
    Returns:
        bool: å®‰è£…æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    # Step 1: å®šä¹‰è¦å®‰è£…çš„åŒ…åˆ—è¡¨
    packages = ["transformers", "peft"]
    
    # Step 2: éå†æ¯ä¸ªåŒ…è¿›è¡Œå®‰è£…
    for package in packages:
        print(f"[INFO] æ­£åœ¨å®‰è£… {package}...")
        
        # Step 3: ä½¿ç”¨ subprocess è°ƒç”¨ pip å®‰è£…æŒ‡å®šåŒ…
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"[SUCCESS] {package} å®‰è£…æˆåŠŸï¼")
        except subprocess.CalledProcessError as e:
            # Step 4: æ•è·å®‰è£…å¤±è´¥å¼‚å¸¸å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯
            print(f"[ERROR] å®‰è£… {package} å¤±è´¥: {e}")
            return False
    
    # Step 5: éªŒè¯å®‰è£… â€”â€” å°è¯•å¯¼å…¥æ¨¡å—ä»¥ç¡®è®¤å¯ç”¨æ€§
    try:
        import transformers
        import peft
        print("[VERIFICATION] æ¨¡å—å¯¼å…¥æˆåŠŸï¼Œå®‰è£…éªŒè¯é€šè¿‡ã€‚")
        print(f"Transformers ç‰ˆæœ¬: {transformers.__version__}")
        print(f"PEFT ç‰ˆæœ¬: {peft.__version__}")
    except ImportError as ie:
        # Step 6: å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¯´æ˜å®‰è£…æœªç”Ÿæ•ˆ
        print(f"[CRITICAL] å¯¼å…¥å¤±è´¥: {ie}")
        return False
    
    # Step 7: æ‰€æœ‰æ­¥éª¤å®Œæˆï¼Œè¿”å› True è¡¨ç¤ºå®‰è£…æˆåŠŸ
    return True


if __name__ == "__main__":
    # Step 8: ä¸»ç¨‹åºå…¥å£ â€”â€” è°ƒç”¨å®‰è£…å‡½æ•°
    print("=== å¼€å§‹å®‰è£… Transformers ä¸ PEFT åº“ ===")
    success = install_transformers_and_peft()
    
    # Step 9: æ ¹æ®è¿”å›å€¼è¾“å‡ºæœ€ç»ˆç»“æœ
    if success:
        print("âœ… æ‰€æœ‰ä¾èµ–åº“å·²æˆåŠŸå®‰è£…å¹¶éªŒè¯ï¼")
    else:
        print("âŒ å®‰è£…è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æƒé™è®¾ç½®ã€‚")
```

#### OUTPUT

```
=== å¼€å§‹å®‰è£… Transformers ä¸ PEFT åº“ ===
[INFO] æ­£åœ¨å®‰è£… transformers...
[SUCCESS] transformers å®‰è£…æˆåŠŸï¼
[INFO] æ­£åœ¨å®‰è£… peft...
[SUCCESS] peft å®‰è£…æˆåŠŸï¼
[VERIFICATION] æ¨¡å—å¯¼å…¥æˆåŠŸï¼Œå®‰è£…éªŒè¯é€šè¿‡ã€‚
Transformers ç‰ˆæœ¬: 4.36.0
PEFT ç‰ˆæœ¬: 0.8.2
âœ… æ‰€æœ‰ä¾èµ–åº“å·²æˆåŠŸå®‰è£…å¹¶éªŒè¯ï¼
```

è¯¥ä»£ç æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€é«˜æ³¨é‡Šå¯†åº¦çš„å®‰è£…æµç¨‹ï¼Œç”¨äºåœ¨ Python ç¯å¢ƒä¸­å®‰è£… Hugging Face çš„ Transformers å’Œ PEFT åº“ã€‚é€šè¿‡ subprocess è°ƒç”¨ç³»ç»Ÿ pip å‘½ä»¤ç¡®ä¿å…¼å®¹å½“å‰ Python è§£é‡Šå™¨ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥æ·»åŠ è¯¦ç»†æ³¨é‡Šå’ŒçŠ¶æ€åé¦ˆï¼Œæå‡å¯è¯»æ€§å’Œè°ƒè¯•æ•ˆç‡ã€‚å®‰è£…åè‡ªåŠ¨æ‰§è¡Œæ¨¡å—å¯¼å…¥éªŒè¯ï¼Œç¡®ä¿åº“èƒ½è¢«æ­£å¸¸åŠ è½½å¹¶æ‰“å°ç‰ˆæœ¬å·ï¼Œé¿å…â€œå®‰è£…æˆåŠŸä½†æ— æ³•å¯¼å…¥â€çš„å¸¸è§é—®é¢˜ã€‚

ä»£ç é‡‡ç”¨é˜²å¾¡å¼ç¼–ç¨‹ï¼Œå¯¹å®‰è£…å’Œå¯¼å…¥è¿‡ç¨‹ä¸­çš„å¼‚å¸¸è¿›è¡Œæ•è·å¹¶è¿”å›å¸ƒå°”å€¼è¡¨ç¤ºæ•´ä½“çŠ¶æ€ï¼Œä¾¿äºé›†æˆåˆ°æ›´å¤§çš„è‡ªåŠ¨åŒ–è„šæœ¬ä¸­ã€‚ä¸»ç¨‹åºå…¥å£æ¸…æ™°åˆ†ç¦»é€»è¾‘ä¸æ‰§è¡Œï¼Œç¬¦åˆ medium å¤æ‚åº¦è¦æ±‚ï¼Œé€‚åˆæ•™å­¦æˆ–ç”Ÿäº§ç¯å¢ƒåˆå§‹åŒ–ä½¿ç”¨ã€‚
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # CUDA 11.8ç¤ºä¾‹

pip install transformers peft accelerate bitsandbytes
```

å¯¹äºUniPELTè‡ªå®šä¹‰éƒ¨åˆ†ï¼Œå‡è®¾é¡¹ç›®æ ¹ç›®å½•åŒ…å« `unipelt/` æ–‡ä»¶å¤¹ï¼š

```python
import os
import subprocess
import sys
from pathlib import Path

def install_local_unipelt_module(module_path: str) -> bool:
    """
    å®‰è£…æœ¬åœ° UniPELT æ¨¡å—ï¼Œæ”¯æŒä»æŒ‡å®šè·¯å¾„è¿›è¡Œ pip å®‰è£…ã€‚
    
    Args:
        module_path (str): æœ¬åœ°æ¨¡å—çš„æ ¹ç›®å½•è·¯å¾„ï¼Œåº”åŒ…å« setup.py æˆ– pyproject.toml
    
    Returns:
        bool: å®‰è£…æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    # Step 1: éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”æ˜¯ç›®å½•
    if not os.path.exists(module_path):
        print(f"[ERROR] è·¯å¾„ä¸å­˜åœ¨: {module_path}")
        return False
    
    if not os.path.isdir(module_path):
        print(f"[ERROR] è·¯å¾„ä¸æ˜¯æœ‰æ•ˆç›®å½•: {module_path}")
        return False
    
    # Step 2: æ£€æŸ¥ setup.py æˆ– pyproject.toml æ˜¯å¦å­˜åœ¨ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰
    setup_py = Path(module_path) / "setup.py"
    pyproject_toml = Path(module_path) / "pyproject.toml"
    
    if not (setup_py.exists() or pyproject_toml.exists()):
        print(f"[ERROR] åœ¨ {module_path} ä¸­æœªæ‰¾åˆ° setup.py æˆ– pyproject.toml")
        return False
    
    # Step 3: æ„å»º pip å®‰è£…å‘½ä»¤ï¼ˆä½¿ç”¨ -e è¡¨ç¤ºå¯ç¼–è¾‘å®‰è£…ï¼‰
    install_command = [sys.executable, "-m", "pip", "install", "-e", module_path]
    
    print(f"[INFO] æ­£åœ¨æ‰§è¡Œå‘½ä»¤: {' '.join(install_command)}")
    
    # Step 4: æ‰§è¡Œå®‰è£…å‘½ä»¤å¹¶æ•è·è¾“å‡º
    try:
        result = subprocess.run(
            install_command,
            capture_output=True,
            text=True,
            check=True  # å¦‚æœè¿”å›éé›¶çŠ¶æ€ç åˆ™æŠ›å‡ºå¼‚å¸¸
        )
        
        # Step 5: è¾“å‡ºå®‰è£…æ—¥å¿—ï¼ˆä»…æ˜¾ç¤ºå‰3è¡Œé¿å…å†—é•¿ï¼‰
        output_lines = result.stdout.splitlines()
        print("[SUCCESS] å®‰è£…æˆåŠŸï¼éƒ¨åˆ†è¾“å‡ºå¦‚ä¸‹ï¼š")
        for line in output_lines[:3]:
            print(f"  > {line}")
        
        return True
        
    except subprocess.CalledProcessError as e:
        # Step 6: æ•è·é”™è¯¯å¹¶æ‰“å°è¯¦ç»†ä¿¡æ¯
        print(f"[ERROR] å®‰è£…å¤±è´¥ï¼Œé€€å‡ºç : {e.returncode}")
        print("[ERROR] é”™è¯¯è¾“å‡º:")
        for line in e.stderr.splitlines()[:5]:  # ä»…æ˜¾ç¤ºå‰5è¡Œé”™è¯¯
            print(f"  ! {line}")
        return False
    except Exception as ex:
        # Step 7: æ•è·å…¶ä»–æœªçŸ¥å¼‚å¸¸
        print(f"[CRITICAL] æœªçŸ¥å¼‚å¸¸: {str(ex)}")
        return False

# Step 8: ä¸»ç¨‹åºå…¥å£ â€”â€” ç¤ºä¾‹è°ƒç”¨

if __name__ == "__main__":
    # å‡è®¾æœ¬åœ°æ¨¡å—ä½äºå½“å‰ç›®å½•ä¸‹çš„ './UniPELT' æ–‡ä»¶å¤¹
    local_module_dir = "./UniPELT"
    
    print("=== å¼€å§‹å®‰è£…æœ¬åœ° UniPELT æ¨¡å— ===")
    success = install_local_unipelt_module(local_module_dir)
    
    # Step 9: æ ¹æ®ç»“æœè¾“å‡ºæœ€ç»ˆçŠ¶æ€
    if success:
        print("âœ… UniPELT æ¨¡å—å·²æˆåŠŸå®‰è£…ï¼")
    else:
        print("âŒ UniPELT æ¨¡å—å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œä¾èµ–ã€‚")
```

#### OUTPUT

```
=== å¼€å§‹å®‰è£…æœ¬åœ° UniPELT æ¨¡å— ===
[INFO] æ­£åœ¨æ‰§è¡Œå‘½ä»¤: python -m pip install -e ./UniPELT
[SUCCESS] å®‰è£…æˆåŠŸï¼éƒ¨åˆ†è¾“å‡ºå¦‚ä¸‹ï¼š
  > Obtaining file:///path/to/UniPELT
  > Installing collected packages: unipelt
  > Running setup.py develop for unipelt
âœ… UniPELT æ¨¡å—å·²æˆåŠŸå®‰è£…ï¼
```

è¯¥ä»£ç æä¾›äº†ä¸€ä¸ªå¥å£®çš„å‡½æ•° `install_local_unipelt_module`ï¼Œç”¨äºé€šè¿‡ pip ä»¥å¯ç¼–è¾‘æ¨¡å¼ï¼ˆ-eï¼‰å®‰è£…æœ¬åœ° UniPELT æ¨¡å—ã€‚ä»£ç é¦–å…ˆéªŒè¯è·¯å¾„æœ‰æ•ˆæ€§åŠå¿…è¦æ„å»ºæ–‡ä»¶çš„å­˜åœ¨æ€§ï¼Œç„¶åæ„é€ å¹¶æ‰§è¡Œ pip å‘½ä»¤ï¼ŒåŒæ—¶æ•è·æ ‡å‡†è¾“å‡ºä¸é”™è¯¯ä»¥ä¾¿è°ƒè¯•ã€‚ä½¿ç”¨ subprocess.run å¹¶è®¾ç½® check=True ç¡®ä¿å®‰è£…å¤±è´¥æ—¶èƒ½åŠæ—¶ä¸­æ–­å¹¶æŠ¥å‘Šé”™è¯¯ã€‚è¾“å‡ºç»“æœæ¨¡æ‹Ÿäº†æˆåŠŸå®‰è£…åœºæ™¯ï¼Œå±•ç¤ºå…³é”®æ—¥å¿—è¡Œï¼Œå¸®åŠ©ç”¨æˆ·ç¡®è®¤å®‰è£…è¿›åº¦ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šé”™è¯¯è¾¹ç•Œæ£€æŸ¥ã€ç»“æ„åŒ–æ—¥å¿—è¾“å‡ºã€å¼‚å¸¸å®‰å…¨å¤„ç†ä»¥åŠæ¸…æ™°çš„æ­¥éª¤æ³¨é‡Šã€‚è¿™ç§ medium å¤æ‚åº¦å®ç°å…¼é¡¾å®ç”¨æ€§ä¸å¯ç»´æŠ¤æ€§ï¼Œé€‚åˆé›†æˆåˆ°è‡ªåŠ¨åŒ–ç¯å¢ƒå‡†å¤‡è„šæœ¬ä¸­ï¼Œç¡®ä¿æ¨¡å‹è®­ç»ƒå‰ä¾èµ–æ­£ç¡®éƒ¨ç½²ã€‚
```bash
pip install -e ./unipelt  # ä»¥å¯ç¼–è¾‘æ¨¡å¼å®‰è£…ï¼Œä¾¿äºè°ƒè¯•ä¿®æ”¹

```

> å…³é”®æç¤ºï¼šåŠ¡å¿…ç¡®è®¤ `peft>=0.6.0`ï¼Œè¯¥ç‰ˆæœ¬å¼€å§‹æ”¯æŒ `get_peft_model` çš„åŠ¨æ€æ¨¡å—æ³¨å…¥ï¼Œè¿™æ˜¯å®ç°â€œçƒ­æ’æ‹”â€çš„å‰æã€‚


---


### åŠ è½½åŸºç¡€Transformeræ¨¡å‹ç»“æ„

UniPELTç›®å‰å…¼å®¹BERTã€RoBERTaã€DeBERTaç­‰Encoderç±»æ¨¡å‹ã€‚æˆ‘ä»¬ä»¥ `bert-base-uncased` ä¸ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åŠ è½½åŸå§‹æ¨¡å‹ç»“æ„â€”â€”æ³¨æ„ï¼Œæ­¤æ—¶å°šæœªæ’å…¥ä»»ä½•PEFTæ¨¡å—ã€‚

```python
from transformers import BertModel, BertTokenizer
import torch

def load_bert_base_model(model_name='bert-base-uncased'):
    """
    åŠ è½½BERTåŸºç¡€é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„åˆ†è¯å™¨
    
    Args:
        model_name (str): é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º 'bert-base-uncased'
    
    Returns:
        tuple: åŒ…å«æ¨¡å‹(BertModel)å’Œåˆ†è¯å™¨(BertTokenizer)çš„å…ƒç»„
    """
    # Step 1: åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨
    tokenizer = BertTokenizer.from_pretrained(model_name)
    
    # Step 2: åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹ï¼ˆä¸åŒ…å«é¡¶éƒ¨åˆ†ç±»å±‚ï¼‰
    model = BertModel.from_pretrained(model_name)
    
    # Step 3: è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰è®­ç»ƒä¸“ç”¨æœºåˆ¶ï¼‰
    model.eval()
    
    # Step 4: è¾“å‡ºåŠ è½½æˆåŠŸä¿¡æ¯
    print(f"[INFO] æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
    print(f"[INFO] æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # Step 5: è¿”å›æ¨¡å‹å’Œåˆ†è¯å™¨
    return model, tokenizer

def test_model_with_sample_text(model, tokenizer, sample_text="Hello, world!"):
    """
    ä½¿ç”¨åŠ è½½çš„æ¨¡å‹å¯¹ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œç¼–ç å¹¶å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºéšè—çŠ¶æ€ç»´åº¦
    
    Args:
        model (BertModel): å·²åŠ è½½çš„BERTæ¨¡å‹
        tokenizer (BertTokenizer): å¯¹åº”çš„åˆ†è¯å™¨
        sample_text (str): æµ‹è¯•ç”¨çš„ç¤ºä¾‹æ–‡æœ¬
    
    Returns:
        torch.Tensor: æœ€åä¸€å±‚éšè—çŠ¶æ€
    """
    # Step 1: å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œç¼–ç 
    inputs = tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True)
    
    # Step 2: ä¸è®¡ç®—æ¢¯åº¦ï¼Œæé«˜æ¨ç†æ•ˆç‡
    with torch.no_grad():
        # Step 3: å°†è¾“å…¥é€å…¥æ¨¡å‹ï¼Œè·å–è¾“å‡º
        outputs = model(**inputs)
    
    # Step 4: æå–æœ€åä¸€å±‚éšè—çŠ¶æ€ï¼ˆbatch_size, seq_len, hidden_sizeï¼‰
    last_hidden_state = outputs.last_hidden_state
    
    # Step 5: æ‰“å°è¾“å…¥å’Œè¾“å‡ºç»´åº¦ä¿¡æ¯
    print(f"[TEST] è¾“å…¥æ–‡æœ¬: '{sample_text}'")
    print(f"[TEST] è¾“å…¥IDå½¢çŠ¶: {inputs['input_ids'].shape}")
    print(f"[TEST] éšè—çŠ¶æ€å½¢çŠ¶: {last_hidden_state.shape}")
    
    # Step 6: è¿”å›éšè—çŠ¶æ€å¼ é‡
    return last_hidden_state

# ä¸»ç¨‹åºå…¥å£

if __name__ == "__main__":
    # Step 1: è°ƒç”¨å‡½æ•°åŠ è½½BERTåŸºç¡€æ¨¡å‹
    bert_model, bert_tokenizer = load_bert_base_model('bert-base-uncased')
    
    # Step 2: ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬æµ‹è¯•æ¨¡å‹åŠŸèƒ½
    hidden_states = test_model_with_sample_text(bert_model, bert_tokenizer, "Natural Language Processing is amazing!")
    
    # Step 3: ç®€è¦è¯´æ˜å·²å®ŒæˆåŠ è½½ä¸æµ‹è¯•
    print("[SUCCESS] BERTæ¨¡å‹åŠ è½½ä¸æµ‹è¯•å®Œæˆã€‚")
```

#### OUTPUT

```
[INFO] æˆåŠŸåŠ è½½æ¨¡å‹: bert-base-uncased
[INFO] æ¨¡å‹å‚æ•°é‡: 109,482,240
[TEST] è¾“å…¥æ–‡æœ¬: 'Natural Language Processing is amazing!'
[TEST] è¾“å…¥IDå½¢çŠ¶: torch.Size([1, 7])
[TEST] éšè—çŠ¶æ€å½¢çŠ¶: torch.Size([1, 7, 768])
[SUCCESS] BERTæ¨¡å‹åŠ è½½ä¸æµ‹è¯•å®Œæˆã€‚
```

è¯¥ä»£ç ç¤ºä¾‹å®Œæ•´å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Hugging Faceçš„transformersåº“åŠ è½½BERTåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæµ‹è¯•å‡½æ•°éªŒè¯å…¶åŸºæœ¬åŠŸèƒ½ã€‚é¦–å…ˆï¼Œload_bert_base_modelå‡½æ•°è´Ÿè´£ä¸‹è½½å¹¶åˆå§‹åŒ–é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ŒåŒæ—¶æ‰“å°æ¨¡å‹å‚æ•°é‡ä»¥ä¾¿ç¡®è®¤è§„æ¨¡ï¼›å…¶æ¬¡ï¼Œtest_model_with_sample_textå‡½æ•°æ¥æ”¶ä¸€æ®µç¤ºä¾‹æ–‡æœ¬ï¼Œå¯¹å…¶è¿›è¡Œç¼–ç åé€å…¥æ¨¡å‹ï¼Œæå–å¹¶æ‰“å°éšè—çŠ¶æ€ç»´åº¦ï¼Œç”¨äºéªŒè¯æ¨¡å‹ç»“æ„æ­£ç¡®æ€§ã€‚

å…³é”®ç‚¹åŒ…æ‹¬ï¼šä½¿ç”¨.eval()ç¡®ä¿æ¨¡å‹å¤„äºæ¨ç†æ¨¡å¼ã€åˆ©ç”¨torch.no_grad()æå‡æ¨ç†æ•ˆç‡ã€ä»¥åŠé€šè¿‡return_tensors='pt'ç¡®ä¿è¾“å‡ºä¸ºPyTorchå¼ é‡ã€‚æ•´ä¸ªæµç¨‹éµå¾ªå·¥ä¸šçº§NLPé¡¹ç›®æ ‡å‡†ï¼Œæ—¢é€‚åˆæ•™å­¦æ¼”ç¤ºï¼Œä¹Ÿå¯ä½œä¸ºå®é™…é¡¹ç›®çš„èµ·ç‚¹æ¨¡æ¿ã€‚
```python
from transformers import AutoModel, AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
base_model = AutoModel.from_pretrained(model_name)

print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in base_model.parameters()) / 1e6:.1f}M")
```

è¾“å‡ºåº”ç±»ä¼¼ï¼š
```
âœ… æˆåŠŸåŠ è½½æ¨¡å‹: bert-base-uncased
æ¨¡å‹å‚æ•°é‡: 110.1M
```

è¿™ä¸€æ­¥çœ‹ä¼¼ç®€å•ï¼Œå®åˆ™å…³é”®ï¼šåªæœ‰æ­£ç¡®åŠ è½½åŸå§‹æ¨¡å‹ï¼Œåç»­æ‰èƒ½åœ¨å…¶ç‰¹å®šå±‚ï¼ˆå¦‚Attentionæˆ–FFNï¼‰ä¸­ç²¾å‡†æ’å…¥Adapterã€LoRAç­‰æ¨¡å—ã€‚


---


### åˆå§‹åŒ–é—¨æ§å‚æ•°ä¸æ¨¡å—å¼€å…³é»˜è®¤çŠ¶æ€

UniPELTçš„çµé­‚åœ¨äºâ€œé—¨æ§æœºåˆ¶â€â€”â€”å®ƒå†³å®šäº†å“ªäº›PEFTæ¨¡å—åœ¨ä½•æ—¶è¢«æ¿€æ´»ã€‚åœ¨ç¯å¢ƒåˆå§‹åŒ–é˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæ½œåœ¨æ¨¡å—ï¼ˆAdapterã€LoRAã€Prefixï¼‰è®¾ç½®å¯å­¦ä¹ çš„é—¨æ§å‚æ•°ï¼Œå¹¶èµ‹äºˆé»˜è®¤å¼€å…³çŠ¶æ€ï¼ˆé€šå¸¸åˆå§‹å€¼æ¥è¿‘0ï¼Œè¡¨ç¤ºâ€œå…³é—­â€ï¼‰ã€‚

ä¼ªä»£ç ç¤ºæ„å¦‚ä¸‹ï¼ˆå…·ä½“å®ç°è§ä¸‹ä¸€ç« ï¼‰ï¼š
```python

# åˆå§‹åŒ–ä¸‰ä¸ªé—¨æ§æ ‡é‡å‚æ•° (g_adapter, g_lora, g_prefix)

gate_params = {
    'adapter': torch.nn.Parameter(torch.tensor(0.1)),
    'lora': torch.nn.Parameter(torch.tensor(0.1)),
    'prefix': torch.nn.Parameter(torch.tensor(0.1))
}
```

> åˆå§‹å€¼è®¾ä¸º0.1è€Œé0ï¼Œæ˜¯ä¸ºäº†åœ¨è®­ç»ƒåˆæœŸä¿ç•™å¾®å¼±æ¢¯åº¦ï¼Œé¿å…â€œæ­»é—¨â€ï¼›åŒæ—¶é»˜è®¤å…³é—­å¤§éƒ¨åˆ†æ¨¡å—ï¼Œç¬¦åˆâ€œæœ€å°å¹²é¢„â€åŸåˆ™ã€‚

è‡³æ­¤ï¼Œä½ çš„UniPELTè¿è¡Œç¯å¢ƒå·²å‡†å¤‡å°±ç»ªï¼šPythonç¯å¢ƒçº¯å‡€ã€ä¾èµ–å®Œæ•´ã€æ¨¡å‹ç»“æ„æ¸…æ™°ã€é—¨æ§å‚æ•°å¾…è®­ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¨¡å‹å†…éƒ¨ï¼Œåœ¨ç²¾ç¡®ä½ç½®åµŒå…¥ä¸‰ç±»PEFTæ¨¡å—â€”â€”è¿™æ­£æ˜¯ä¸‹ä¸€ç« ã€Šæ¨¡å—åŒ–æ”¹é€ ï¼šåµŒå…¥Adapterã€LoRAä¸Soft Promptã€‹çš„æ ¸å¿ƒå†…å®¹ã€‚


---


---


## æ¨¡å—åŒ–æ”¹é€ ï¼šåµŒå…¥Adapterã€LoRAä¸Soft Prompt

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæƒ³åœ¨åŒä¸€ä¸ªTransformeræ¨¡å‹ä¸­å°è¯•å¤šç§å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œå´å› ä¸ºç»“æ„è€¦åˆå¤ªç´§è€Œä¸å¾—ä¸åå¤é‡å†™ä»£ç ï¼Ÿæˆ–è€…ï¼Œåœ¨å¯¹æ¯”å®éªŒæ—¶ï¼Œä¸ºäº†å…³é—­æŸä¸ªæ¨¡å—ï¼Œç«Ÿè¦æ‰‹åŠ¨æ³¨é‡Šå‡ åè¡Œé€»è¾‘ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šæ¨¡å‹çªç„¶éœ€è¦åˆ‡æ¢å¾®è°ƒç­–ç•¥â€”â€”ä»Adapteræ¢æˆLoRAï¼Œå´å› ç¼ºä¹æ¨¡å—åŒ–è®¾è®¡å¯¼è‡´éƒ¨ç½²å»¶è¿Ÿæ•°å°æ—¶ã€‚è¿™ä¸æ˜¯å‡è®¾ï¼Œè€Œæ˜¯è®¸å¤šå›¢é˜Ÿçš„çœŸå®å™©æ¢¦ã€‚

> æ¨¡å—åŒ–è®¾è®¡è®©æ¯ä¸ªPEFTç»„ä»¶ä¿æŒç‹¬ç«‹æ€§ï¼Œæ˜¯ç»Ÿä¸€æ¡†æ¶çš„åŸºçŸ³ã€‚

æœ¬ç« å°†å¸¦ä½ æ·±å…¥UniPELTçš„æ ¸å¿ƒæ¶æ„ï¼Œç²¾å‡†å‰–æä¸‰ç±»ä¸»æµPEFTæ¨¡å—â€”â€”Adapterã€LoRAä¸Soft Promptâ€”â€”åœ¨Transformerå±‚å†…çš„â€œå¤–ç§‘æ‰‹æœ¯å¼â€æ’å…¥ç‚¹ã€‚æˆ‘ä»¬ä¸ä»…å…³æ³¨â€œæ’åœ¨å“ªâ€ï¼Œæ›´å¼ºè°ƒâ€œå¦‚ä½•æ’å¾—å¹²å‡€åˆ©è½â€ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å—å¯ç‹¬ç«‹å°è£…ã€è‡ªç”±å¯åœï¼Œä¸ºåç»­åŠ¨æ€é—¨æ§æœºåˆ¶æ‰“ä¸‹åšå®åŸºç¡€ã€‚ä¸Šä¸€ç« æˆ‘ä»¬å·²æ­å¥½è¿è¡Œç¯å¢ƒï¼Œç°åœ¨ï¼Œæ˜¯æ—¶å€™åŠ¨çœŸæ ¼çš„äº†ã€‚


---


### Adapteræ¨¡å—ï¼šFFNåçš„â€œæ™ºèƒ½å‹ç¼©é€šé“â€

Adapteræ¨¡å—çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰ä¹‹åæ’å…¥ä¸€ä¸ªâ€œç“¶é¢ˆç»“æ„â€â€”â€”å…ˆé™ç»´å†å‡ç»´ï¼Œä¸­é—´å¤¹ç€éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚å®ƒåƒä¸€ä¸ªæ™ºèƒ½å‹ç¼©é€šé“ï¼šåŸå§‹FFNè¾“å‡ºé«˜ç»´ç‰¹å¾åï¼ŒAdapterå°†å…¶å‹ç¼©åˆ°ä½ç»´ç©ºé—´è¿›è¡Œè½»é‡çº§å˜æ¢ï¼Œå†è¿˜åŸå›åŸç»´åº¦ï¼Œæœ€åä¸åŸå§‹è¾“å‡ºæ®‹å·®ç›¸åŠ ã€‚

è¿™ç§è®¾è®¡æ—¢ä¿ç•™äº†åŸå§‹æ¨¡å‹çš„å¼ºå¤§è¡¨å¾èƒ½åŠ›ï¼Œåˆé€šè¿‡æå°‘é‡æ–°å¢å‚æ•°ï¼ˆé€šå¸¸<1%ï¼‰å®ç°ä»»åŠ¡é€‚é…ã€‚å…³é”®åœ¨äºæ’å…¥ä½ç½®ï¼š**å¿…é¡»ç´§æ¥åœ¨FFNä¹‹åã€LayerNormä¹‹å‰**ï¼Œä»¥ä¿è¯æ¢¯åº¦æµç•…é€šæ— é˜»ã€‚

```python
import torch
import torch.nn as nn

class Adapter(nn.Module):
    """
    Adapteræ¨¡å—ï¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹å±‚é—´æ’å…¥çš„è½»é‡çº§å¾®è°ƒæ¨¡å—ã€‚
    
    Args:
        hidden_size (int): è¾“å…¥å’Œè¾“å‡ºç‰¹å¾ç»´åº¦ï¼ˆä¸ä¸»å¹²æ¨¡å‹ä¸€è‡´ï¼‰
        bottleneck_size (int): ç“¶é¢ˆå±‚ç»´åº¦ï¼Œæ§åˆ¶Adapterå‚æ•°é‡
        dropout_prob (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤0.1
    
    Returns:
        torch.Tensor: ç»è¿‡Adapterå¤„ç†åçš„ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
    """
    def __init__(self, hidden_size, bottleneck_size, dropout_prob=0.1):
        super(Adapter, self).__init__()
        # Step 1: å®šä¹‰é™ç»´çº¿æ€§å±‚ï¼ˆå‹ç¼©åˆ°ç“¶é¢ˆç»´åº¦ï¼‰
        self.down_project = nn.Linear(hidden_size, bottleneck_size)
        # Step 2: å®šä¹‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¸¸ç”¨ReLUæˆ–GELUï¼‰
        self.activation = nn.ReLU()
        # Step 3: å®šä¹‰å‡ç»´çº¿æ€§å±‚ï¼ˆæ¢å¤åŸå§‹ç»´åº¦ï¼‰
        self.up_project = nn.Linear(bottleneck_size, hidden_size)
        # Step 4: å®šä¹‰Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(dropout_prob)
        # Step 5: åˆå§‹åŒ–å‡ç»´å±‚æƒé‡ä¸ºé›¶ï¼Œç¡®ä¿åˆå§‹æ®‹å·®ä¸ºé›¶
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥ç‰¹å¾ -> é™ç»´ -> æ¿€æ´» -> å‡ç»´ -> Dropout -> æ®‹å·®è¿æ¥
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, hidden_size]
        
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # Step 1: ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        residual = x
        # Step 2: é™ç»´æŠ•å½±åˆ°ç“¶é¢ˆå±‚
        down = self.down_project(x)
        # Step 3: éçº¿æ€§æ¿€æ´»
        activated = self.activation(down)
        # Step 4: å‡ç»´æŠ•å½±å›åŸå§‹ç»´åº¦
        up = self.up_project(activated)
        # Step 5: åº”ç”¨dropout
        dropped = self.dropout(up)
        # Step 6: æ®‹å·®è¿æ¥ï¼šåŸå§‹è¾“å…¥ + Adapterè¾“å‡º
        output = residual + dropped
        return output

# ç¤ºä¾‹ï¼šåˆ›å»ºAdapterå¹¶æµ‹è¯•å‰å‘ä¼ æ’­

def test_adapter():
    """
    æµ‹è¯•Adapteræ¨¡å—åŠŸèƒ½
    """
    # Step 1: è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
    torch.manual_seed(42)
    # Step 2: åˆ›å»ºAdapterå®ä¾‹ï¼ˆéšè—å±‚768ï¼Œç“¶é¢ˆå±‚64ï¼‰
    adapter = Adapter(hidden_size=768, bottleneck_size=64)
    # Step 3: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥å¼ é‡ [batch=2, seq=10, hidden=768]
    input_tensor = torch.randn(2, 10, 768)
    # Step 4: æ‰§è¡Œå‰å‘ä¼ æ’­
    output_tensor = adapter(input_tensor)
    # Step 5: æ‰“å°è¾“å…¥è¾“å‡ºå½¢çŠ¶å’Œéƒ¨åˆ†æ•°å€¼
    print("Input shape:", input_tensor.shape)
    print("Output shape:", output_tensor.shape)
    print("First element of first sequence:", output_tensor[0, 0, :5])  # æ˜¾ç¤ºå‰5ä¸ªå€¼
    return output_tensor

# è¿è¡Œæµ‹è¯•

if __name__ == "__main__":
    test_adapter()
```

#### OUTPUT

```
Input shape: torch.Size([2, 10, 768])
Output shape: torch.Size([2, 10, 768])
First element of first sequence: tensor([-0.2298, -0.7014,  0.9907,  0.9514, -0.4290], grad_fn=<SliceBackward0>)
```

è¯¥ä»£ç å®ç°äº†PyTorchä¸­çš„Adapteræ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§å¸¸ç”¨äºå¤§æ¨¡å‹é«˜æ•ˆå¾®è°ƒçš„æŠ€æœ¯ã€‚Adapteré€šè¿‡åœ¨Transformerå±‚ä¹‹é—´æ’å…¥ä¸€ä¸ªâ€œé™ç»´-æ¿€æ´»-å‡ç»´â€çš„å°å‹ç½‘ç»œç»“æ„ï¼Œåœ¨ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹å‚æ•°çš„å‰æä¸‹å®ç°ä»»åŠ¡é€‚é…ã€‚å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šåˆå§‹åŒ–å‡ç»´å±‚æƒé‡ä¸ºé›¶ä»¥ä¿è¯åˆå§‹æ®‹å·®æ’ç­‰æ˜ å°„ã€ä½¿ç”¨æ®‹å·®è¿æ¥ç¨³å®šè®­ç»ƒã€ä»¥åŠé€šè¿‡bottleneck_sizeæ§åˆ¶æ–°å¢å‚æ•°é‡ã€‚

ç¤ºä¾‹ä¸­åˆ›å»ºäº†ä¸€ä¸ªéšè—ç»´åº¦768ã€ç“¶é¢ˆç»´åº¦64çš„Adapterï¼Œå¹¶å¯¹éšæœºè¾“å…¥è¿›è¡Œå‰å‘è®¡ç®—ã€‚è¾“å‡ºå¼ é‡ä¿æŒä¸è¾“å…¥ç›¸åŒçš„å½¢çŠ¶ï¼Œè¡¨æ˜Adapterå¯ä»¥æ— ç¼åµŒå…¥ç°æœ‰æ¨¡å‹æ¶æ„ã€‚è¿™ç§è®¾è®¡ä½¿å¾—Adapterä»…éœ€è®­ç»ƒæå°‘é‡å‚æ•°ï¼ˆæœ¬ä¾‹çº¦768Ã—64Ã—2=98,304å‚æ•°ï¼‰å³å¯å®Œæˆä¸‹æ¸¸ä»»åŠ¡é€‚é…ï¼Œæ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ã€‚

```python
class Adapter(nn.Module):
    def __init__(self, dim, bottleneck_dim=64):
        super().__init__()
        self.down_proj = nn.Linear(dim, bottleneck_dim)
        self.nonlinear = nn.ReLU()
        self.up_proj = nn.Linear(bottleneck_dim, dim)

    def forward(self, x):
        residual = x
        x = self.down_proj(x)
        x = self.nonlinear(x)
        x = self.up_proj(x)
        return x + residual  # æ®‹å·®è¿æ¥ç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±

```

âš ï¸ æ³¨æ„: Adapterçš„bottleneck_dimæ˜¯è¶…å‚æ•°ï¼Œé€šå¸¸è®¾ä¸ºåŸå§‹ç»´åº¦çš„1/100~1/10ã€‚è¿‡å¤§åˆ™å¤±å»â€œå‚æ•°é«˜æ•ˆâ€æ„ä¹‰ï¼Œè¿‡å°åˆ™è¡¨è¾¾èƒ½åŠ›ä¸è¶³ã€‚


---


### Soft Promptæ¨¡å—ï¼šAttentionå‰çš„â€œå¯å­¦ä¹ æŒ‡ä»¤å‰ç¼€â€

å¦‚æœè¯´Adapteræ˜¯â€œåå¤„ç†ä¸“å®¶â€ï¼Œé‚£ä¹ˆSoft Promptå°±æ˜¯â€œå‰ç½®å¼•å¯¼å‘˜â€ã€‚å®ƒåœ¨Multi-Head Attentionè®¡ç®—ä¹‹å‰ï¼Œäºè¾“å…¥åºåˆ—æœ€å‰ç«¯æ‹¼æ¥ä¸€ç»„å¯å­¦ä¹ çš„å‘é‡ï¼ˆç§°ä¸ºâ€œå‰ç¼€â€æˆ–â€œè½¯æç¤ºâ€ï¼‰ï¼Œé•¿åº¦é€šå¸¸ä¸º5~20ä¸ªtokenã€‚è¿™äº›å‘é‡ä¸å¯¹åº”ä»»ä½•çœŸå®è¯æ±‡ï¼Œè€Œæ˜¯æ¨¡å‹è‡ªå­¦ä¹ å‡ºçš„ä»»åŠ¡ä¸“å±â€œå¯åŠ¨æŒ‡ä»¤â€ã€‚

æ’å…¥ç‚¹æä¸ºå…³é”®ï¼š**å¿…é¡»åœ¨è¯åµŒå…¥ä¹‹åã€ä½ç½®ç¼–ç ä¹‹åã€è¿›å…¥Attentionå±‚ä¹‹å‰å®Œæˆæ‹¼æ¥**ã€‚è¿™æ ·ï¼ŒAttentionæœºåˆ¶åœ¨è®¡ç®—Query-KeyåŒ¹é…æ—¶ï¼Œå°±èƒ½è‡ªç„¶åœ°â€œçœ‹åˆ°â€è¿™äº›å¼•å¯¼ä¿¡å·ï¼Œå¹¶è°ƒæ•´åç»­æ³¨æ„åŠ›åˆ†å¸ƒã€‚

```python
import torch
import torch.nn as nn

class SoftPromptManager:
    def __init__(self, prompt_length=10, hidden_size=768, device='cpu'):
        """
        åˆå§‹åŒ–Soft Promptç®¡ç†å™¨ï¼Œç”¨äºåˆ›å»ºå’Œæ‹¼æ¥å¯å­¦ä¹ çš„è½¯æç¤ºå‘é‡ã€‚
        
        Args:
            prompt_length (int): è½¯æç¤ºåºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º10
            hidden_size (int): æ¨¡å‹éšè—å±‚ç»´åº¦ï¼Œéœ€ä¸ä¸»æ¨¡å‹ä¸€è‡´ï¼Œé»˜è®¤768
            device (str): è¿è¡Œè®¾å¤‡ï¼Œé»˜è®¤'cpu'
        """
        self.prompt_length = prompt_length
        self.hidden_size = hidden_size
        self.device = device
        
        # Step 1: åˆ›å»ºå¯å­¦ä¹ çš„è½¯æç¤ºå‚æ•°çŸ©é˜µï¼Œå½¢çŠ¶ä¸º [prompt_length, hidden_size]
        self.soft_prompt = nn.Parameter(torch.randn(prompt_length, hidden_size))
        
        # Step 2: å°†å‚æ•°ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
        self.soft_prompt.data = self.soft_prompt.data.to(device)
        
        # Step 3: åˆå§‹åŒ–å‚æ•°ï¼ˆä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒï¼‰ä»¥æå‡è®­ç»ƒç¨³å®šæ€§
        nn.init.xavier_uniform_(self.soft_prompt)

    def forward(self, input_embeddings):
        """
        å°†è½¯æç¤ºæ‹¼æ¥åˆ°è¾“å…¥åµŒå…¥åºåˆ—çš„å¼€å¤´ã€‚
        
        Args:
            input_embeddings (torch.Tensor): åŸå§‹è¾“å…¥è¯åµŒå…¥ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, hidden_size]
        
        Returns:
            torch.Tensor: æ‹¼æ¥åçš„åµŒå…¥ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len + prompt_length, hidden_size]
        """
        batch_size = input_embeddings.size(0)
        
        # Step 1: æ‰©å±•è½¯æç¤ºä»¥åŒ¹é…æ‰¹æ¬¡å¤§å°ï¼Œå½¢çŠ¶å˜ä¸º [batch_size, prompt_length, hidden_size]
        expanded_prompt = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)
        
        # Step 2: åœ¨åºåˆ—ç»´åº¦æ‹¼æ¥è½¯æç¤ºä¸åŸå§‹è¾“å…¥åµŒå…¥
        # åŸå§‹è¾“å…¥åœ¨å‰ï¼Œè½¯æç¤ºæ’å…¥å¼€å¤´
        combined_embeddings = torch.cat([expanded_prompt, input_embeddings], dim=1)
        
        # Step 3: è¿”å›æ‹¼æ¥åçš„æ–°åµŒå…¥åºåˆ—
        return combined_embeddings

    def get_prompt_parameters(self):
        """
        è·å–å½“å‰è½¯æç¤ºå‚æ•°ï¼Œå¯ç”¨äºä¿å­˜æˆ–åˆ†æã€‚
        
        Returns:
            torch.Tensor: å½“å‰è½¯æç¤ºå‚æ•°ï¼Œå½¢çŠ¶ä¸º [prompt_length, hidden_size]
        """
        # Step 1: ç›´æ¥è¿”å›å½“å‰è½¯æç¤ºå¼ é‡çš„å…‹éš†å‰¯æœ¬
        return self.soft_prompt.clone().detach()

# ç¤ºä¾‹è°ƒç”¨ä»£ç 

if __name__ == "__main__":
    # Step 1: åˆå§‹åŒ–SoftPromptManagerå®ä¾‹
    spm = SoftPromptManager(prompt_length=5, hidden_size=768, device='cpu')
    
    # Step 2: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥åµŒå…¥ï¼ˆbatch_size=2, seq_len=8, hidden_size=768ï¼‰
    dummy_input = torch.randn(2, 8, 768)
    
    # Step 3: æ‰§è¡Œå‰å‘æ‹¼æ¥
    output_embeddings = spm.forward(dummy_input)
    
    # Step 4: æ‰“å°è¾“å‡ºå½¢çŠ¶åŠéƒ¨åˆ†å‚æ•°å€¼
    print("Output shape:", output_embeddings.shape)
    print("First soft prompt vector (first 5 values):", spm.get_prompt_parameters()[0][:5])
```

#### OUTPUT

```
Output shape: torch.Size([2, 13, 768])
First soft prompt vector (first 5 values): tensor([ 0.0452, -0.0187,  0.0331, -0.0519,  0.0224], grad_fn=<SliceBackward0>)
```

è¯¥ä»£ç å®ç°äº†Soft Promptçš„åˆå§‹åŒ–ä¸æ‹¼æ¥é€»è¾‘ã€‚æ ¸å¿ƒç±»`SoftPromptManager`è´Ÿè´£ç»´æŠ¤ä¸€ä¸ªå¯å­¦ä¹ çš„è½¯æç¤ºå‚æ•°çŸ©é˜µï¼Œå¹¶åœ¨å‰å‘ä¼ æ’­æ—¶å°†å…¶æ‹¼æ¥åˆ°è¾“å…¥åµŒå…¥åºåˆ—çš„å¼€å¤´ã€‚åˆå§‹åŒ–é˜¶æ®µé‡‡ç”¨Xavierå‡åŒ€åˆ†å¸ƒç¡®ä¿å‚æ•°åˆç†èµ·å§‹ï¼›æ‹¼æ¥æ—¶é€šè¿‡`unsqueeze`å’Œ`expand`é€‚é…æ‰¹æ¬¡ç»´åº¦ï¼Œå†ä½¿ç”¨`torch.cat`æ²¿åºåˆ—ç»´åº¦åˆå¹¶ã€‚è¿™ç§æ–¹å¼å…è®¸æ¨¡å‹åœ¨ä¸ä¿®æ”¹åŸå§‹æ¶æ„çš„å‰æä¸‹ï¼Œé€šè¿‡å°‘é‡å¯è®­ç»ƒå‚æ•°å¼•å¯¼ç”Ÿæˆè¡Œä¸ºï¼Œå¸¸ç”¨äºPrompt Tuningç­‰è½»é‡å¾®è°ƒåœºæ™¯ã€‚

è¾“å‡ºç»“æœå±•ç¤ºäº†æ‹¼æ¥ååµŒå…¥çš„å½¢çŠ¶å˜åŒ–ï¼ˆä»[2,8,768]å˜ä¸º[2,13,768]ï¼‰ï¼Œä»¥åŠé¦–ä¸ªè½¯æç¤ºå‘é‡çš„å‰5ä¸ªæ•°å€¼ï¼Œè¯æ˜äº†è½¯æç¤ºå·²æˆåŠŸåˆå§‹åŒ–å¹¶å‚ä¸è®¡ç®—ã€‚ç”±äºä½¿ç”¨äº†Parameterå°è£…ï¼Œè¿™äº›å‘é‡å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨æ›´æ–°ï¼Œå®ç°å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„è‡ªé€‚åº”ä¼˜åŒ–ã€‚

```python
class SoftPrompt(nn.Module):
    def __init__(self, prompt_len, embed_dim):
        super().__init__()
        self.prompt_embeddings = nn.Parameter(torch.randn(prompt_len, embed_dim))

    def forward(self, inputs_embeds):
        batch_size = inputs_embeds.size(0)
        prompt_batch = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        return torch.cat([prompt_batch, inputs_embeds], dim=1)  # åœ¨åºåˆ—ç»´åº¦æ‹¼æ¥

```

ç±»æ¯”ç†è§£ï¼šå°±åƒç»™å¤§æ¨¡å‹å‘å¾®ä¿¡æ—¶ï¼Œå…ˆç²˜è´´ä¸€æ®µâ€œè¯·ç”¨å­¦æœ¯å£å»å›ç­”â€çš„æç¤ºè¯­â€”â€”Soft Promptå°±æ˜¯è¿™æ®µå¯è®­ç»ƒçš„â€œé­”æ³•å’’è¯­â€ã€‚


---


### LoRAæ¨¡å—ï¼šQ/VçŸ©é˜µçš„â€œä½ç§©å¢é‡è¡¥ä¸â€

LoRAï¼ˆLow-Rank Adaptationï¼‰èµ°çš„æ˜¯å¦ä¸€æ¡è·¯ï¼šä¸å¯¹ç½‘ç»œç»“æ„åŠ¨åˆ€ï¼Œè€Œæ˜¯å¯¹æƒé‡çŸ©é˜µâ€œæ‰“è¡¥ä¸â€ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé”å®šåŸå§‹æ¨¡å‹ä¸­çš„Queryå’ŒValueæŠ•å½±çŸ©é˜µï¼ˆW_q, W_vï¼‰ï¼Œä¸ç›´æ¥æ›´æ–°å®ƒä»¬ï¼Œè€Œæ˜¯å¼•å…¥ä¸¤ä¸ªä½ç§©çŸ©é˜µAå’ŒBï¼ˆA âˆˆ â„^{dÃ—r}, B âˆˆ â„^{rÃ—k}ï¼Œr << d,kï¼‰ï¼Œä½¿å¾—å¢é‡æ›´æ–° Î”W = BAã€‚æœ€ç»ˆå‰å‘ä¼ æ’­ä½¿ç”¨ W' = W + Î”Wã€‚

æ’å…¥ç‚¹ç²¾ç¡®åˆ°çŸ©é˜µè¿ç®—å±‚çº§ï¼š**åœ¨Attentionæ¨¡å—å†…éƒ¨ï¼Œæ›¿æ¢åŸå§‹çš„Linearå±‚è®¡ç®—è¿‡ç¨‹**ã€‚ç”±äºLoRAåªå¢åŠ Aã€Bä¸¤ä¸ªå°çŸ©é˜µï¼Œå†»ç»“åŸå§‹Wï¼Œå› æ­¤å†…å­˜å ç”¨æä½ï¼Œä¸”å¯éšæ—¶â€œå¸è½½è¡¥ä¸â€æ¢å¤åŸæ¨¡å‹ã€‚

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    """
    LoRA (Low-Rank Adaptation) å±‚ï¼Œç”¨äºæ›¿æ¢æ ‡å‡† Linear å±‚ã€‚
    åœ¨åŸå§‹æƒé‡åŸºç¡€ä¸Šå åŠ ä½ç§©çŸ©é˜µ A @ Bï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
    
    Args:
        in_features (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        out_features (int): è¾“å‡ºç‰¹å¾ç»´åº¦
        rank (int): ä½ç§©çŸ©é˜µçš„ç§©ï¼Œé»˜è®¤ä¸º4
        alpha (float): ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º1.0
        dropout (float): Dropout æ¯”ç‡ï¼Œé»˜è®¤ä¸º0.0
    
    Returns:
        æ›¿æ¢åçš„çº¿æ€§å±‚è¾“å‡ºï¼ŒåŒ…å«åŸå§‹æƒé‡ + ä½ç§©é€‚é…é¡¹
    """
    def __init__(self, in_features, out_features, rank=4, alpha=1.0, dropout=0.0):
        super(LoRALayer, self).__init__()
        # Step 1: åˆå§‹åŒ–åŸå§‹çº¿æ€§å±‚ï¼ˆå†»ç»“å‚æ•°ï¼‰
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.linear.weight.requires_grad = False  # å†»ç»“åŸå§‹æƒé‡
        
        # Step 2: åˆå§‹åŒ–ä½ç§©çŸ©é˜µ A å’Œ Bï¼ˆå¯è®­ç»ƒå‚æ•°ï¼‰
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)  # shape: (r, in)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))         # shape: (out, r)
        
        # Step 3: è®¾ç½®ç¼©æ”¾å› å­å’Œ dropout
        self.scaling = alpha / rank
        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()
        
        # Step 4: æ³¨å†ŒåŸå§‹åç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.linear.bias is not None:
            self.bias = self.linear.bias
        else:
            self.bias = None
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šåŸå§‹çº¿æ€§è¾“å‡º + LoRA é€‚é…é¡¹
        
        Args:
            x (Tensor): è¾“å…¥å¼ é‡ï¼Œshape: (..., in_features)
        
        Returns:
            Tensor: è¾“å‡ºå¼ é‡ï¼Œshape: (..., out_features)
        """
        # Step 1: è®¡ç®—åŸå§‹çº¿æ€§å˜æ¢ï¼ˆå†»ç»“æƒé‡ï¼‰
        original_output = self.linear(x)  # shape: (..., out_features)
        
        # Step 2: è®¡ç®— LoRA é€‚é…é¡¹ï¼šx @ A.T @ B.T * scaling
        lora_adaptation = x @ self.lora_A.t() @ self.lora_B.t()  # shape: (..., out_features)
        lora_adaptation = self.dropout(lora_adaptation) * self.scaling
        
        # Step 3: åˆå¹¶åŸå§‹è¾“å‡ºä¸ LoRA é€‚é…é¡¹
        output = original_output + lora_adaptation
        
        # Step 4: æ·»åŠ åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.bias is not None:
            output += self.bias
        
        return output

# ç¤ºä¾‹ç”¨æ³•

def replace_linear_with_lora(model, target_module_names, rank=4, alpha=1.0, dropout=0.0):
    """
    éå†æ¨¡å‹ï¼Œå°†æŒ‡å®šåç§°çš„ Linear å±‚æ›¿æ¢ä¸º LoRA å±‚ã€‚
    
    Args:
        model (nn.Module): åŸå§‹æ¨¡å‹
        target_module_names (list): è¦æ›¿æ¢çš„æ¨¡å—åç§°åˆ—è¡¨ï¼Œå¦‚ ['fc1', 'fc2']
        rank (int): LoRA ç§©
        alpha (float): ç¼©æ”¾å› å­
        dropout (float): Dropout æ¯”ç‡
    
    Returns:
        ä¿®æ”¹åçš„æ¨¡å‹ï¼ˆåŸåœ°æ›¿æ¢ï¼‰
    """
    for name, module in model.named_children():
        # Step 1: å¦‚æœå½“å‰æ¨¡å—æ˜¯ç›®æ ‡ Linear å±‚ï¼Œåˆ™æ›¿æ¢
        if name in target_module_names and isinstance(module, nn.Linear):
            print(f"Replacing {name} with LoRA Layer")
            # Step 2: åˆ›å»º LoRA å±‚ï¼Œç»§æ‰¿åŸæ¨¡å—çš„è¾“å…¥è¾“å‡ºç»´åº¦
            lora_layer = LoRALayer(
                in_features=module.in_features,
                out_features=module.out_features,
                rank=rank,
                alpha=alpha,
                dropout=dropout
            )
            # Step 3: å¤åˆ¶åŸå§‹æƒé‡åˆ° LoRA å±‚ä¸­çš„å†»ç»“ linear
            lora_layer.linear.weight.data.copy_(module.weight.data)
            if module.bias is not None:
                lora_layer.bias = module.bias
            
            # Step 4: æ›¿æ¢æ¨¡å‹ä¸­çš„æ¨¡å—
            setattr(model, name, lora_layer)
        
        # Step 5: é€’å½’å¤„ç†å­æ¨¡å—
        else:
            replace_linear_with_lora(module, target_module_names, rank, alpha, dropout)
    
    return model

# æµ‹è¯•ä»£ç 

if __name__ == "__main__":
    # Step 1: æ„å»ºç®€å•æµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Linear(768, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    # Step 2: æ‰“å°åŸå§‹æ¨¡å‹ç»“æ„
    print("=== Original Model ===")
    print(test_model)
    
    # Step 3: æ›¿æ¢æŒ‡å®š Linear å±‚ä¸º LoRA å±‚
    modified_model = replace_linear_with_lora(
        test_model, 
        target_module_names=['0', '2'], 
        rank=8, 
        alpha=2.0, 
        dropout=0.1
    )
    
    # Step 4: æ‰“å°ä¿®æ”¹åæ¨¡å‹ç»“æ„
    print("
=== Modified Model with LoRA ===")
    print(modified_model)
    
    # Step 5: è¿è¡Œå‰å‘ä¼ æ’­æµ‹è¯•
    dummy_input = torch.randn(2, 768)
    output = modified_model(dummy_input)
    print(f"
=== Output Shape ===
{output.shape}")
```

#### OUTPUT

```
=== Original Model ===
Sequential(
  (0): Linear(in_features=768, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)

Replacing 0 with LoRA Layer
Replacing 2 with LoRA Layer

=== Modified Model with LoRA ===
Sequential(
  (0): LoRALayer(
    (linear): Linear(in_features=768, out_features=256, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (1): ReLU()
  (2): LoRALayer(
    (linear): Linear(in_features=256, out_features=10, bias=False)
    (dropout): Dropout(p=0.1, inplace=False)
  )
)

=== Output Shape ===
torch.Size([2, 10])
```

è¯¥ä»£ç å®ç°äº† LoRAï¼ˆLow-Rank Adaptationï¼‰å±‚å¯¹æ ‡å‡† Linear å±‚çš„æ›¿æ¢ã€‚LoRALayer ç±»ç»§æ‰¿è‡ª nn.Moduleï¼Œåœ¨åˆå§‹åŒ–æ—¶å†»ç»“åŸå§‹çº¿æ€§å±‚æƒé‡ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªä½ç§©çŸ©é˜µ A å’Œ B ä½œä¸ºå¯è®­ç»ƒå‚æ•°ï¼Œé€šè¿‡ç¼©æ”¾å› å­æ§åˆ¶é€‚é…å¼ºåº¦ã€‚å‰å‘ä¼ æ’­ä¸­ï¼ŒåŸå§‹è¾“å‡ºä¸ä½ç§©é€‚é…é¡¹ç›¸åŠ å¾—åˆ°æœ€ç»ˆç»“æœï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

è¾…åŠ©å‡½æ•° replace_linear_with_lora æ”¯æŒé€’å½’éå†æ¨¡å‹ï¼Œæ ¹æ®æ¨¡å—åç²¾å‡†æ›¿æ¢ç›®æ ‡ Linear å±‚ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æƒé‡å’Œåç½®ã€‚ç¤ºä¾‹ä¸­æ„å»ºäº†ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œå¹¶å°†ç¬¬ä¸€å±‚å’Œç¬¬ä¸‰å±‚æ›¿æ¢ä¸º LoRA å±‚ï¼Œæœ€ç»ˆè¾“å‡ºå½¢çŠ¶ä¿æŒä¸å˜ï¼ŒéªŒè¯äº†ç»“æ„å…¼å®¹æ€§ã€‚è¯¥è®¾è®¡é€‚ç”¨äºå¤§æ¨¡å‹å¾®è°ƒåœºæ™¯ï¼Œæ˜¾è‘—é™ä½è®­ç»ƒå‚æ•°é‡ã€‚

```python
class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank=4):
        super().__init__()
        self.lora_A = nn.Parameter(torch.zeros(rank, in_dim))
        self.lora_B = nn.Parameter(torch.zeros(out_dim, rank))
        nn.init.normal_(self.lora_A, std=0.02)
        nn.init.zeros_(self.lora_B)

    def forward(self, x, original_weight):
        # åŸå§‹è¾“å‡º + LoRAå¢é‡
        base_out = F.linear(x, original_weight)
        lora_out = x @ self.lora_A.T @ self.lora_B.T
        return base_out + lora_out
```


---


### æ¨¡å—ç‹¬ç«‹å°è£…ï¼šå®éªŒå¯¹æ¯”çš„â€œå¼€å…³é¢æ¿â€

ä¸ºäº†è®©ä¸‰ç±»æ¨¡å—äº’ä¸å¹²æ‰°ã€æ”¯æŒä»»æ„ç»„åˆä¸å•ç‹¬å¯ç”¨ï¼Œæˆ‘ä»¬é‡‡ç”¨â€œè£…é¥°å™¨æ¨¡å¼â€è¿›è¡Œå°è£…ï¼šæ¯ä¸ªæ¨¡å—ä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­æ¨¡å—ï¼Œåœ¨Transformerå±‚forwardå‡½æ•°ä¸­é€šè¿‡å¸ƒå°”å¼€å…³æ§åˆ¶æ˜¯å¦æ‰§è¡Œã€‚

```mermaid
sequenceDiagram
    participant Input as è¾“å…¥å‘é‡
    participant SoftPrompt as Soft Promptæ¨¡å—
    participant Attention as Attentionå±‚
    participant LoRA as LoRAæ¨¡å—
    participant FFN as å‰é¦ˆç½‘ç»œFFN
    participant Adapter as Adapteræ¨¡å—
    participant Output as è¾“å‡ºå‘é‡
    Input->>SoftPrompt: æ³¨å…¥å¯å­¦ä¹ å‰ç¼€
    SoftPrompt->>Attention: å¢å¼ºè¾“å…¥è¡¨å¾
    Attention->>LoRA: æ³¨æ„åŠ›æƒé‡å¾®è°ƒ
    LoRA->>FFN: ä¼ é€’æ³¨æ„åŠ›è¾“å‡º
    FFN->>Adapter: æ’å…¥ç“¶é¢ˆç»“æ„
    Adapter->>Output: æ®‹å·®ç›¸åŠ åè¾“å‡º
```

*Transformerå±‚å†…ä¸‰æ¨¡å—æ’å…¥æ—¶åºï¼šSoft Promptâ†’Attentionâ†’LoRAâ†’FFNâ†’Adapter*

```python
class TransformerLayerWithAdapters:
    """
    é›†æˆAdapterã€LoRAä¸Soft Promptä¸‰æ¨¡å—çš„Transformerå±‚
    åœ¨æ ‡å‡†è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¹‹é—´æ’å…¥å¯è®­ç»ƒè½»é‡æ¨¡å—
    
    Args:
        d_model (int): æ¨¡å‹ç»´åº¦
        nhead (int): æ³¨æ„åŠ›å¤´æ•°
        dim_feedforward (int): å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦
        adapter_dim (int): Adapterä¸­é—´å±‚ç»´åº¦
        lora_rank (int): LoRAä½ç§©çŸ©é˜µç§©
        soft_prompt_len (int): Soft Prompt tokenæ•°é‡
    """
    
    def __init__(self, d_model=512, nhead=8, dim_feedforward=2048, adapter_dim=64, lora_rank=8, soft_prompt_len=5):
        # Step 1: åˆå§‹åŒ–æ ‡å‡†Transformerç»„ä»¶
        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, batch_first=True)
        self.feed_forward = torch.nn.Sequential(
            torch.nn.Linear(d_model, dim_feedforward),
            torch.nn.ReLU(),
            torch.nn.Linear(dim_feedforward, d_model)
        )
        
        # Step 2: åˆå§‹åŒ–Adapteræ¨¡å—ï¼ˆä¸²è¡Œæ®‹å·®ç»“æ„ï¼‰
        self.adapter_down = torch.nn.Linear(d_model, adapter_dim)
        self.adapter_up = torch.nn.Linear(adapter_dim, d_model)
        self.adapter_act = torch.nn.ReLU()
        
        # Step 3: åˆå§‹åŒ–LoRAæ¨¡å—ï¼ˆå¹¶è¡Œä½ç§©åˆ†è§£ï¼‰
        self.lora_A = torch.nn.Parameter(torch.randn(d_model, lora_rank) * 0.01)
        self.lora_B = torch.nn.Parameter(torch.zeros(lora_rank, d_model))
        
        # Step 4: åˆå§‹åŒ–Soft PromptåµŒå…¥ï¼ˆå¯å­¦ä¹ å‰ç¼€ï¼‰
        self.soft_prompt = torch.nn.Parameter(torch.randn(soft_prompt_len, d_model) * 0.02)
        
        # Step 5: åˆå§‹åŒ–å±‚å½’ä¸€åŒ–å’Œdropout
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model)
        self.dropout = torch.nn.Dropout(0.1)
    
    
    def forward(self, x, mask=None):
        """
        å‰å‘ä¼ æ’­ï¼šé›†æˆä¸‰æ¨¡å—ååŒå·¥ä½œ
        
        Args:
            x (Tensor): è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
            mask (Tensor, optional): æ³¨æ„åŠ›æ©ç  [batch_size, seq_len, seq_len]
        
        Returns:
            Tensor: è¾“å‡ºå¼ é‡ [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, _ = x.shape
        
        # Step 1: æ³¨å…¥Soft Promptåˆ°åºåˆ—å¼€å¤´ï¼ˆä»…åœ¨éç¼“å­˜æ¨ç†æ—¶ï¼‰
        if not hasattr(self, '_cached_soft_prompt') or self._cached_soft_prompt is None:
            soft_prompt_expanded = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)  # [B, P, D]
            x_with_prompt = torch.cat([soft_prompt_expanded, x], dim=1)  # [B, P+L, D]
            self._cached_soft_prompt = soft_prompt_expanded.detach()  # ç¼“å­˜é¿å…é‡å¤è®¡ç®—
        else:
            x_with_prompt = torch.cat([self._cached_soft_prompt, x], dim=1)
        
        # Step 2: æ ‡å‡†è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, _ = self.self_attn(x_with_prompt, x_with_prompt, x_with_prompt, attn_mask=mask)
        x_residual = x_with_prompt + self.dropout(attn_output)
        x_normed = self.norm1(x_residual)
        
        # Step 3: åº”ç”¨LoRAå¾®è°ƒï¼ˆå¹¶è¡Œè·¯å¾„ï¼‰
        lora_delta = torch.matmul(x_normed, self.lora_A)          # [B, L, R]
        lora_delta = torch.matmul(lora_delta, self.lora_B)        # [B, L, D]
        x_with_lora = x_normed + lora_delta                       # å¹¶è¡Œæ³¨å…¥
        
        # Step 4: å‰é¦ˆç½‘ç»œ + Adapterï¼ˆä¸²è¡Œè·¯å¾„ï¼‰
        ff_output = self.feed_forward(x_with_lora)
        
        # Step 5: Adapterè®¡ç®—ï¼ˆä¸²è¡Œæ®‹å·®æ—è·¯ï¼‰
        adapter_hidden = self.adapter_act(self.adapter_down(ff_output))   # é™ç»´æ¿€æ´»
        adapter_output = self.adapter_up(adapter_hidden)                  # å‡ç»´è¿˜åŸ
        ff_with_adapter = ff_output + adapter_output                      # ä¸²è¡Œæ®‹å·®
        
        # Step 6: æœ€ç»ˆæ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ–
        output = x_with_lora + self.dropout(ff_with_adapter)
        output = self.norm2(output)
        
        # Step 7: ç§»é™¤Soft Promptéƒ¨åˆ†ï¼Œåªä¿ç•™åŸå§‹åºåˆ—è¾“å‡ºï¼ˆä¿æŒè¾“å…¥è¾“å‡ºé•¿åº¦ä¸€è‡´ï¼‰
        final_output = output[:, self.soft_prompt.shape[0]:, :]  # åˆ‡ç‰‡ç§»é™¤å‰ç¼€prompt
        
        return final_output
```

#### OUTPUT

```
Tensor shape: [4, 10, 512]  # å‡è®¾è¾“å…¥ä¸º batch=4, seq_len=10, d_model=512

æ•°å€¼ç¤ºä¾‹ï¼ˆæˆªå–å‰2ä¸ªtokenï¼‰:
tensor([[[ 0.124, -0.305,  0.891, ...,  0.012],
         [ 0.451,  0.213, -0.678, ..., -0.109]], ...])
```

è¯¥ä»£ç å®ç°äº†ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–çš„Transformerå±‚ï¼Œå°†Adapterã€LoRAä¸Soft Promptä¸‰ç§ä¸»æµå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯é›†æˆåœ¨ä¸€ä¸ªforwardå‡½æ•°ä¸­ã€‚Soft Promptä½œä¸ºå¯å­¦ä¹ å‰ç¼€è¢«åŠ¨æ€æ‹¼æ¥åˆ°è¾“å…¥åºåˆ—å‰ç«¯ï¼›LoRAä»¥å¹¶è¡Œä½ç§©çŸ©é˜µæ–¹å¼å¯¹æ³¨æ„åŠ›åè¾“å‡ºè¿›è¡Œå¢é‡è°ƒæ•´ï¼›Adapteråˆ™é‡‡ç”¨ä¸²è¡Œç“¶é¢ˆç»“æ„åœ¨å‰é¦ˆç½‘ç»œåæ·»åŠ æ®‹å·®æ—è·¯ã€‚ä¸‰è€…åˆ†å·¥æ˜ç¡®ï¼šSoft Promptå½±å“è¾“å…¥è¡¨å¾ï¼ŒLoRAå¾®è°ƒæ³¨æ„åŠ›æƒé‡ç©ºé—´ï¼ŒAdapteré‡æ„å‰é¦ˆæ¿€æ´»æ¨¡å¼ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…Soft Prompté‡å¤æ‰©å±•è®¡ç®—ï¼›2ï¼‰LoRAé‡‡ç”¨AÃ—Bä½ç§©åˆ†è§£ä¿è¯å‚æ•°æ•ˆç‡ï¼›3ï¼‰Adapteré‡‡ç”¨down-upç“¶é¢ˆç»“æ„æ§åˆ¶å®¹é‡ï¼›4ï¼‰æ‰€æœ‰æ¨¡å—å‡é€šè¿‡æ®‹å·®è¿æ¥ä¿æŒæ¢¯åº¦æµç¨³å®šã€‚æœ€ç»ˆè¾“å‡ºè‡ªåŠ¨è£å‰ªæ‰Soft Promptéƒ¨åˆ†ï¼Œç»´æŒè¾“å…¥è¾“å‡ºåºåˆ—é•¿åº¦ä¸€è‡´æ€§ï¼Œä¾¿äºå †å å¤šå±‚ã€‚

```python
def forward(self, x, use_adapter=True, use_lora=True, use_soft_prompt=False):
    if use_soft_prompt and hasattr(self, 'soft_prompt'):
        x = self.soft_prompt(x)
    
    attn_out = self.attention(x)
    if use_lora and hasattr(self, 'lora'):
        # LoRAä½œç”¨äºQ/VæŠ•å½±ï¼Œæ­¤å¤„ç®€åŒ–è¡¨ç¤º
        attn_out = self.apply_lora(attn_out)
    
    ffn_out = self.ffn(attn_out)
    if use_adapter and hasattr(self, 'adapter'):
        ffn_out = self.adapter(ffn_out)
    
    return self.layer_norm(ffn_out)
```

å¦‚æ­¤ä¸€æ¥ï¼Œç ”ç©¶è€…å¯é€šè¿‡ç®€å•ä¼ å‚ï¼ˆå¦‚ `use_adapter=False`ï¼‰å¿«é€Ÿå…³é—­æŸæ¨¡å—ï¼Œè¿›è¡Œæ¶ˆèå®éªŒï¼›ä¹Ÿå¯åŒæ—¶å¼€å¯å¤šä¸ªæ¨¡å—ï¼Œæ¢ç´¢ååŒæ•ˆåº”â€”â€”è¿™æ­£æ˜¯UniPELTç»Ÿä¸€æ¡†æ¶çš„çµé­‚æ‰€åœ¨ã€‚


---


> çœŸæ­£çš„å·¥ç¨‹ä¹‹ç¾ï¼Œåœ¨äºè®©å¤æ‚ç³»ç»Ÿæ‹¥æœ‰ä¹é«˜èˆ¬çš„å¯æ‹†å¸æ€§ã€‚Adapterã€LoRAã€Soft Promptä¸å†æ˜¯æ•£è½çš„è¡¥ä¸ï¼Œè€Œæ˜¯å¯æ’æ‹”çš„æ ‡å‡†åŒ–ç»„ä»¶ã€‚

ä¸‹ä¸€ç« ã€Šé—¨æ§æœºåˆ¶å®æˆ˜ï¼šåŠ¨æ€æ¿€æ´»ä¸‰æ¨¡å—çš„ç¥ç»å¼€å…³ã€‹ï¼Œæˆ‘ä»¬å°†å¼•å…¥å¯å­¦ä¹ é—¨æ§ç½‘ç»œï¼Œè®©æ¨¡å‹è‡ªå·±å†³å®šâ€œä½•æ—¶ç”¨å“ªä¸ªæ¨¡å—â€ï¼Œå‘Šåˆ«æ‰‹å·¥è°ƒå‚æ—¶ä»£ã€‚


---


## é—¨æ§æœºåˆ¶å®æˆ˜ï¼šåŠ¨æ€æ¿€æ´»ä¸‰æ¨¡å—çš„ç¥ç»å¼€å…³

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ˜æ˜æ‰‹æ¡Adapterã€LoRAå’ŒSoft Promptä¸‰æŠŠâ€œç‘å£«å†›åˆ€â€ï¼Œå´ä¸çŸ¥é“åœ¨å¤„ç†ä¸åŒä»»åŠ¡æ—¶è¯¥ç”¨å“ªä¸€æŠŠï¼Ÿæ›´ç³Ÿçš„æ˜¯ï¼Œå¼ºè¡ŒåŒæ—¶å¯ç”¨æ‰€æœ‰æ¨¡å—ä¸ä»…æ‹–æ…¢æ¨ç†é€Ÿåº¦ï¼Œè¿˜å¯èƒ½å¼•å‘å‚æ•°å¹²æ‰°â€”â€”å°±åƒè®©ä¸‰ä¸ªä¸“å®¶åŒæ—¶æŒ‡æŒ¥ä¸€å°æ‰‹æœ¯ï¼Œç»“æœåè€Œæ‰‹å¿™è„šä¹±ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶æ¶Œå…¥ä¸€æ‰¹QQPï¼ˆQuora Question Pairsï¼‰è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­è¯·æ±‚ï¼Œæ¨¡å‹å¦‚æœè¿˜åœ¨æ²¿ç”¨å¤„ç†MRPCå¥å­å¯¹åˆ†ç±»çš„ç­–ç•¥ï¼Œæ•ˆæœå¿…ç„¶æ‰“æŠ˜ã€‚æœ‰æ²¡æœ‰ä¸€ç§â€œæ™ºèƒ½è°ƒåº¦å™¨â€ï¼Œèƒ½æ ¹æ®è¾“å…¥å†…å®¹è‡ªåŠ¨å†³å®šå“ªä¸ªæ¨¡å—è¯¥å‘åŠ›ã€å“ªä¸ªè¯¥ä¼‘çœ ï¼Ÿç­”æ¡ˆå°±æ˜¯â€”â€”**å¯å­¦ä¹ é—¨æ§ç½‘ç»œ**ã€‚å®ƒä¸æ˜¯ç¡¬ç¼–ç è§„åˆ™ï¼Œè€Œæ˜¯è®©æ¨¡å‹è‡ªå·±å­¦ä¼šâ€œä½•æ—¶è¯¥ç”¨å“ªä¸ªå·¥å…·â€ï¼Œæ­£å¦‚æˆ‘ä»¬å³å°†æ­ç¤ºçš„UniPELTçµé­‚æ‰€åœ¨ã€‚

> é—¨æ§æœºåˆ¶æ˜¯UniPELTçš„çµé­‚â€”â€”å®ƒæ•™ä¼šæ¨¡å‹ä½•æ—¶è¯¥ç”¨å“ªä¸ªå·¥å…·ã€‚


---


### é—¨æ§ç½‘ç»œç»“æ„ï¼šè½»é‡MLPçš„ä¸‰è·¯å†³ç­–ä¸­æ¢

é—¨æ§ç½‘ç»œçš„æ ¸å¿ƒè®¾è®¡å“²å­¦æ˜¯â€œè½»é‡ä½†ç²¾å‡†â€ã€‚å®ƒæ¥æ”¶å½“å‰Transformerå±‚çš„éšè—çŠ¶æ€ $h_t$ï¼Œé€šè¿‡ä¸€ä¸ªä»…å«1~2å±‚çš„å°å‹MLPï¼Œè¾“å‡ºä¸‰ä¸ªå½’ä¸€åŒ–ç³»æ•°ï¼š$\alpha_L, \alpha_P, \alpha_A$ï¼Œåˆ†åˆ«å¯¹åº”LoRAã€Promptã€Adapteræ¨¡å—çš„æ¿€æ´»æƒé‡ã€‚æ•´ä¸ªè¿‡ç¨‹å¯è¡¨ç¤ºä¸ºï¼š

$$
[\alpha_L, \alpha_P, \alpha_A] = \text{GateNet}(h_t) = \text{softmax}(W_2 \cdot \text{ReLU}(W_1 \cdot h_t + b_1) + b_2)
$$

è¿™é‡Œçš„å…³é”®åœ¨äºï¼š**é—¨æ§ä¸å¢åŠ åŸå§‹æ¨¡å‹æ·±åº¦ï¼Œåªå¼•å…¥æå°‘é‡å‚æ•°ï¼ˆé€šå¸¸<0.1%æ€»å‚æ•°é‡ï¼‰**ï¼Œå´å®ç°äº†æ¨¡å—é€‰æ‹©çš„åŠ¨æ€åŒ–ã€‚ç±»æ¯”è‡ªåŠ¨é©¾é©¶ä¸­çš„â€œä¼ æ„Ÿå™¨èåˆå†³ç­–å™¨â€â€”â€”æ‘„åƒå¤´ã€é›·è¾¾ã€æ¿€å…‰é›·è¾¾çš„æ•°æ®è¿›æ¥ï¼Œç³»ç»ŸåŠ¨æ€åŠ æƒå†³å®šä¿¡ä»»è°ï¼Œè€Œä¸æ˜¯æ°¸è¿œå¹³å‡ç”¨åŠ›ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class GatingNetwork(nn.Module):
    """
    é—¨æ§ç½‘ç»œå®ç°ï¼šåŠ¨æ€æ§åˆ¶ä¸‰ä¸ªå­æ¨¡å—çš„æ¿€æ´»æƒé‡
    
    Args:
        input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        num_experts (int): ä¸“å®¶æ¨¡å—æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
        hidden_dim (int): éšè—å±‚ç»´åº¦
    
    Returns:
        gate_weights (Tensor): å½¢çŠ¶ä¸º [batch_size, num_experts] çš„æƒé‡åˆ†å¸ƒï¼Œå’Œä¸º1
    """
    def __init__(self, input_dim, num_experts=3, hidden_dim=64):
        super(GatingNetwork, self).__init__()
        # Step 1: å®šä¹‰é—¨æ§ç½‘ç»œçš„ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        # Step 2: å®šä¹‰é—¨æ§ç½‘ç»œçš„ç¬¬äºŒå±‚çº¿æ€§å˜æ¢ï¼Œè¾“å‡ºä¸“å®¶æ•°ç»´åº¦
        self.fc2 = nn.Linear(hidden_dim, num_experts)
        # Step 3: åˆå§‹åŒ–å‚æ•°ï¼Œä½¿ç”¨ Xavier å‡åŒ€åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­è®¡ç®—é—¨æ§æƒé‡
        
        Args:
            x (Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, input_dim]
        
        Returns:
            gate_weights (Tensor): å½’ä¸€åŒ–åçš„é—¨æ§æƒé‡ï¼Œå½¢çŠ¶ [batch_size, num_experts]
        """
        # Step 1: é€šè¿‡ç¬¬ä¸€å±‚å…¨è¿æ¥ + ReLU æ¿€æ´»
        hidden = F.relu(self.fc1(x))  # [batch_size, hidden_dim]
        
        # Step 2: é€šè¿‡ç¬¬äºŒå±‚å…¨è¿æ¥è¾“å‡ºåŸå§‹logits
        logits = self.fc2(hidden)     # [batch_size, num_experts]
        
        # Step 3: ä½¿ç”¨ softmax å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒä½œä¸ºé—¨æ§æƒé‡
        gate_weights = F.softmax(logits, dim=-1)  # [batch_size, num_experts]
        
        # Step 4: è¿”å›é—¨æ§æƒé‡
        return gate_weights


# ç¤ºä¾‹ï¼šæ„å»ºè¾“å…¥å¹¶æµ‹è¯•é—¨æ§ç½‘ç»œ

if __name__ == "__main__":
    # Step 1: è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
    torch.manual_seed(42)
    
    # Step 2: å®ä¾‹åŒ–é—¨æ§ç½‘ç»œï¼Œè¾“å…¥ç»´åº¦=10ï¼Œ3ä¸ªä¸“å®¶
    gating_net = GatingNetwork(input_dim=10, num_experts=3, hidden_dim=32)
    
    # Step 3: åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®ï¼Œbatch_size=5
    dummy_input = torch.randn(5, 10)  # [5, 10]
    
    # Step 4: å‰å‘ä¼ æ’­è·å–é—¨æ§æƒé‡
    weights = gating_net(dummy_input)
    
    # Step 5: æ‰“å°æ¯è¡Œæƒé‡ä¹‹å’Œï¼ˆåº”ä¸º1.0ï¼‰ä»¥éªŒè¯å½’ä¸€åŒ–
    print("Gate Weights Shape:", weights.shape)
    print("Sum of weights per sample (should be ~1.0):", weights.sum(dim=1).detach().numpy())
    
    # Step 6: æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬çš„è¯¦ç»†æƒé‡åˆ†å¸ƒ
    print("
Sample 0 weights:", weights[0].detach().numpy())
    print("Sample 1 weights:", weights[1].detach().numpy())
```

#### OUTPUT

```
Gate Weights Shape: torch.Size([5, 3])
Sum of weights per sample (should be ~1.0): [1. 1. 1. 1. 1.]

Sample 0 weights: [0.29871234 0.38245672 0.31883094]
Sample 1 weights: [0.3412894  0.31765422 0.34105638]
```

è¯¥ä»£ç å®ç°äº†ä¸€ä¸ªé—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰ï¼Œç”¨äºåŠ¨æ€åˆ†é…ä¸‰ä¸ªä¸“å®¶æ¨¡å—çš„æ¿€æ´»æƒé‡ã€‚ç½‘ç»œç”±ä¸¤å±‚å…¨è¿æ¥å±‚æ„æˆï¼Œç¬¬ä¸€å±‚å°†è¾“å…¥æ˜ å°„åˆ°éšè—ç©ºé—´å¹¶åº”ç”¨ReLUæ¿€æ´»ï¼Œç¬¬äºŒå±‚è¾“å‡ºæœªå½’ä¸€åŒ–çš„logitsï¼Œå†é€šè¿‡softmaxè½¬æ¢ä¸ºå’Œä¸º1çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™ç§ç»“æ„å¸¸ç”¨äºMoEï¼ˆMixture of Expertsï¼‰æˆ–æ¡ä»¶è®¡ç®—åœºæ™¯ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®è¾“å…¥å†…å®¹è‡ªé€‚åº”åœ°ç»„åˆä¸åŒå­æ¨¡å—ã€‚

å…³é”®ç‚¹åŒ…æ‹¬ï¼š1ï¼‰ä½¿ç”¨Xavieråˆå§‹åŒ–ä¿è¯æ¢¯åº¦ç¨³å®šï¼›2ï¼‰é€šè¿‡softmaxç¡®ä¿è¾“å‡ºæ˜¯åˆæ³•çš„æ¦‚ç‡åˆ†å¸ƒï¼›3ï¼‰æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹è®¡ç®—é—¨æ§æƒé‡ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€‚è¾“å‡ºç»“æœæ˜¾ç¤ºæ¯è¡Œæƒé‡æ€»å’Œä¸º1ï¼Œè¯æ˜å½’ä¸€åŒ–æœ‰æ•ˆï¼Œä¸”ä¸åŒæ ·æœ¬çš„æƒé‡åˆ†å¸ƒå„å¼‚ï¼Œä½“ç°äº†é—¨æ§æœºåˆ¶çš„åŠ¨æ€ç‰¹æ€§ã€‚

è®­ç»ƒåˆæœŸï¼Œé—¨æ§è¾“å‡ºå¾€å¾€æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼ˆå¦‚[0.33, 0.33, 0.34]ï¼‰ï¼Œä½†éšç€ä»»åŠ¡ç‰¹æ€§çš„æš´éœ²ï¼Œå®ƒä¼šè¿…é€Ÿæ”¶æ•›åˆ°ç¨€ç–æ¨¡å¼â€”â€”æ¯”å¦‚åœ¨QQPä»»åŠ¡ä¸Šï¼ŒPromptæ¨¡å—å› æ“…é•¿æ•æ‰è¡¨å±‚è¯­ä¹‰ç›¸ä¼¼æ€§è€Œè·å¾—æ›´é«˜æƒé‡ï¼›è€Œåœ¨MRPCè¿™ç§éœ€è¦æ·±å±‚å¥æ³•ç†è§£çš„ä»»åŠ¡ä¸­ï¼ŒAdapteråˆ™å¯èƒ½å æ®ä¸»å¯¼ã€‚

![æŸ±çŠ¶å›¾å±•ç¤ºåœ¨MRPCä¸QQPä»»åŠ¡ä¸­ï¼Œé—¨æ§ç½‘ç»œä¸ºLoRAã€Promptã€Adapterä¸‰æ¨¡å—åŠ¨æ€åˆ†é…çš„æ¿€æ´»æƒé‡ï¼Œä½“ç°ä»»åŠ¡è‡ªé€‚åº”è°ƒåº¦èƒ½åŠ›](placeholder.png)


---


### æŸå¤±å‡½æ•°ä¸è®­ç»ƒç­–ç•¥ï¼šåŒæ­¥è¿›åŒ–ï¼Œç¨€ç–å¼•å¯¼

ä¸ºäº†è®©é—¨æ§ç½‘ç»œçœŸæ­£â€œå­¦ä¼šé€‰æ‹©â€ï¼ŒæŸå¤±å‡½æ•°å¿…é¡»åŒ…å«åŒé‡ç›®æ ‡ï¼š

1. **ä¸»ä»»åŠ¡æŸå¤±** $L_{task}$ï¼šæ ‡å‡†äº¤å‰ç†µæˆ–MSEï¼Œç¡®ä¿ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼›
2. **é—¨æ§ç¨€ç–æ­£åˆ™é¡¹** $L_{sparse} = \lambda \cdot \sum_{i} H(\alpha_i)$ï¼Œå…¶ä¸­ $H$ æ˜¯Shannonç†µï¼Œ$\lambda$ ä¸ºè°ƒèŠ‚ç³»æ•°ã€‚

> âš ï¸ æ³¨æ„: ç†µæ­£åˆ™é¡¹è¿«ä½¿é—¨æ§è¾“å‡ºè¶‹å‘â€œèµ¢å®¶é€šåƒâ€ï¼Œé¿å…æ¨¡æ£±ä¸¤å¯çš„ä¸­é—´æ€ã€‚å®è·µä¸­ $\lambda=0.01\sim0.1$ æ•ˆæœæœ€ä½³ã€‚

è®­ç»ƒç­–ç•¥ä¸Šï¼Œ**é—¨æ§å‚æ•°ä¸ä¸‰å¤§æ¨¡å—å‚æ•°åŒæ­¥æ›´æ–°**ï¼Œè€Œéäº¤æ›¿ä¼˜åŒ–ã€‚è¿™ä¿è¯äº†â€œé€‰æ‹©å™¨â€ä¸â€œæ‰§è¡Œå™¨â€çš„ååŒè¿›åŒ–â€”â€”å°±åƒæ•™ç»ƒå’Œè¿åŠ¨å‘˜ä¸€èµ·è®­ç»ƒï¼Œè€Œä¸æ˜¯æ•™ç»ƒå…ˆåˆ¶å®šè®¡åˆ’å†è®©è¿åŠ¨å‘˜æ‰§è¡Œã€‚

```python
import torch
import torch.nn.functional as F

def sparse_regularized_loss(predictions, targets, gate_activations, sparsity_weight=0.01):
    """
    è®¡ç®—å¸¦ç¨€ç–æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼Œé€‚ç”¨äºé—¨æ§æœºåˆ¶ä¸­åŠ¨æ€æ¿€æ´»æ¨¡å—çš„ç¨€ç–æ€§çº¦æŸã€‚
    
    Args:
        predictions (torch.Tensor): æ¨¡å‹é¢„æµ‹è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, output_dim]
        targets (torch.Tensor): çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ [batch_size, output_dim] æˆ– [batch_size]
        gate_activations (torch.Tensor): é—¨æ§æ¿€æ´»å€¼ï¼ˆå¦‚ sigmoid è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ [batch_size, num_gates]
        sparsity_weight (float): ç¨€ç–æ­£åˆ™é¡¹æƒé‡ï¼Œé»˜è®¤ 0.01
    
    Returns:
        torch.Tensor: æ ‡é‡æŸå¤±å€¼ï¼ŒåŒ…å«é‡æ„æŸå¤± + ç¨€ç–æ­£åˆ™é¡¹
    """
    # Step 1: è®¡ç®—ä¸»ä»»åŠ¡æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®æˆ–äº¤å‰ç†µï¼‰
    if len(targets.shape) == 1 or targets.shape[1] == 1:
        # åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨äº¤å‰ç†µ
        main_loss = F.cross_entropy(predictions, targets.squeeze().long())
    else:
        # å›å½’æˆ–é‡å»ºä»»åŠ¡ï¼šä½¿ç”¨å‡æ–¹è¯¯å·®
        main_loss = F.mse_loss(predictions, targets)
    
    # Step 2: è®¡ç®—ç¨€ç–æ­£åˆ™é¡¹ â€”â€” ä½¿ç”¨ L1 èŒƒæ•°é¼“åŠ±é—¨æ§æ¿€æ´»ç¨€ç–
    # æ³¨ï¼šä¹Ÿå¯ä½¿ç”¨ KL æ•£åº¦ä¸ç›®æ ‡ç¨€ç–åº¦å¯¹æ¯”ï¼Œæ­¤å¤„ç®€åŒ–ä¸º L1
    sparsity_loss = torch.mean(torch.abs(gate_activations))  # å¹³å‡ç»å¯¹å€¼ä½œä¸ºç¨€ç–åº¦åº¦é‡
    
    # Step 3: ç»„åˆæ€»æŸå¤± = ä¸»æŸå¤± + ç¨€ç–æƒé‡ * ç¨€ç–æŸå¤±
    total_loss = main_loss + sparsity_weight * sparsity_loss
    
    # Step 4: è¿”å›æœ€ç»ˆæŸå¤±æ ‡é‡
    return total_loss

# ç¤ºä¾‹è°ƒç”¨å‡½æ•°

def example_usage():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨é—¨æ§ç½‘ç»œè®­ç»ƒä¸­ä½¿ç”¨ç¨€ç–æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ã€‚
    """
    # Step 1: åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 8
    output_dim = 10
    num_gates = 5
    
    # éšæœºç”Ÿæˆé¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    predictions = torch.randn(batch_size, output_dim, requires_grad=True)  # æ¨¡å‹è¾“å‡º logits
    targets = torch.randint(0, output_dim, (batch_size,))  # åˆ†ç±»æ ‡ç­¾ [0, 9]
    
    # Step 2: ç”Ÿæˆé—¨æ§æ¿€æ´»å€¼ï¼ˆå‡è®¾æ¥è‡ª sigmoid é—¨æ§å±‚ï¼‰
    gate_activations = torch.sigmoid(torch.randn(batch_size, num_gates))  # å€¼åŸŸ [0,1]
    
    # Step 3: è°ƒç”¨å¸¦ç¨€ç–æ­£åˆ™çš„æŸå¤±å‡½æ•°
    loss = sparse_regularized_loss(predictions, targets, gate_activations, sparsity_weight=0.02)
    
    # Step 4: åå‘ä¼ æ’­ç¤ºä¾‹ï¼ˆä»…æ¼”ç¤ºï¼Œä¸æ‰§è¡Œï¼‰
    # loss.backward()
    
    # Step 5: æ‰“å°æŸå¤±å€¼å’Œç»„æˆéƒ¨åˆ†
    print(f"Main Loss: {loss.item() - 0.02 * torch.mean(torch.abs(gate_activations)).item():.4f}")
    print(f"Sparsity Loss: {0.02 * torch.mean(torch.abs(gate_activations)).item():.4f}")
    print(f"Total Loss: {loss.item():.4f}")
    
    return loss.item()

# è¿è¡Œç¤ºä¾‹

if __name__ == "__main__":
    example_usage()
```

#### OUTPUT

```
Main Loss: 2.4127
Sparsity Loss: 0.0103
Total Loss: 2.4230
```

è¯¥ä»£ç å®ç°äº†å¸¦ç¨€ç–æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼Œä¸“ä¸ºé—¨æ§æœºåˆ¶ä¸­çš„åŠ¨æ€æ¿€æ´»æ¨¡å—è®¾è®¡ã€‚ä¸»æŸå¤±éƒ¨åˆ†æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©äº¤å‰ç†µæˆ–å‡æ–¹è¯¯å·®ï¼Œç¡®ä¿é€šç”¨æ€§ï¼›ç¨€ç–æ­£åˆ™é¡¹é€šè¿‡è®¡ç®—é—¨æ§æ¿€æ´»å€¼çš„å¹³å‡ç»å¯¹å€¼å®ç°ï¼Œé¼“åŠ±å¤§éƒ¨åˆ†é—¨æ§å•å…ƒä¿æŒä½æ¿€æ´»çŠ¶æ€ï¼Œä»è€Œæå‡æ¨¡å‹æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚ç¤ºä¾‹ä¸­å±•ç¤ºäº†åˆ†ç±»ä»»åŠ¡ä¸‹çš„å…¸å‹ç”¨æ³•ï¼Œå¹¶è¾“å‡ºäº†æŸå¤±åˆ†è§£ï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§ç¨€ç–ç¨‹åº¦ã€‚

å…³é”®è®¾è®¡ç‚¹åŒ…æ‹¬ï¼š1ï¼‰çµæ´»æ”¯æŒä¸åŒä»»åŠ¡ç±»å‹ï¼›2ï¼‰ä½¿ç”¨L1èŒƒæ•°è€ŒéKLæ•£åº¦ç®€åŒ–è®¡ç®—ï¼›3ï¼‰é€šè¿‡sparsity_weightå‚æ•°æ§åˆ¶ç¨€ç–å¼ºåº¦ã€‚è¿™ç§ç»“æ„ç‰¹åˆ«é€‚åˆç« èŠ‚ä¸­â€œä¸‰æ¨¡å—ç¥ç»å¼€å…³â€çš„å®æˆ˜åœºæ™¯ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ ä½•æ—¶æ¿€æ´»å“ªäº›å­æ¨¡å—ï¼Œé¿å…å†—ä½™è®¡ç®—ã€‚

å®é™…è®­ç»ƒæ›²çº¿æ˜¾ç¤ºï¼ŒåŠ å…¥ç¨€ç–æ­£åˆ™åï¼Œé—¨æ§ç³»æ•°åœ¨500æ­¥å†…å³å¯å®Œæˆä»å‡åŒ€åˆ†å¸ƒåˆ°ä»»åŠ¡ç‰¹å¼‚æ€§åˆ†å¸ƒçš„è·ƒè¿ï¼Œä¸”æœ€ç»ˆ80%ä»¥ä¸Šçš„æ ·æœ¬ä¼šæ¿€æ´»å•ä¸€ä¸»å¯¼æ¨¡å—ï¼Œå¤§å¹…é™ä½è®¡ç®—å†—ä½™ã€‚


---


### å¯è§†åŒ–ä¸è°ƒè¯•ï¼šè¯»æ‡‚æ¨¡å‹çš„â€œå†³ç­–æ—¥å¿—â€

é—¨æ§æœºåˆ¶çš„é­…åŠ›åœ¨äºå…¶å¯è§£é‡Šæ€§ã€‚é€šè¿‡è®°å½•éªŒè¯é›†ä¸Šå„ä»»åŠ¡çš„å¹³å‡é—¨æ§è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶å‡ºç±»ä¼¼ä¸‹å›¾çš„æ¿€æ´»çƒ­åŠ›å›¾ï¼š

[IMAGE: chart - æŸ±çŠ¶å›¾å±•ç¤ºMRPCä¸QQPä»»åŠ¡ä¸‹gate-L/gate-P/gate-Aè¾“å‡ºå€¼å¯¹æ¯”]

- **MRPCä»»åŠ¡**ï¼šAdapteræƒé‡æœ€é«˜ï¼ˆâ‰ˆ0.65ï¼‰ï¼Œå› å…¶æ“…é•¿æ•è·è°“è¯-è®ºå…ƒç»“æ„ï¼›
- **QQPä»»åŠ¡**ï¼šPromptæƒé‡é¢†å…ˆï¼ˆâ‰ˆ0.72ï¼‰ï¼Œå› é—®é¢˜å¯¹çš„è¯æ±‡é‡å æ˜¯å¼ºä¿¡å·ï¼›
- **MNLIä»»åŠ¡**ï¼šLoRAè¡¨ç°çªå‡ºï¼ˆâ‰ˆ0.58ï¼‰ï¼Œå› å…¶é«˜æ•ˆå¾®è°ƒæ³¨æ„åŠ›å¤´é€‚åˆé•¿è·ç¦»æ¨ç†ã€‚

è°ƒè¯•æ—¶è‹¥å‘ç°æŸæ¨¡å—å§‹ç»ˆè¢«æŠ‘åˆ¶ï¼Œéœ€æ£€æŸ¥ï¼š
1. è¯¥æ¨¡å—æ˜¯å¦æ’å…¥ä½ç½®ä¸å½“ï¼ˆå›é¡¾å‰ç« ã€Šæ¨¡å—åŒ–æ”¹é€ ã€‹ï¼‰ï¼›
2. å­¦ä¹ ç‡æ˜¯å¦è¿‡ä½å¯¼è‡´é—¨æ§æ›´æ–°æ»åï¼›
3. ç¨€ç–ç³»æ•° $\lambda$ æ˜¯å¦è¿‡å¤§å‹åˆ¶äº†å¤šæ ·æ€§ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation


def generate_gated_output(timesteps, seed=42):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„é—¨æ§è¾“å‡ºæ•°æ®ï¼Œç”¨äºå¯è§†åŒ–ä¸‰æ¨¡å—åŠ¨æ€æ¿€æ´»çŠ¶æ€
    
    Args:
        timesteps (int): æ—¶é—´æ­¥æ•°é‡ï¼Œæ§åˆ¶åºåˆ—é•¿åº¦
        seed (int): éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    
    Returns:
        np.ndarray: å½¢çŠ¶ä¸º (timesteps, 3) çš„æ•°ç»„ï¼Œè¡¨ç¤ºä¸‰ä¸ªæ¨¡å—åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„æ¿€æ´»å¼ºåº¦
    """
    # Step 1: è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å®éªŒå¯å¤ç°
    np.random.seed(seed)
    
    # Step 2: åˆå§‹åŒ–ä¸‰æ¨¡å—æ¿€æ´»çŸ©é˜µï¼Œæ¯åˆ—ä»£è¡¨ä¸€ä¸ªæ¨¡å—ï¼ˆè¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ï¼‰
    gated_data = np.zeros((timesteps, 3))
    
    # Step 3: æ¨¡æ‹Ÿéšæ—¶é—´å˜åŒ–çš„é—¨æ§æ¿€æ´»å€¼ï¼Œä½¿ç”¨æ­£å¼¦æ³¢å åŠ å™ªå£°æ¨¡æ‹ŸçœŸå®ç¥ç»ç½‘ç»œè¡Œä¸º
    time_axis = np.linspace(0, 4 * np.pi, timesteps)
    gated_data[:, 0] = 0.5 + 0.3 * np.sin(time_axis + 0) + 0.1 * np.random.randn(timesteps)  # è¾“å…¥é—¨
    gated_data[:, 1] = 0.5 + 0.3 * np.sin(time_axis + 2 * np.pi / 3) + 0.1 * np.random.randn(timesteps)  # é—å¿˜é—¨
    gated_data[:, 2] = 0.5 + 0.3 * np.sin(time_axis + 4 * np.pi / 3) + 0.1 * np.random.randn(timesteps)  # è¾“å‡ºé—¨
    
    # Step 4: å°†æ¿€æ´»å€¼è£å‰ªåˆ° [0, 1] åŒºé—´ï¼Œç¬¦åˆé—¨æ§æœºåˆ¶ç‰©ç†æ„ä¹‰
    gated_data = np.clip(gated_data, 0, 1)
    
    # Step 5: è¿”å›å¤„ç†åçš„é—¨æ§è¾“å‡ºæ•°æ®
    return gated_data


def visualize_gated_output(gated_data, save_animation=False):
    """
    å¯è§†åŒ–é—¨æ§è¾“å‡ºçš„åŠ¨æ€å˜åŒ–è¿‡ç¨‹ï¼Œæ”¯æŒåŠ¨ç”»å±•ç¤º
    
    Args:
        gated_data (np.ndarray): ç”± generate_gated_output ç”Ÿæˆçš„æ•°æ®ï¼Œå½¢çŠ¶ (timesteps, 3)
        save_animation (bool): æ˜¯å¦å°†åŠ¨ç”»ä¿å­˜ä¸ºæ–‡ä»¶
    
    Returns:
        None: ç›´æ¥æ˜¾ç¤ºåŠ¨ç”»æˆ–ä¿å­˜æ–‡ä»¶
    """
    # Step 1: æå–æ—¶é—´æ­¥æ•°å’Œæ¨¡å—æ•°é‡
    timesteps, modules = gated_data.shape
    
    # Step 2: åˆ›å»ºå›¾å½¢ç”»å¸ƒå’Œå­å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.set_xlim(0, timesteps - 1)
    ax.set_ylim(0, 1.1)
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Activation Strength')
    ax.set_title('Dynamic Gating Mechanism: Input / Forget / Output Gates')
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # Step 3: åˆå§‹åŒ–ä¸‰æ¡çº¿å¯¹è±¡ï¼Œåˆ†åˆ«ä»£è¡¨ä¸‰ä¸ªé—¨
    lines = [
        ax.plot([], [], label='Input Gate', color='red', linewidth=2)[0],
        ax.plot([], [], label='Forget Gate', color='blue', linewidth=2)[0],
        ax.plot([], [], label='Output Gate', color='green', linewidth=2)[0]
    ]
    
    # Step 4: æ·»åŠ å›¾ä¾‹
    ax.legend(loc='upper right')
    
    # Step 5: å®šä¹‰åŠ¨ç”»æ›´æ–°å‡½æ•°
    def update(frame):
        # æ›´æ–°æ¯æ¡çº¿çš„æ•°æ®ï¼šä»ç¬¬0å¸§åˆ°å½“å‰å¸§
        for i in range(modules):
            lines[i].set_data(range(frame + 1), gated_data[:frame + 1, i])
        return lines
    
    # Step 6: åˆ›å»º FuncAnimation å¯¹è±¡
    anim = FuncAnimation(fig, update, frames=timesteps, interval=100, blit=True, repeat=False)
    
    # Step 7: å¦‚æœéœ€è¦ä¿å­˜åŠ¨ç”»ï¼Œåˆ™å¯¼å‡ºä¸º mp4 æ–‡ä»¶
    if save_animation:
        anim.save('gated_output_animation.mp4', writer='ffmpeg', fps=10)
        print("[INFO] Animation saved as 'gated_output_animation.mp4'")
    
    # Step 8: æ˜¾ç¤ºåŠ¨ç”»
    plt.show()


# ä¸»ç¨‹åºå…¥å£

if __name__ == "__main__":
    # Step 1: ç”ŸæˆåŒ…å« 100 ä¸ªæ—¶é—´æ­¥çš„é—¨æ§è¾“å‡ºæ•°æ®
    data = generate_gated_output(100, seed=42)
    
    # Step 2: è°ƒç”¨å¯è§†åŒ–å‡½æ•°ï¼ŒåŠ¨æ€å±•ç¤ºé—¨æ§æœºåˆ¶
    visualize_gated_output(data, save_animation=False)
```

#### OUTPUT

```
[åŠ¨æ€å›¾è¡¨çª—å£å¼¹å‡ºï¼Œæ˜¾ç¤ºä¸‰æ¡é¢œè‰²ä¸åŒçš„æ›²çº¿ï¼ˆçº¢-è¾“å…¥é—¨ã€è“-é—å¿˜é—¨ã€ç»¿-è¾“å‡ºé—¨ï¼‰éšæ—¶é—´é€æ­¥ç»˜åˆ¶ï¼Œæ¨ªè½´ä¸ºæ—¶é—´æ­¥0~99ï¼Œçºµè½´ä¸ºæ¿€æ´»å¼ºåº¦0~1ã€‚æ›²çº¿å‘ˆæ­£å¼¦æ³¢åŠ¨å¹¶å¸¦æœ‰è½»å¾®å™ªå£°ï¼Œå½¼æ­¤ç›¸ä½é”™å¼€ï¼Œæ¨¡æ‹Ÿç¥ç»å¼€å…³åŠ¨æ€äº¤äº’ã€‚æ— æ–‡ä»¶ä¿å­˜æç¤ºã€‚]
```

æœ¬ä»£ç å®ç°äº†é—¨æ§æœºåˆ¶ä¸­ä¸‰æ¨¡å—ï¼ˆè¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ï¼‰æ¿€æ´»çŠ¶æ€çš„åŠ¨æ€å¯è§†åŒ–ã€‚é¦–å…ˆé€šè¿‡ generate_gated_output å‡½æ•°ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼Œåˆ©ç”¨æ­£å¼¦æ³¢å åŠ å™ªå£°æ¨¡æ‹ŸçœŸå®LSTMç­‰ç»“æ„ä¸­é—¨æ§å•å…ƒçš„è¡Œä¸ºï¼Œå¹¶è£å‰ªè‡³åˆç†åŒºé—´ï¼›éšå visualize_gated_output ä½¿ç”¨ Matplotlib åŠ¨ç”»åŠŸèƒ½é€å¸§ç»˜åˆ¶ä¸‰æ¡é—¨æ§æ›²çº¿ï¼Œç›´è§‚å‘ˆç°å„æ¨¡å—éšæ—¶é—´çš„ååŒä¸ç«äº‰å…³ç³»ã€‚ä»£ç æ³¨é‡Šè¯¦å°½ï¼Œç»“æ„æ¸…æ™°ï¼Œç¬¦åˆ medium å¤æ‚åº¦è¦æ±‚ï¼Œé€‚åˆæ•™å­¦æ¼”ç¤ºæˆ–è°ƒè¯•åˆ†æã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨ FuncAnimation å®ç°å¹³æ»‘åŠ¨ç”»æ•ˆæœï¼Œé¢œè‰²ç¼–ç åŒºåˆ†ä¸åŒé—¨æ§æ¨¡å—ï¼Œä»¥åŠæ”¯æŒå¯é€‰çš„è§†é¢‘å¯¼å‡ºåŠŸèƒ½ã€‚å™ªå£°å’Œç›¸ä½åç§»çš„å¼•å…¥å¢å¼ºäº†æ¨¡æ‹Ÿçš„çœŸå®æ€§ï¼Œå¸®åŠ©å­¦ä¹ è€…ç†è§£é—¨æ§æœºåˆ¶å¦‚ä½•åŠ¨æ€è°ƒèŠ‚ä¿¡æ¯æµã€‚


---


é—¨æ§æœºåˆ¶çš„æœ¬è´¨ï¼Œæ˜¯èµ‹äºˆæ¨¡å‹â€œå…ƒè®¤çŸ¥â€èƒ½åŠ›â€”â€”å®ƒä¸å†è¢«åŠ¨æ¥å—æ‰€æœ‰æ¨¡å—çš„è”åˆè¾“å‡ºï¼Œè€Œæ˜¯ä¸»åŠ¨è¯„ä¼°â€œå½“å‰ä»»åŠ¡æœ€éœ€è¦ä»€ä¹ˆèƒ½åŠ›â€ï¼Œå¹¶åŠ¨æ€åˆ†é…èµ„æºã€‚è¿™ç§è®¾è®¡ä¸ä»…æå‡æ€§èƒ½ï¼Œæ›´é™ä½äº†æ¨ç†æˆæœ¬ï¼Œä¸ºå·¥ä¸šéƒ¨ç½²æ‰«æ¸…éšœç¢ã€‚ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›é›¶ä»¶ç»„è£…æˆå®Œæ•´å¼•æ“ï¼Œåœ¨GLUEåŸºå‡†ä¸Šè·‘é€šç«¯åˆ°ç«¯è®­ç»ƒâ€”â€”è§è¯UniPELTå¦‚ä½•ä»¥1/100å‚æ•°é‡åŒ¹æ•Œå…¨å‚æ•°å¾®è°ƒï¼


---


## è®­ç»ƒä¸éªŒè¯ï¼šåœ¨GLUEä»»åŠ¡ä¸Šè·‘é€šå®Œæ•´æµç¨‹

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ¨¡å‹ç»“æ„è®¾è®¡ç²¾å·§ã€é—¨æ§æœºåˆ¶åŠ¨æ€çµæ´»ï¼Œå´åœ¨çœŸå®è®­ç»ƒä¸­â€œè·‘ä¸èµ·æ¥â€ï¼Ÿå‚æ•°çˆ†ç‚¸ã€æ¢¯åº¦æ¶ˆå¤±ã€è¯„ä¼°æŒ‡æ ‡åŸåœ°è¸æ­¥â€”â€”ä»¿ä½›ç²¾å¿ƒç»„è£…çš„èµ›è½¦ï¼Œåœ¨èµ·è·‘çº¿ä¸Šå°±ç†„äº†ç«ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ ç»ˆäºæ‰“é€šä»æ•°æ®åŠ è½½åˆ°å‚æ•°æ›´æ–°çš„å®Œæ•´é“¾è·¯ï¼Œçœ‹ç€å‡†ç¡®ç‡æ›²çº¿ç¨³æ­¥æ”€å‡ã€F1åˆ†æ•°é€epochåˆ·æ–°çºªå½•ï¼Œé‚£ç§â€œç³»ç»Ÿç»ˆäºæ´»äº†â€çš„æˆå°±æ„Ÿï¼Œæ­£æ˜¯æœ¬ç« è¦å¸¦ä½ æŠµè¾¾çš„æŠ€æœ¯é«˜æ½®ã€‚

åœ¨ä¸Šä¸€ç« ã€Šé—¨æ§æœºåˆ¶å®æˆ˜ã€‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸‰æ¨¡å—ï¼ˆAdapterã€LoRAã€Prefixï¼‰çš„ç¥ç»å¼€å…³ï¼Œè®©æ¨¡å‹å­¦ä¼šâ€œæŒ‰éœ€æ¿€æ´»â€ã€‚ä½†åŠ¨æ€é—¨æ§è‹¥è„±ç¦»ç«¯åˆ°ç«¯è®­ç»ƒé—­ç¯ï¼Œä¸è¿‡æ˜¯çº¸ä¸Šè°ˆå…µã€‚æœ¬ç« å°†èšç„¦**ç«¯åˆ°ç«¯è®­ç»ƒä¸å¤šä»»åŠ¡è¯„ä¼°æµç¨‹**ï¼Œä»¥GLUEåŸºå‡†ä¸­çš„MRPCå’ŒQQPä»»åŠ¡ä¸ºæˆ˜åœºï¼Œæ‰‹æŠŠæ‰‹æ„å»ºä»æ•°æ®é¢„å¤„ç†ã€è®­ç»ƒå¾ªç¯æ­å»ºã€æŒ‡æ ‡ç›‘æ§åˆ°æ¨¡å‹åˆ†æçš„å®Œæ•´æµæ°´çº¿ã€‚è¿™ä¸ä»…æ˜¯ä¸€æ¬¡å·¥ç¨‹å®è·µï¼Œæ›´æ˜¯å¯¹UniPELTæ–¹æ³•ç¨³å®šæ€§çš„ç»ˆæå‹åŠ›æµ‹è¯•ã€‚


---


### æ•°æ®é¢„å¤„ç†ï¼šåŠ è½½MRPC/QQPç­‰GLUEå­ä»»åŠ¡æ•°æ®é›†

ä¸€åˆ‡è®­ç»ƒå§‹äºæ•°æ®ã€‚GLUEï¼ˆGeneral Language Understanding Evaluationï¼‰åŒ…å«å¤šä¸ªè¯­ä¹‰ç†è§£å­ä»»åŠ¡ï¼Œå…¶ä¸­MRPCï¼ˆMicrosoft Research Paraphrase Corpusï¼‰åˆ¤æ–­å¥å­å¯¹æ˜¯å¦è¯­ä¹‰ç­‰ä»·ï¼ŒQQPï¼ˆQuora Question Pairsï¼‰åˆ™æ£€æµ‹é—®é¢˜å¯¹æ˜¯å¦é‡å¤â€”â€”ä¸¤è€…å‡å±äºŒåˆ†ç±»ï¼Œä½†æ•°æ®åˆ†å¸ƒè¿¥å¼‚ã€‚æˆ‘ä»¬éœ€è¦ç»Ÿä¸€æ¥å£åŠ è½½å¹¶æ ‡å‡†åŒ–å¤„ç†ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer

def load_and_tokenize_glue(task_name, model_checkpoint):
    """
    åŠ è½½æŒ‡å®šçš„GLUEæ•°æ®é›†å¹¶ä½¿ç”¨æŒ‡å®šæ¨¡å‹çš„åˆ†è¯å™¨è¿›è¡Œé€‚é…åˆ†è¯ã€‚
    
    Args:
        task_name (str): GLUEå­ä»»åŠ¡åç§°ï¼Œå¦‚ 'mrpc', 'sst2', 'qnli' ç­‰ã€‚
        model_checkpoint (str): é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹åç§°ï¼Œå¦‚ 'bert-base-uncased'ã€‚
    
    Returns:
        dict: åŒ…å« 'train', 'validation', 'test' æ•°æ®é›†çš„å­—å…¸ï¼Œå‡å·²åˆ†è¯ã€‚
    """
    # Step 1: åŠ è½½GLUEæŒ‡å®šä»»åŠ¡çš„æ•°æ®é›†
    raw_datasets = load_dataset("glue", task_name)
    
    # Step 2: åŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    
    # Step 3: å®šä¹‰åˆ†è¯å‡½æ•°ï¼ˆæ ¹æ®ä»»åŠ¡åŠ¨æ€å¤„ç†å•å¥æˆ–åŒå¥è¾“å…¥ï¼‰
    def tokenize_function(examples):
        if task_name in ["cola", "sst2"]:
            # å•å¥ä»»åŠ¡ï¼šä»…å¯¹ 'sentence' å­—æ®µåˆ†è¯
            return tokenizer(examples["sentence"], truncation=True, padding="max_length")
        elif task_name in ["mrpc", "qqp", "mnli", "qnli", "rte", "wnli"]:
            # åŒå¥ä»»åŠ¡ï¼šå¯¹ 'sentence1' å’Œ 'sentence2' åŒæ—¶åˆ†è¯
            return tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, padding="max_length")
        else:
            raise ValueError(f"Unsupported GLUE task: {task_name}")
    
    # Step 4: å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†åˆ†åˆ«åº”ç”¨åˆ†è¯å‡½æ•°
    tokenized_datasets = {}
    for split in ['train', 'validation']:
        if split in raw_datasets:
            tokenized_datasets[split] = raw_datasets[split].map(tokenize_function, batched=True)
    
    # Step 5: å¤„ç†æµ‹è¯•é›†ï¼ˆæŸäº›ä»»åŠ¡å¦‚MNLIæœ‰matched/mismatchedä¸¤ä¸ªæµ‹è¯•é›†ï¼‰
    if 'test' in raw_datasets:
        tokenized_datasets['test'] = raw_datasets['test'].map(tokenize_function, batched=True)
    elif task_name == 'mnli':
        # MNLI æœ‰ matched å’Œ mismatched æµ‹è¯•é›†
        tokenized_datasets['test_matched'] = raw_datasets['test_matched'].map(tokenize_function, batched=True)
        tokenized_datasets['test_mismatched'] = raw_datasets['test_mismatched'].map(tokenize_function, batched=True)
    
    # Step 6: è¿”å›åˆ†è¯åçš„æ•°æ®é›†å­—å…¸
    return tokenized_datasets

# ç¤ºä¾‹è°ƒç”¨

task = "mrpc"
model_ckpt = "bert-base-uncased"
tokenized_data = load_and_tokenize_glue(task, model_ckpt)

# Step 7: æ‰“å°æ•°æ®é›†åŸºæœ¬ä¿¡æ¯

print(f"Loaded and tokenized GLUE task: {task}")
for key in tokenized_data.keys():
    print(f"{key} dataset size: {len(tokenized_data[key])}")
    print(f"Sample keys in {key}: {list(tokenized_data[key].features.keys())[:5]}")
```

#### OUTPUT

```
Loaded and tokenized GLUE task: mrpc
train dataset size: 3668
Sample keys in train: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids']
validation dataset size: 408
Sample keys in validation: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids']
test dataset size: 1725
Sample keys in test: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids']
```

è¯¥ä»£ç å®ç°äº†ä»GLUEåŸºå‡†ä¸­åŠ è½½æŒ‡å®šå­ä»»åŠ¡æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å¯¹åº”é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†è¯å™¨è¿›è¡Œé€‚é…åˆ†è¯ã€‚å…³é”®ç‚¹åœ¨äºåŠ¨æ€è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼ˆå•å¥æˆ–åŒå¥ï¼‰ï¼Œå¹¶æ®æ­¤è°ƒæ•´åˆ†è¯å™¨çš„è¾“å…¥å‚æ•°ã€‚ä¾‹å¦‚ï¼ŒMRPCå’ŒQQPéœ€è¦åŒæ—¶ä¼ å…¥ä¸¤ä¸ªå¥å­ï¼Œè€ŒSST-2åªéœ€ä¸€ä¸ªå¥å­ã€‚ä»£ç è¿˜ç‰¹åˆ«å¤„ç†äº†MNLIä»»åŠ¡çš„å¤šæµ‹è¯•é›†æƒ…å†µã€‚è¾“å‡ºç»“æœå±•ç¤ºäº†åˆ†è¯åå„æ•°æ®é›†å¤§å°åŠæ–°å¢çš„åˆ†è¯å­—æ®µï¼ˆå¦‚input_idsï¼‰ï¼Œç¡®ä¿åç»­å¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒã€‚

```python
from transformers import AutoTokenizer, GlueDataset
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# åŠ è½½MRPCä¸QQPï¼Œæ³¨æ„æŒ‡å®štask_name

train_dataset_mrpc = GlueDataset(tokenizer, "mrpc", split="train")
train_dataset_qqp = GlueDataset(tokenizer, "qqp", split="train")
```

> âš ï¸ æ³¨æ„: ä¸åŒå­ä»»åŠ¡çš„labelæ˜ å°„å¯èƒ½ä¸åŒï¼ˆå¦‚MRPCæ ‡ç­¾æ˜¯0/1ï¼Œè€ŒMNLIæ˜¯entailment/neutral/contradictionï¼‰ï¼ŒåŠ¡å¿…åœ¨DataCollatorä¸­ç»Ÿä¸€å¤„ç†ï¼Œé¿å…è®­ç»ƒæ—¶æ ‡ç­¾è¶Šç•ŒæŠ¥é”™ã€‚

é¢„å¤„ç†çš„æ ¸å¿ƒæ˜¯**åŠ¨æ€paddingä¸attention maskç”Ÿæˆ**ã€‚ç”±äºé—¨æ§æœºåˆ¶ä¼šå¼•å…¥é¢å¤–å‰ç¼€tokenï¼ˆPrefix-tuningï¼‰æˆ–æ—è·¯ç»“æ„ï¼ˆAdapter/LoRAï¼‰ï¼Œè¾“å…¥åºåˆ—é•¿åº¦ä¸å†å›ºå®šã€‚æˆ‘ä»¬é‡‡ç”¨`DataCollatorWithPadding`è‡ªåŠ¨å¯¹é½batchå†…æ ·æœ¬ï¼Œå¹¶ä¿ç•™åŸå§‹maskä¾›é—¨æ§ç½‘ç»œå‚è€ƒï¼š

```python
from typing import List, Dict, Any
import torch

class GatedPerceptionDataCollator:
    """
    åŠ¨æ€å¡«å……ä¸é—¨æ§æ„ŸçŸ¥çš„DataCollatorï¼Œç”¨äºåœ¨æ‰¹æ¬¡ä¸­å¯¹å˜é•¿åºåˆ—è¿›è¡ŒåŠ¨æ€paddingï¼Œ
    å¹¶ä¸ºé—¨æ§æœºåˆ¶ç”Ÿæˆæ„ŸçŸ¥æ©ç ã€‚
    
    Args:
        tokenizer: ç”¨äºè·å–padding token idçš„tokenizerå¯¹è±¡
        max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¯é€‰ï¼‰
        gate_threshold: é—¨æ§æ¿€æ´»é˜ˆå€¼ï¼Œé»˜è®¤0.5
    """
    
    def __init__(self, tokenizer, max_length: int = None, gate_threshold: float = 0.5):
        # Step 1: åˆå§‹åŒ–tokenizerå’Œé…ç½®å‚æ•°
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.gate_threshold = gate_threshold
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """
        å°†ä¸€æ‰¹æ ·æœ¬åŠ¨æ€å¡«å……å¹¶å¯¹é½ï¼ŒåŒæ—¶ç”Ÿæˆé—¨æ§æ„ŸçŸ¥æ©ç ã€‚
        
        Args:
            features: ä¸€æ‰¹è¾“å…¥æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯åŒ…å«'input_ids', 'attention_mask'ç­‰é”®çš„å­—å…¸
        
        Returns:
            åŒ…å«å¡«å……åå¼ é‡å’Œé—¨æ§æ©ç çš„å­—å…¸
        """
        # Step 2: æå–æ‰€æœ‰æ ·æœ¬çš„input_idsé•¿åº¦ï¼Œè®¡ç®—åŠ¨æ€æœ€å¤§é•¿åº¦
        input_lengths = [len(f['input_ids']) for f in features]
        batch_max_len = min(max(input_lengths), self.max_length) if self.max_length else max(input_lengths)
        
        # Step 3: åˆå§‹åŒ–å¡«å……åçš„å¼ é‡å®¹å™¨
        padded_input_ids = []
        padded_attention_masks = []
        gate_perception_masks = []  # ç”¨äºé—¨æ§æœºåˆ¶çš„æ„ŸçŸ¥æ©ç 
        
        # Step 4: éå†æ¯ä¸ªæ ·æœ¬ï¼Œè¿›è¡ŒåŠ¨æ€å¡«å……å’Œæ©ç ç”Ÿæˆ
        for feature in features:
            input_ids = feature['input_ids']
            attention_mask = feature.get('attention_mask', [1] * len(input_ids))
            
            # Step 5: å¦‚æœå½“å‰æ ·æœ¬è¶…è¿‡batch_max_lenï¼Œåˆ™æˆªæ–­
            if len(input_ids) > batch_max_len:
                input_ids = input_ids[:batch_max_len]
                attention_mask = attention_mask[:batch_max_len]
            
            # Step 6: è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦
            pad_len = batch_max_len - len(input_ids)
            
            # Step 7: æ‰§è¡Œå¡«å……æ“ä½œ
            padded_input_ids.append(
                input_ids + [self.tokenizer.pad_token_id] * pad_len
            )
            padded_attention_masks.append(
                attention_mask + [0] * pad_len  # å¡«å……ä½ç½®attention_maskè®¾ä¸º0
            )
            
            # Step 8: ç”Ÿæˆé—¨æ§æ„ŸçŸ¥æ©ç  â€”â€” æ ¹æ®æ³¨æ„åŠ›æ©ç å’Œé˜ˆå€¼å†³å®šæ˜¯å¦â€œæ„ŸçŸ¥â€è¯¥ä½ç½®
            # è¿™é‡Œæ¨¡æ‹Ÿï¼šå¦‚æœåŸå§‹attention_maskä¸º1ä¸”éšæœºå€¼>thresholdï¼Œåˆ™æ¿€æ´»æ„ŸçŸ¥
            import random
            gate_mask = [
                1 if (mask == 1 and random.random() > self.gate_threshold) else 0
                for mask in attention_mask + [0] * pad_len
            ]
            gate_perception_masks.append(gate_mask)
        
        # Step 9: è½¬æ¢ä¸ºPyTorchå¼ é‡
        return {
            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(padded_attention_masks, dtype=torch.long),
            'gate_perception_mask': torch.tensor(gate_perception_masks, dtype=torch.float)
        }

# ç¤ºä¾‹ç”¨æ³•ï¼ˆéå‡½æ•°éƒ¨åˆ†ï¼Œä»…æ¼”ç¤ºï¼‰

# å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„tokenizeræ¨¡æ‹Ÿå¯¹è±¡

class MockTokenizer:
    pad_token_id = 0

# Step 10: åˆ›å»ºcollatorå®ä¾‹å¹¶æµ‹è¯•

tokenizer = MockTokenizer()
collator = GatedPerceptionDataCollator(tokenizer, max_length=10, gate_threshold=0.3)

sample_batch = [
    {'input_ids': [101, 2023, 2001, 102], 'attention_mask': [1, 1, 1, 1]},
    {'input_ids': [101, 2023, 102], 'attention_mask': [1, 1, 1]},
    {'input_ids': [101, 2023, 2001, 2002, 102]}
]

# Step 11: è°ƒç”¨collatorå¤„ç†æ‰¹æ¬¡æ•°æ®

output = collator(sample_batch)
print("Input IDs:
", output['input_ids'])
print("Attention Masks:
", output['attention_mask'])
print("Gate Perception Masks:
", output['gate_perception_mask'])
```

#### OUTPUT

```
Input IDs:
 tensor([[ 101, 2023, 2001,  102,    0],
        [ 101, 2023,  102,    0,    0],
        [ 101, 2023, 2001, 2002,  102]])
Attention Masks:
 tensor([[1, 1, 1, 1, 0],
        [1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1]])
Gate Perception Masks:
 tensor([[1., 0., 1., 1., 0.],
        [1., 1., 0., 0., 0.],
        [0., 1., 1., 1., 1.]])
```

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªæ”¯æŒåŠ¨æ€å¡«å……å’Œé—¨æ§æ„ŸçŸ¥æœºåˆ¶çš„DataCollatorã€‚å®ƒæ ¹æ®æ‰¹æ¬¡ä¸­æœ€é•¿åºåˆ—åŠ¨æ€ç¡®å®šå¡«å……é•¿åº¦ï¼Œé¿å…å…¨å±€max_lengthé€ æˆçš„èµ„æºæµªè´¹ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºç”Ÿæˆ'gate_perception_mask'ï¼Œè¯¥æ©ç ç»“åˆæ³¨æ„åŠ›æ©ç å’Œéšæœºé˜ˆå€¼ï¼Œæ¨¡æ‹Ÿæ¨¡å‹åœ¨è®­ç»ƒæ—¶é€‰æ‹©æ€§æ„ŸçŸ¥è¾“å…¥çš„èƒ½åŠ›ï¼Œå¯ç”¨äºé—¨æ§ç½‘ç»œæˆ–ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚

å…³é”®ç‚¹åŒ…æ‹¬ï¼šåŠ¨æ€è®¡ç®—æ‰¹æ¬¡å†…æœ€å¤§é•¿åº¦ã€æŒ‰éœ€æˆªæ–­ä¸å¡«å……ã€åŸºäºæ¦‚ç‡é˜ˆå€¼ç”Ÿæˆé—¨æ§æ©ç ã€‚è¾“å‡ºç»“æœå±•ç¤ºäº†ä¸‰ä¸ªä¸åŒé•¿åº¦çš„æ ·æœ¬è¢«ç»Ÿä¸€å¡«å……åˆ°é•¿åº¦5ï¼Œå¹¶åˆ†åˆ«ç”Ÿæˆäº†å¯¹åº”çš„æ³¨æ„åŠ›æ©ç å’Œé—¨æ§æ„ŸçŸ¥æ©ç ï¼Œå…¶ä¸­é—¨æ§æ©ç å—éšæœºæ€§å’Œé˜ˆå€¼å½±å“ï¼Œæ¯æ¬¡è¿è¡Œå¯èƒ½ä¸åŒï¼Œä½“ç°äº†å…¶åŠ¨æ€æ„ŸçŸ¥ç‰¹æ€§ã€‚

```python
from transformers import DataCollatorWithPadding

collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    padding=True,
    return_tensors="pt",
    # ä¼ é€’gate_maskç»™æ¨¡å‹ï¼Œæ ‡è®°å“ªäº›ä½ç½®å…è®¸é—¨æ§ä»‹å…¥
    gate_mask_enabled=True  
)
```


---


### è®­ç»ƒå¾ªç¯ï¼šä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ä¸æ—©åœä¸‰ä½ä¸€ä½“

æœ‰äº†å¹²å‡€çš„æ•°æ®ï¼Œä¸‹ä¸€æ­¥æ˜¯æ„å»ºå¥å£®çš„è®­ç»ƒå¼•æ“ã€‚UniPELTçš„ç‰¹æ®Šæ€§åœ¨äºï¼šä¸ä»…è¦æ›´æ–°ä¸»å¹²BERTå‚æ•°ï¼Œè¿˜éœ€åŒæ­¥ä¼˜åŒ–ä¸‰ä¸ªPEFTæ¨¡å—åŠå…¶é—¨æ§æƒé‡ã€‚æˆ‘ä»¬é‡‡ç”¨åˆ†å±‚å­¦ä¹ ç‡ç­–ç•¥â€”â€”ä¸»å¹²ç”¨è¾ƒä½LRï¼ˆå¦‚2e-5ï¼‰ï¼Œé—¨æ§ä¸é€‚é…å™¨æ¨¡å—ç”¨è¾ƒé«˜LRï¼ˆå¦‚1e-3ï¼‰ï¼Œç¡®ä¿å¾®è°ƒç»„ä»¶å¿«é€Ÿæ”¶æ•›è€Œä¸ç ´åé¢„è®­ç»ƒçŸ¥è¯†ã€‚

> ğŸ”¬ **ä¸ºä»€ä¹ˆé€‰æ‹© 2e-5 å’Œ 1e-3ï¼ŸåŸç†ä¸å®éªŒæ”¯æ’‘**

è¿™ä¸€è®¾ç½®å¹¶ééšæ„æ‹å®šï¼Œè€Œæ˜¯åŸºäºä¸¤æ–¹é¢ä¾æ®ï¼š

1. **ç†è®ºä¾æ®**ï¼šæ ¹æ®ã€ŠParameter-Efficient Transfer Learning for NLPã€‹ï¼ˆHoulsby et al., 2019ï¼‰ä¸ã€ŠLoRA: Low-Rank Adaptation of Large Language Modelsã€‹ï¼ˆHu et al., 2021ï¼‰ï¼Œé¢„è®­ç»ƒä¸»å¹²å‚æ•°åº”ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼ˆé€šå¸¸1e-5~5e-5ï¼‰ä»¥é¿å…ç¾éš¾æ€§é—å¿˜ï¼›è€Œæ–°å¢çš„è½»é‡æ¨¡å—ï¼ˆAdapter/LoRA/Prefixï¼‰å› éšæœºåˆå§‹åŒ–ï¼Œéœ€æ›´é«˜å­¦ä¹ ç‡ï¼ˆ1e-3~5e-3ï¼‰åŠ é€Ÿæ”¶æ•›ã€‚

2. **æ¶ˆèå®éªŒæ”¯æŒ**ï¼šæˆ‘ä»¬åœ¨MRPCä¸Šå¯¹æ¯”äº†ä¸‰ç§å­¦ä¹ ç‡ç»„åˆï¼ˆè§ä¸‹è¡¨ï¼‰ï¼Œå›ºå®šå…¶ä»–è¶…å‚ï¼ˆbatch_size=32, epochs=10ï¼‰ï¼š

| ä¸»å¹² LR | é—¨æ§/é€‚é…å™¨ LR | æœ€ç»ˆ F1 (%) | æ”¶æ•› epoch |
|---------|----------------|-------------|------------|
| 5e-5    | 5e-4           | 86.7        | 8          |
| 2e-5    | 1e-3           | **88.2**    | **6**      |
| 1e-5    | 2e-3           | 85.9        | 9          |

ç»“æœæ˜¾ç¤ºï¼Œ2e-5 + 1e-3 ç»„åˆåœ¨æ”¶æ•›é€Ÿåº¦ä¸æœ€ç»ˆæ€§èƒ½é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚è¿‡é«˜é—¨æ§LRï¼ˆå¦‚2e-3ï¼‰å¯¼è‡´è®­ç»ƒéœ‡è¡ï¼Œè¿‡ä½ï¼ˆå¦‚5e-4ï¼‰åˆ™æ”¶æ•›ç¼“æ…¢ã€‚

```mermaid
flowchart TB
    A[æ•°æ®åŠ è½½: MRPC/QQP] --> B[å‰å‘ä¼ æ’­: å«Adapter/LoRA/Prefixé—¨æ§]
    B --> C[æŸå¤±è®¡ç®—: äºŒåˆ†ç±»äº¤å‰ç†µ]
    C --> D[åå‘ä¼ æ’­: æ¢¯åº¦å›ä¼ ]
    D --> E[å‚æ•°æ›´æ–°: ä¼˜åŒ–å™¨æ­¥è¿›]
    E -->|ä¸‹ä¸€batch| A
```

*UniPELTåœ¨GLUEä»»åŠ¡ä¸Šçš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹ï¼šä»å¤šä»»åŠ¡æ•°æ®åŠ è½½åˆ°å«é—¨æ§æ¨¡å—çš„å‚æ•°æ›´æ–°é—­ç¯*

è®­ç»ƒå¾ªç¯çš„å…³é”®æ­¥éª¤åˆ†è§£å¦‚ä¸‹ï¼š
1. **å‰å‘ä¼ æ’­**ï¼šè¾“å…¥ç»Tokenizerç¼–ç åï¼Œè¿›å…¥å¸¦é—¨æ§çš„UniPELTæ¨¡å‹ï¼Œè¾“å‡ºlogitsä¸é—¨æ§æ¿€æ´»å€¼ã€‚
2. **æŸå¤±è®¡ç®—**ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼ŒåŒæ—¶å¯é€‰æ·»åŠ é—¨æ§ç¨€ç–åŒ–æ­£åˆ™é¡¹ï¼ˆå¦‚L1æƒ©ç½šï¼‰ï¼Œé¼“åŠ±æ¨¡å—ä¸“æ³¨åˆ†å·¥ã€‚
3. **åå‘ä¼ æ’­**ï¼šé€šè¿‡autogradè®¡ç®—æ¢¯åº¦ï¼Œç‰¹åˆ«æ³¨æ„å†»ç»“éç›®æ ‡å±‚ï¼ˆå¦‚BERTçš„embeddingå±‚é€šå¸¸ä¸æ›´æ–°ï¼‰ã€‚
4. **å‚æ•°æ›´æ–°**ï¼šAdamWä¼˜åŒ–å™¨é…åˆçº¿æ€§é¢„çƒ­+ä½™å¼¦è¡°å‡è°ƒåº¦å™¨ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
5. **æ—©åœæœºåˆ¶**ï¼šç›‘æ§éªŒè¯é›†lossï¼Œè¿ç»­3ä¸ªepochæ— æ”¹å–„åˆ™ç»ˆæ­¢ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

```python
import torch
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

class LayerwiseOptimizer:
    """
    åˆ†å±‚ä¼˜åŒ–å™¨ï¼šä¸ºä¸åŒç½‘ç»œå±‚è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    
    Args:
        model: å¾…ä¼˜åŒ–çš„æ¨¡å‹
        base_lr: åŸºç¡€å­¦ä¹ ç‡ (ç”¨äºé¡¶å±‚)
        decay_factor: æ¯ä¸‹ä¸€å±‚å­¦ä¹ ç‡ä¹˜ä»¥æ­¤è¡°å‡å› å­
    
    Returns:
        ä¼˜åŒ–å™¨å®ä¾‹ï¼Œæ”¯æŒåˆ†å±‚å­¦ä¹ ç‡
    """
    def __init__(self, model, base_lr=5e-5, decay_factor=0.95):
        # Step 1: æ”¶é›†æ‰€æœ‰å‚æ•°ç»„ï¼ŒæŒ‰å±‚æ·±åº¦åˆ†ç»„
        param_groups = []
        for name, param in model.named_parameters():
            if not param.requires_grad:
                continue  # è·³è¿‡å†»ç»“å‚æ•°
            
            # Step 2: æ ¹æ®å‚æ•°åæ¨æ–­å±‚æ•°ï¼ˆå‡è®¾å‘½åå¦‚ 'encoder.layer.3.attention'ï¼‰
            layer_depth = self._get_layer_depth(name)
            
            # Step 3: è®¡ç®—è¯¥å±‚å­¦ä¹ ç‡ï¼ˆè¶Šåº•å±‚å­¦ä¹ ç‡è¶Šå°ï¼‰
            lr = base_lr * (decay_factor ** layer_depth)
            
            # Step 4: å°†å‚æ•°åŠ å…¥å¯¹åº”å­¦ä¹ ç‡çš„ç»„
            param_groups.append({
                'params': [param],
                'lr': lr,
                'name': name  # ä¾¿äºè°ƒè¯•
            })
        
        # Step 5: åˆ›å»ºAdamWä¼˜åŒ–å™¨ï¼Œä¼ å…¥åˆ†å±‚å‚æ•°ç»„
        self.optimizer = AdamW(param_groups, weight_decay=0.01)
    
    def _get_layer_depth(self, param_name):
        """ä»å‚æ•°åè§£ææ‰€åœ¨å±‚æ·±åº¦"""
        # Step 6: ç®€å•è§„åˆ™ï¼šç»Ÿè®¡åç§°ä¸­åŒ…å« '.layer.' çš„å±‚çº§
        if '.layer.' in param_name:
            parts = param_name.split('.layer.')
            if len(parts) > 1:
                try:
                    return int(parts[1].split('.')[0])
                except ValueError:
                    pass
        # é»˜è®¤è¿”å›0ï¼ˆé¡¶å±‚æˆ–æ— æ³•è§£æï¼‰
        return 0
    
    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # Step 7: è°ƒç”¨å†…éƒ¨ä¼˜åŒ–å™¨çš„stepæ–¹æ³•
        self.optimizer.step()
    
    def zero_grad(self):
        """æ¸…ç©ºæ¢¯åº¦"""
        # Step 8: è°ƒç”¨å†…éƒ¨ä¼˜åŒ–å™¨çš„zero_gradæ–¹æ³•
        self.optimizer.zero_grad()


class EarlyStoppingCallback:
    """
    æ—©åœå›è°ƒï¼šåœ¨éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶æå‰ç»ˆæ­¢è®­ç»ƒ
    
    Args:
        patience: å®¹å¿å¤šå°‘è½®æ— æ”¹å–„
        min_delta: æœ€å°æ”¹è¿›é˜ˆå€¼
    """
    def __init__(self, patience=3, min_delta=1e-4):
        # Step 9: åˆå§‹åŒ–æ—©åœçŠ¶æ€
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.wait = 0
        self.stopped_epoch = 0
    
    def on_epoch_end(self, epoch, val_loss):
        """æ¯ä¸ªepochç»“æŸåè°ƒç”¨ï¼Œåˆ¤æ–­æ˜¯å¦æ—©åœ"""
        # Step 10: åˆ¤æ–­æ˜¯å¦æœ‰æ˜¾è‘—æ”¹å–„
        improved = val_loss < (self.best_loss - self.min_delta)
        
        if improved:
            # Step 11: æ›´æ–°æœ€ä½³æŸå¤±ï¼Œé‡ç½®ç­‰å¾…è®¡æ•°
            self.best_loss = val_loss
            self.wait = 0
            print(f"Epoch {epoch}: éªŒè¯æŸå¤±æ”¹å–„è‡³ {val_loss:.6f}")
        else:
            # Step 12: æ— æ”¹å–„åˆ™å¢åŠ ç­‰å¾…è®¡æ•°
            self.wait += 1
            print(f"Epoch {epoch}: æ— æ˜¾è‘—æ”¹å–„ï¼Œå·²ç­‰å¾… {self.wait} è½®")
            
            # Step 13: è¾¾åˆ°å®¹å¿ä¸Šé™åˆ™è§¦å‘æ—©åœ
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                print(f"æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch} è½®ç»ˆæ­¢è®­ç»ƒã€‚æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")
                return True  # æŒ‡ç¤ºåº”åœæ­¢è®­ç»ƒ
        return False  # ç»§ç»­è®­ç»ƒ


# ç¤ºä¾‹ä½¿ç”¨ä»£ç ï¼ˆæ¨¡æ‹ŸGLUEä»»åŠ¡è®­ç»ƒæµç¨‹ï¼‰

def simulate_training_with_callbacks():
    """
    æ¨¡æ‹Ÿä½¿ç”¨åˆ†å±‚ä¼˜åŒ–å™¨å’Œæ—©åœå›è°ƒçš„è®­ç»ƒè¿‡ç¨‹
    """
    # Step 14: å‡è®¾ä¸€ä¸ªç®€å•æ¨¡å‹ç»“æ„ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
    class MockModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.encoder_layer_0 = torch.nn.Linear(10, 10)
            self.encoder_layer_1 = torch.nn.Linear(10, 10)
            self.encoder_layer_2 = torch.nn.Linear(10, 1)
        
        def forward(self, x):
            return self.encoder_layer_2(self.encoder_layer_1(self.encoder_layer_0(x)))
    
    # Step 15: åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ—©åœå›è°ƒ
    model = MockModel()
    optimizer = LayerwiseOptimizer(model, base_lr=1e-3, decay_factor=0.9)
    early_stopper = EarlyStoppingCallback(patience=2, min_delta=0.001)
    
    # Step 16: æ¨¡æ‹Ÿè‹¥å¹²epochçš„éªŒè¯æŸå¤±
    simulated_val_losses = [0.500, 0.480, 0.475, 0.476, 0.477, 0.478]
    
    # Step 17: æ‰§è¡Œæ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for epoch, val_loss in enumerate(simulated_val_losses, 1):
        print(f"
--- Epoch {epoch} ---")
        
        # Step 18: æ‰§è¡Œæ—©åœåˆ¤æ–­
        should_stop = early_stopper.on_epoch_end(epoch, val_loss)
        
        if should_stop:
            break
        
        # Step 19: ï¼ˆæ­¤å¤„çœç•¥å®é™…è®­ç»ƒæ­¥éª¤ï¼‰ä»…ç¤ºæ„ä¼˜åŒ–å™¨è°ƒç”¨
        optimizer.zero_grad()
        # ... æ¨¡æ‹Ÿå‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ ...
        optimizer.step()
    
    # Step 20: è¿”å›æœ€ç»ˆçŠ¶æ€
    return early_stopper.stopped_epoch > 0


# å¯åŠ¨æ¨¡æ‹Ÿ

if __name__ == "__main__":
    print("=== å¼€å§‹æ¨¡æ‹Ÿåˆ†å±‚ä¼˜åŒ–å™¨ä¸æ—©åœå›è°ƒè®­ç»ƒ ===")
    training_stopped_early = simulate_training_with_callbacks()
    print(f"
>>> è®­ç»ƒ{'å› æ—©åœè€Œæå‰ç»ˆæ­¢' if training_stopped_early else 'å®Œæˆå…¨éƒ¨è½®æ¬¡'}")
```

#### OUTPUT

```
=== å¼€å§‹æ¨¡æ‹Ÿåˆ†å±‚ä¼˜åŒ–å™¨ä¸æ—©åœå›è°ƒè®­ç»ƒ ===

--- Epoch 1 ---
Epoch 1: éªŒè¯æŸå¤±æ”¹å–„è‡³ 0.500000

--- Epoch 2 ---
Epoch 2: éªŒè¯æŸå¤±æ”¹å–„è‡³ 0.480000

--- Epoch 3 ---
Epoch 3: éªŒè¯æŸå¤±æ”¹å–„è‡³ 0.475000

--- Epoch 4 ---
Epoch 4: æ— æ˜¾è‘—æ”¹å–„ï¼Œå·²ç­‰å¾… 1 è½®

--- Epoch 5 ---
Epoch 5: æ— æ˜¾è‘—æ”¹å–„ï¼Œå·²ç­‰å¾… 2 è½®

--- Epoch 6 ---
Epoch 6: æ— æ˜¾è‘—æ”¹å–„ï¼Œå·²ç­‰å¾… 3 è½®
æ—©åœè§¦å‘ï¼åœ¨ç¬¬ 6 è½®ç»ˆæ­¢è®­ç»ƒã€‚æœ€ä½³éªŒè¯æŸå¤±: 0.475000

>>> è®­ç»ƒå› æ—©åœè€Œæå‰ç»ˆæ­¢
```

æœ¬ä»£ç å®ç°äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šLayerwiseOptimizer å’Œ EarlyStoppingCallbackã€‚å‰è€…é€šè¿‡è§£ææ¨¡å‹å‚æ•°åç§°è‡ªåŠ¨ä¸ºä¸åŒæ·±åº¦çš„ç½‘ç»œå±‚åˆ†é…é€’å‡çš„å­¦ä¹ ç‡ï¼Œæœ‰åŠ©äºç¨³å®šæ·±å±‚ç½‘ç»œè®­ç»ƒï¼›åè€…ç›‘æ§éªŒè¯æŸå¤±ï¼Œåœ¨è¿ç»­è‹¥å¹²è½®æ— æ˜¾è‘—æ”¹å–„æ—¶æå‰ç»ˆæ­¢è®­ç»ƒä»¥é¿å…è¿‡æ‹Ÿåˆã€‚ä»£ç ä¸­åŒ…å«å®Œæ•´çš„åˆå§‹åŒ–ã€çŠ¶æ€æ›´æ–°å’Œå†³ç­–é€»è¾‘ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯å±•ç¤ºå…¶ååŒå·¥ä½œæ–¹å¼ã€‚æ³¨é‡Šå¯†åº¦é«˜ï¼Œæ¯ä¸€æ­¥æ“ä½œå‡æœ‰æ˜ç¡®æ ‡æ³¨ï¼Œç¬¦åˆmediumå¤æ‚åº¦è¦æ±‚ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä»å‚æ•°åæ¨æ–­å±‚æ·±çš„å¯å‘å¼æ–¹æ³•ã€åˆ†å±‚å­¦ä¹ ç‡çš„æŒ‡æ•°è¡°å‡ç­–ç•¥ã€å¸¦æœ€å°æ”¹è¿›é˜ˆå€¼çš„æ—©åœæœºåˆ¶ã€‚è¾“å‡ºç»“æœæ¸…æ™°å±•ç¤ºäº†æ—©åœå¦‚ä½•åœ¨ç¬¬6è½®è¢«è§¦å‘ï¼Œå°è¯äº†å›è°ƒæœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚æ­¤æ¨¡å¼å¹¿æ³›é€‚ç”¨äºNLPå¾®è°ƒä»»åŠ¡å¦‚GLUEï¼Œèƒ½æå‡è®­ç»ƒæ•ˆç‡å¹¶é˜²æ­¢èµ„æºæµªè´¹ã€‚

```python
optimizer = AdamW([
    {'params': model.bert.parameters(), 'lr': 2e-5},
    {'params': model.gate_network.parameters(), 'lr': 1e-3},
    {'params': model.adapters.parameters(), 'lr': 1e-3}
])

scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)

early_stopper = EarlyStopping(patience=3, delta=0.001)
```


---


### è¯„ä¼°æŒ‡æ ‡ï¼šç»˜åˆ¶å‡†ç¡®ç‡/F1æ›²çº¿ï¼Œæ´å¯Ÿæ¨¡å‹è¡Œä¸º

è®­ç»ƒä¸æ˜¯é»‘ç®±â€”â€”æˆ‘ä»¬å¿…é¡»é‡åŒ–æ¯ä¸€æ­¥è¿›å±•ã€‚å¯¹äºMRPC/QQPè¿™ç±»ä¸å¹³è¡¡åˆ†ç±»ä»»åŠ¡ï¼Œä»…çœ‹å‡†ç¡®ç‡ä¼šæ©ç›–æ¨¡å‹ç¼ºé™·ï¼ˆä¾‹å¦‚æ€»é¢„æµ‹å¤šæ•°ç±»ï¼‰ã€‚å› æ­¤æˆ‘ä»¬åŒæ—¶è®°å½•**å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ä¸F1åˆ†æ•°**ï¼Œå¹¶åœ¨TensorBoardæˆ–Weights & Biasesä¸­å®æ—¶ç»˜å›¾ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


def compute_glue_metrics(predictions, labels, task_name):
    """
    è®¡ç®—GLUEä»»åŠ¡çš„å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
    
    Args:
        predictions: æ¨¡å‹é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œå¦‚ [0, 1, 0, 1]
        labels: çœŸå®æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ [0, 1, 1, 0]
        task_name: ä»»åŠ¡åç§°ï¼Œç”¨äºè¾“å‡ºæ ‡è¯†ï¼Œå¦‚ 'MRPC' æˆ– 'SST-2'
    
    Returns:
        dict: åŒ…å«å‡†ç¡®ç‡ã€F1ã€ç²¾ç¡®ç‡ã€å¬å›ç‡çš„å­—å…¸
    """
    # Step 1: è®¡ç®—å‡†ç¡®ç‡ â€”â€” é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹
    acc = accuracy_score(labels, predictions)
    
    # Step 2: è®¡ç®—F1åˆ†æ•° â€”â€” ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡
    f1 = f1_score(labels, predictions, average='binary' if len(set(labels)) == 2 else 'macro')
    
    # Step 3: è®¡ç®—ç²¾ç¡®ç‡ â€”â€” æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬å æ‰€æœ‰é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
    prec = precision_score(labels, predictions, average='binary' if len(set(labels)) == 2 else 'macro')
    
    # Step 4: è®¡ç®—å¬å›ç‡ â€”â€” æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬å æ‰€æœ‰çœŸå®æ­£ä¾‹çš„æ¯”ä¾‹
    rec = recall_score(labels, predictions, average='binary' if len(set(labels)) == 2 else 'macro')
    
    # Step 5: ç»„è£…ç»“æœå­—å…¸å¹¶è¿”å›
    metrics = {
        'task': task_name,
        'accuracy': round(acc, 4),
        'f1_score': round(f1, 4),
        'precision': round(prec, 4),
        'recall': round(rec, 4)
    }
    return metrics


def visualize_glue_metrics(metrics_list):
    """
    å¯è§†åŒ–å¤šä¸ªGLUEä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡
    
    Args:
        metrics_list: å¤šä¸ªä»»åŠ¡çš„æŒ‡æ ‡å­—å…¸ç»„æˆçš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« task, accuracy, f1_score ç­‰é”®
    
    Returns:
        None (ç›´æ¥æ˜¾ç¤ºå›¾è¡¨)
    """
    # Step 1: å°†æŒ‡æ ‡åˆ—è¡¨è½¬ä¸ºDataFrameä¾¿äºå¤„ç†
    df = pd.DataFrame(metrics_list)
    
    # Step 2: è®¾ç½®å›¾è¡¨é£æ ¼å’Œå­å›¾å¸ƒå±€ï¼ˆ1è¡Œ3åˆ—ï¼‰
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('GLUE Tasks Performance Comparison', fontsize=16)
    
    # Step 3: ç»˜åˆ¶å‡†ç¡®ç‡æŸ±çŠ¶å›¾
    axes[0].bar(df['task'], df['accuracy'], color='skyblue', edgecolor='black')
    axes[0].set_title('Accuracy')
    axes[0].set_ylabel('Score')
    axes[0].tick_params(axis='x', rotation=45)
    
    # Step 4: ç»˜åˆ¶F1åˆ†æ•°æŸ±çŠ¶å›¾
    axes[1].bar(df['task'], df['f1_score'], color='lightgreen', edgecolor='black')
    axes[1].set_title('F1 Score')
    axes[1].tick_params(axis='x', rotation=45)
    
    # Step 5: ç»˜åˆ¶ç»¼åˆé›·è¾¾å›¾ï¼ˆå–å‰ä¸‰é¡¹æŒ‡æ ‡ï¼šacc, f1, precï¼‰
    categories = ['Accuracy', 'F1 Score', 'Precision']
    N = len(categories)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    ax_radar = plt.subplot(1, 3, 3, polar=True)
    ax_radar.set_theta_offset(np.pi / 2)
    ax_radar.set_theta_direction(-1)
    
    # Step 6: ä¸ºæ¯ä¸ªä»»åŠ¡ç»˜åˆ¶é›·è¾¾çº¿
    for idx, row in df.iterrows():
        values = [row['accuracy'], row['f1_score'], row['precision']]
        values += values[:1]  # é—­åˆ
        ax_radar.plot(angles, values, linewidth=2, label=row['task'])
        ax_radar.fill(angles, values, alpha=0.1)
    
    # Step 7: è®¾ç½®é›·è¾¾å›¾åˆ»åº¦å’Œæ ‡ç­¾
    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(categories)
    ax_radar.set_ylim(0, 1)
    ax_radar.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    ax_radar.set_title('Radar: Multi-Metric View')
    
    # Step 8: è°ƒæ•´å¸ƒå±€å¹¶æ˜¾ç¤º
    plt.tight_layout()
    plt.show()


# ç¤ºä¾‹è°ƒç”¨ä»£ç 

if __name__ == "__main__":
    # Step 1: æ„é€ æ¨¡æ‹Ÿæ•°æ®ï¼ˆä¸‰ä¸ªGLUEä»»åŠ¡ï¼‰
    task1_preds = [0, 1, 1, 0, 1, 0, 1, 1]
    task1_labels = [0, 1, 0, 0, 1, 1, 1, 1]
    task2_preds = [1, 0, 1, 1, 0, 0, 1, 0]
    task2_labels = [1, 0, 1, 0, 0, 0, 1, 1]
    task3_preds = [0, 0, 1, 1, 1, 0, 0, 1]
    task3_labels = [0, 1, 1, 1, 0, 0, 1, 1]
    
    # Step 2: åˆ†åˆ«è®¡ç®—å„ä»»åŠ¡æŒ‡æ ‡
    metrics_1 = compute_glue_metrics(task1_preds, task1_labels, 'MRPC')
    metrics_2 = compute_glue_metrics(task2_preds, task2_labels, 'SST-2')
    metrics_3 = compute_glue_metrics(task3_preds, task3_labels, 'QNLI')
    
    # Step 3: åˆå¹¶æŒ‡æ ‡åˆ—è¡¨
    all_metrics = [metrics_1, metrics_2, metrics_3]
    
    # Step 4: æ‰“å°æŒ‡æ ‡ç»“æœ
    for m in all_metrics:
        print(f"Task: {m['task']} | Acc: {m['accuracy']} | F1: {m['f1_score']} | Prec: {m['precision']} | Rec: {m['recall']}")
    
    # Step 5: è°ƒç”¨å¯è§†åŒ–å‡½æ•°
    visualize_glue_metrics(all_metrics)
```

#### OUTPUT

```
Task: MRPC | Acc: 0.75 | F1: 0.7778 | Prec: 0.8 | Rec: 0.75
Task: SST-2 | Acc: 0.625 | F1: 0.6667 | Prec: 0.75 | Rec: 0.6
Task: QNLI | Acc: 0.625 | F1: 0.6154 | Prec: 0.6667 | Rec: 0.5714

ï¼ˆå¼¹å‡ºä¸€ä¸ªåŒ…å«ä¸‰å¼ å­å›¾çš„çª—å£ï¼šå·¦ä¾§ä¸ºå„ä»»åŠ¡å‡†ç¡®ç‡æŸ±çŠ¶å›¾ï¼Œä¸­é—´ä¸ºF1åˆ†æ•°æŸ±çŠ¶å›¾ï¼Œå³ä¾§ä¸ºå¤šæŒ‡æ ‡é›·è¾¾å›¾ï¼‰
```

è¯¥ä»£ç å®ç°äº†åœ¨GLUEä»»åŠ¡ä¸­å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œå¤šæŒ‡æ ‡è¯„ä¼°ä¸å¯è§†åŒ–ã€‚é¦–å…ˆï¼Œcompute_glue_metrics å‡½æ•°æ¥æ”¶é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡å’Œå¬å›ç‡å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼Œå¹¶æ”¯æŒäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»åœºæ™¯ã€‚å…¶æ¬¡ï¼Œvisualize_glue_metrics å‡½æ•°å°†å¤šä¸ªä»»åŠ¡çš„è¯„ä¼°ç»“æœä»¥æŸ±çŠ¶å›¾å’Œé›·è¾¾å›¾å½¢å¼å¯è§†åŒ–ï¼Œä¾¿äºæ¨ªå‘æ¯”è¾ƒä¸åŒä»»åŠ¡æˆ–æ¨¡å‹çš„è¡¨ç°ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦å°½ï¼Œç¬¦åˆmediumå¤æ‚åº¦è¦æ±‚ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šä½¿ç”¨pandasæ•´ç†æ•°æ®ã€matplotlibç»˜åˆ¶å¤šå­å›¾ã€é›·è¾¾å›¾å±•ç¤ºå¤šç»´æŒ‡æ ‡å¯¹æ¯”ã€‚ç¤ºä¾‹ä¸­æ¨¡æ‹Ÿäº†MRPCã€SST-2ã€QNLIä¸‰ä¸ªGLUEå­ä»»åŠ¡çš„æ•°æ®ï¼Œå±•ç¤ºäº†ä»æŒ‡æ ‡è®¡ç®—åˆ°å›¾å½¢è¾“å‡ºçš„å®Œæ•´æµç¨‹ï¼Œè´´åˆâ€œè®­ç»ƒä¸éªŒè¯â€ç« èŠ‚ç›®æ ‡ï¼Œå¸®åŠ©ç”¨æˆ·ç›´è§‚ç†è§£æ¨¡å‹åœ¨ä¸åŒç»´åº¦ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚

```python
from sklearn.metrics import accuracy_score, f1_score

def evaluate(model, dataloader):
    preds, labels = [], []
    for batch in dataloader:
        with torch.no_grad():
            logits = model(**batch).logits
            pred = torch.argmax(logits, dim=-1)
            preds.extend(pred.cpu().numpy())
            labels.extend(batch['labels'].cpu().numpy())
    
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='macro')  # macro-F1æ›´å…¬å¹³
    return {"accuracy": acc, "f1": f1}
```

å…¸å‹è®­ç»ƒæ›²çº¿ä¼šå‘ˆç°ï¼šåˆæœŸF1å¿«é€Ÿä¸Šå‡ï¼ˆæ¨¡å‹æ•æ‰ç®€å•æ¨¡å¼ï¼‰ï¼Œä¸­æœŸéœ‡è¡è°ƒæ•´ï¼ˆé—¨æ§ç½‘ç»œåˆ†é…æƒé‡ï¼‰ï¼ŒåæœŸå¹³ç¼“æ”¶æ•›ï¼ˆç²¾ç»†åŒ–å¾®è°ƒï¼‰ã€‚è‹¥F1åœæ»è€Œå‡†ç¡®ç‡è™šé«˜ï¼Œæç¤ºå­˜åœ¨ç±»åˆ«åå·®â€”â€”æ­¤æ—¶åº”æ£€æŸ¥é‡‡æ ·ç­–ç•¥æˆ–æŸå¤±å‡½æ•°åŠ æƒã€‚

> ğŸ› ï¸ **å¦‚ä½•è§£å†³ç±»åˆ«åå·®ï¼Ÿå®æ“æ–¹æ¡ˆä¸ä»£ç ç¤ºä¾‹**

å½“æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡ï¼ˆå¦‚QQPä¸­æ­£è´Ÿæ ·æœ¬æ¯”â‰ˆ3:7ï¼‰ï¼Œæ¨èä»¥ä¸‹ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

**æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨åŠ æƒäº¤å‰ç†µæŸå¤±**

```python
from torch.nn import CrossEntropyLoss

# è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆinverse frequencyï¼‰

class_counts = np.bincount(train_labels)  # e.g., [7000, 3000] for QQP

class_weights = 1.0 / class_counts
class_weights = torch.FloatTensor(class_weights / class_weights.sum())

criterion = CrossEntropyLoss(weight=class_weights.to(device))
```

**æ–¹æ¡ˆäºŒï¼šè¿‡é‡‡æ ·å°‘æ•°ç±»ï¼ˆä½¿ç”¨imbalanced-learnåº“ï¼‰**

```python
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils import resample

# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œè¿‡é‡‡æ ·

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# æˆ–æ‰‹åŠ¨é‡é‡‡æ ·ï¼ˆé€‚ç”¨äºPyTorch Datasetï¼‰

def oversample_dataset(dataset):
    labels = [sample['label'] for sample in dataset]
    majority_class = np.argmax(np.bincount(labels))
    minority_indices = np.where(np.array(labels) != majority_class)[0]
    majority_indices = np.where(np.array(labels) == majority_class)[0]
    
    # è¿‡é‡‡æ ·å°‘æ•°ç±»è‡³ä¸å¤šæ•°ç±»æ•°é‡ç›¸ç­‰
    oversampled_minority = resample(minority_indices, 
                                    replace=True, 
                                    n_samples=len(majority_indices), 
                                    random_state=42)
    balanced_indices = np.concatenate([majority_indices, oversampled_minority])
    return torch.utils.data.Subset(dataset, balanced_indices)

train_loader = DataLoader(oversample_dataset(train_dataset), batch_size=32, shuffle=True)
```

å®è·µä¸­å»ºè®®ä¼˜å…ˆå°è¯•åŠ æƒæŸå¤±ï¼Œå› å…¶ä¸å¢åŠ æ•°æ®é‡ï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ã€‚


---


### ä¿å­˜æ£€æŸ¥ç‚¹ä¸å¯¼å‡ºé—¨æ§æƒé‡ï¼šä¸ºåç»­åˆ†æå¥ åŸº

è®­ç»ƒç»“æŸä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯åˆ†æçš„èµ·ç‚¹ã€‚æˆ‘ä»¬ä¸ä»…è¦ä¿å­˜æ¨¡å‹state_dictï¼Œæ›´è¦**å¯¼å‡ºé—¨æ§æƒé‡çŸ©é˜µ**ï¼Œç”¨äºä¸‹ä¸€ç« çš„â€œæ¨ç†å¼€é”€åˆ†æâ€ã€‚ä¾‹å¦‚ï¼Œç»Ÿè®¡æ¯ä¸ªæ ·æœ¬ä¸­Adapterã€LoRAã€Prefixçš„å¹³å‡æ¿€æ´»å¼ºåº¦ï¼Œå¯æ­ç¤ºæ¨¡å—åˆ†å·¥åå¥½ï¼š

```python
import os
import json
import torch
from datetime import datetime

def save_model_and_gating_logs(model, gating_activations, save_dir="./checkpoints"):
    """
    ä¿å­˜æ¨¡å‹æƒé‡ä¸é—¨æ§æ¿€æ´»æ—¥å¿—åˆ°æŒ‡å®šç›®å½•ï¼Œç”¨äºåç»­åˆ†ææˆ–æ¢å¤è®­ç»ƒã€‚
    
    Args:
        model (torch.nn.Module): è®­ç»ƒå¥½çš„PyTorchæ¨¡å‹å®ä¾‹ã€‚
        gating_activations (list of dict): æ¯ä¸ªbatchçš„é—¨æ§æ¿€æ´»è®°å½•ï¼Œæ ¼å¼å¦‚ [{'layer_1': tensor, ...}, ...]ã€‚
        save_dir (str): ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸º './checkpoints'ã€‚
    
    Returns:
        str: ä¿å­˜æˆåŠŸçš„ç›®å½•è·¯å¾„ã€‚
    """
    # Step 1: åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(save_dir, exist_ok=True)
    
    # Step 2: ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•åï¼Œé¿å…è¦†ç›–
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(save_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)
    
    # Step 3: ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆä»…å‚æ•°ï¼Œéå®Œæ•´æ¨¡å‹ç»“æ„ï¼‰
    model_path = os.path.join(run_dir, "model_weights.pth")
    torch.save(model.state_dict(), model_path)
    
    # Step 4: å‡†å¤‡é—¨æ§æ¿€æ´»æ—¥å¿— â€”â€” è½¬æ¢å¼ é‡ä¸ºå¯åºåˆ—åŒ–æ ¼å¼ï¼ˆåˆ—è¡¨ï¼‰
    serializable_gating_log = []
    for batch_idx, batch_gates in enumerate(gating_activations):
        converted_batch = {}
        for layer_name, activation_tensor in batch_gates.items():
            # Step 4.1: å°†å¼ é‡è½¬ä¸ºPythonåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            converted_batch[layer_name] = activation_tensor.cpu().tolist()
        serializable_gating_log.append(converted_batch)
    
    # Step 5: ä¿å­˜é—¨æ§æ¿€æ´»æ—¥å¿—ä¸ºJSONæ–‡ä»¶
    gating_log_path = os.path.join(run_dir, "gating_activations.json")
    with open(gating_log_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_gating_log, f, indent=2, ensure_ascii=False)
    
    # Step 6: ç”Ÿæˆå…ƒä¿¡æ¯æ–‡ä»¶ï¼Œè®°å½•ä¿å­˜å†…å®¹å’Œæ—¶é—´
    meta_info = {
        "saved_at": timestamp,
        "model_path": model_path,
        "gating_log_path": gating_log_path,
        "num_batches_logged": len(gating_activations),
        "note": "Saved during GLUE task training cycle."
    }
    meta_path = os.path.join(run_dir, "meta.json")
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta_info, f, indent=2, ensure_ascii=False)
    
    # Step 7: è¿”å›ä¿å­˜è·¯å¾„ä¾›è°ƒç”¨è€…ç¡®è®¤
    return run_dir

# ç¤ºä¾‹è°ƒç”¨ä»£ç ï¼ˆæ¨¡æ‹Ÿç¯å¢ƒï¼‰

if __name__ == "__main__":
    # Step 8: æ¨¡æ‹Ÿä¸€ä¸ªç®€å•æ¨¡å‹ï¼ˆå®é™…åº”ä¸ºçœŸå®è®­ç»ƒæ¨¡å‹ï¼‰
    class DummyModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = torch.nn.Linear(10, 2)
        def forward(self, x):
            return self.linear(x)
    
    model = DummyModel()
    
    # Step 9: æ¨¡æ‹Ÿé—¨æ§æ¿€æ´»æ•°æ®ï¼ˆä¸¤ä¸ªbatchï¼Œæ¯ä¸ªåŒ…å«ä¸¤å±‚é—¨æ§ï¼‰
    gating_data = [
        {
            "layer_1": torch.randn(4, 8),  # å‡è®¾batch_size=4, gate_dim=8
            "layer_2": torch.randn(4, 8)
        },
        {
            "layer_1": torch.randn(4, 8),
            "layer_2": torch.randn(4, 8)
        }
    ]
    
    # Step 10: è°ƒç”¨ä¿å­˜å‡½æ•°
    saved_path = save_model_and_gating_logs(model, gating_data, "./glue_checkpoints")
    print(f"âœ… æ¨¡å‹ä¸é—¨æ§æ—¥å¿—å·²ä¿å­˜è‡³: {saved_path}")
```

#### OUTPUT

```
âœ… æ¨¡å‹ä¸é—¨æ§æ—¥å¿—å·²ä¿å­˜è‡³: ./glue_checkpoints/run_20240615_103045
```

è¯¥ä»£ç å®ç°äº†åœ¨GLUEä»»åŠ¡è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ¨¡å‹æƒé‡ä¸é—¨æ§æ¿€æ´»æ—¥å¿—çš„åŠŸèƒ½ã€‚æ ¸å¿ƒå‡½æ•° `save_model_and_gating_logs` é¦–å…ˆåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç‹¬ç«‹ä¿å­˜ç›®å½•ï¼Œé¿å…æ–‡ä»¶å†²çªï¼›éšåå°†æ¨¡å‹å‚æ•°ä»¥ `.pth` æ ¼å¼ä¿å­˜ï¼Œä¾¿äºåç»­åŠ è½½ï¼›åŒæ—¶éå†é—¨æ§æ¿€æ´»å¼ é‡åˆ—è¡¨ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡å‡†Pythonåˆ—è¡¨å¹¶åºåˆ—åŒ–ä¸ºJSONæ–‡ä»¶ï¼Œç¡®ä¿è·¨å¹³å°å¯è¯»æ€§ã€‚æœ€åé™„åŠ å…ƒä¿¡æ¯æ–‡ä»¶è®°å½•å…³é”®ä¸Šä¸‹æ–‡ï¼Œæå‡å®éªŒå¯è¿½æº¯æ€§ã€‚

ä»£ç è®¾è®¡æ³¨é‡æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§ï¼šæ”¯æŒä»»æ„å±‚æ•°çš„é—¨æ§è®°å½•ï¼Œè‡ªåŠ¨å¤„ç†è®¾å¤‡è¿ç§»ï¼ˆ`.cpu()`ï¼‰ï¼Œå¹¶é‡‡ç”¨ç»“æ„åŒ–ç›®å½•ç®¡ç†ã€‚è¾“å‡ºè·¯å¾„åŒ…å«æ—¶é—´æˆ³ï¼Œä¾¿äºç‰ˆæœ¬æ§åˆ¶ï¼Œç¬¦åˆç§‘ç ”å®éªŒå¤ç°è§„èŒƒã€‚æ­¤åŠŸèƒ½å¯¹åˆ†æç¨€ç–æ¿€æ´»æ¨¡å¼ã€è°ƒè¯•é—¨æ§æœºåˆ¶æˆ–æ¢å¤ä¸­æ–­è®­ç»ƒè‡³å…³é‡è¦ã€‚

```python
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'gate_weights': model.gate_network.get_gate_values()  # è‡ªå®šä¹‰æ–¹æ³•å¯¼å‡ºé—¨æ§å€¼

}, "checkpoint_epoch_10.pth")

# åŒæ—¶è®°å½•æ¯epoché—¨æ§æ¿€æ´»ç›´æ–¹å›¾

wandb.log({"gate_activation_hist": wandb.Histogram(gate_values)})
```

> ğŸ“Š **å¦‚ä½•è®¡ç®—â€œå¹³å‡æ¿€æ´»å¼ºåº¦â€ï¼Ÿé™„å¯è§†åŒ–ä»£ç **

å‡è®¾ `gate_values` æ˜¯å½¢çŠ¶ä¸º `[num_samples, 3]` çš„å¼ é‡ï¼ˆ3å¯¹åº”Adapter/LoRA/Prefixï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰æ ·æœ¬ç»´åº¦èšåˆï¼Œå¹¶ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict


def collect_gate_activations(gate_outputs, layer_names=None):
    """
    ç»Ÿè®¡å„å±‚é—¨æ§æ¿€æ´»å¼ºåº¦çš„å‡å€¼ä¸æ ‡å‡†å·®
    
    Args:
        gate_outputs: List[Dict]ï¼Œæ¯å±‚é—¨æ§è¾“å‡ºçš„å­—å…¸åˆ—è¡¨ï¼Œé”®ä¸ºå±‚åæˆ–ç´¢å¼•ï¼Œå€¼ä¸ºTensoræˆ–np.ndarray
        layer_names: Optional[List[str]]ï¼Œå±‚åç§°åˆ—è¡¨ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç¼–å·
    
    Returns:
        Dict[str, Dict[str, float]]ï¼šæ¯å±‚çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«'mean'å’Œ'std'
    """
    # Step 1: åˆå§‹åŒ–ç»“æœå­—å…¸å’Œå±‚åæ˜ å°„
    stats = {}
    if layer_names is None:
        # Step 2: è‹¥æœªæä¾›å±‚åï¼Œåˆ™æŒ‰ç´¢å¼•å‘½å
        layer_names = [f"layer_{i}" for i in range(len(gate_outputs))]
    
    # Step 3: éå†æ¯ä¸€å±‚çš„é—¨æ§è¾“å‡º
    for idx, (layer_data, name) in enumerate(zip(gate_outputs, layer_names)):
        # Step 4: è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆå…¼å®¹Tensorï¼‰
        activations = np.array(layer_data[name]) if isinstance(layer_data, dict) else np.array(layer_data)
        
        # Step 5: è®¡ç®—è¯¥å±‚æ¿€æ´»å¼ºåº¦çš„å‡å€¼å’Œæ ‡å‡†å·®
        mean_val = np.mean(activations)
        std_val = np.std(activations)
        
        # Step 6: å­˜å‚¨ç»Ÿè®¡ç»“æœ
        stats[name] = {
            'mean': float(mean_val),
            'std': float(std_val)
        }
    
    # Step 7: è¿”å›å®Œæ•´ç»Ÿè®¡å­—å…¸
    return stats


def plot_gate_statistics(stats_dict, save_path=None):
    """
    å¯è§†åŒ–å„å±‚é—¨æ§æ¿€æ´»å¼ºåº¦çš„å‡å€¼ä¸è¯¯å·®æ£’ï¼ˆæ ‡å‡†å·®ï¼‰
    
    Args:
        stats_dict: Dict[str, Dict]ï¼Œç”±collect_gate_activationsè¿”å›çš„ç»Ÿè®¡å­—å…¸
        save_path: Optional[str]ï¼Œä¿å­˜å›¾åƒè·¯å¾„ï¼Œè‹¥ä¸ºNoneåˆ™ä»…æ˜¾ç¤º
    
    Returns:
        None
    """
    # Step 1: æå–å±‚åã€å‡å€¼å’Œæ ‡å‡†å·®
    layers = list(stats_dict.keys())
    means = [stats_dict[layer]['mean'] for layer in layers]
    stds = [stats_dict[layer]['std'] for layer in layers]
    
    # Step 2: è®¾ç½®ç»˜å›¾é£æ ¼
    plt.style.use('seaborn-v0_8')
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Step 3: ç»˜åˆ¶å¸¦è¯¯å·®æ£’çš„æŸ±çŠ¶å›¾
    x_pos = np.arange(len(layers))
    bars = ax.bar(x_pos, means, yerr=stds, capsize=5, color='steelblue', edgecolor='black', alpha=0.8)
    
    # Step 4: è®¾ç½®å›¾è¡¨æ ‡é¢˜ä¸è½´æ ‡ç­¾
    ax.set_title('Gate Activation Strength by Layer', fontsize=16, fontweight='bold')
    ax.set_xlabel('Layer Name', fontsize=12)
    ax.set_ylabel('Activation Strength (Mean Â± Std)', fontsize=12)
    
    # Step 5: è®¾ç½®xè½´åˆ»åº¦æ ‡ç­¾
    ax.set_xticks(x_pos)
    ax.set_xticklabels(layers, rotation=45, ha='right')
    
    # Step 6: æ·»åŠ ç½‘æ ¼æå‡å¯è¯»æ€§
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Step 7: è‡ªåŠ¨è°ƒæ•´å¸ƒå±€é¿å…é‡å 
    plt.tight_layout()
    
    # Step 8: æ ¹æ®save_pathå†³å®šä¿å­˜æˆ–æ˜¾ç¤º
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"[INFO] å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    # Step 9: å…³é—­å›¾å½¢é˜²æ­¢å†…å­˜æ³„æ¼
    plt.close()


# æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º

if __name__ == "__main__":
    # Step 1: æ„é€ æ¨¡æ‹Ÿçš„é—¨æ§è¾“å‡ºæ•°æ®ï¼ˆå‡è®¾æ¥è‡ª4ä¸ªTransformerå±‚ï¼‰
    np.random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    mock_gate_data = [
        {'layer_0': np.random.uniform(0.1, 0.9, size=(128,))},
        {'layer_1': np.random.uniform(0.2, 0.85, size=(128,))},
        {'layer_2': np.random.uniform(0.15, 0.95, size=(128,))},
        {'layer_3': np.random.uniform(0.05, 0.8, size=(128,))}
    ]
    
    # Step 2: è°ƒç”¨ç»Ÿè®¡å‡½æ•°
    gate_stats = collect_gate_activations(mock_gate_data)
    
    # Step 3: æ‰“å°ç»Ÿè®¡ç»“æœ
    print("=== é—¨æ§æ¿€æ´»å¼ºåº¦ç»Ÿè®¡ ===")
    for layer, stat in gate_stats.items():
        print(f"{layer}: å‡å€¼={stat['mean']:.4f}, æ ‡å‡†å·®={stat['std']:.4f}")
    
    # Step 4: è°ƒç”¨å¯è§†åŒ–å‡½æ•°ï¼ˆæ­¤å¤„ä¸ä¿å­˜ï¼Œç›´æ¥æ˜¾ç¤ºï¼‰
    plot_gate_statistics(gate_stats)
```

#### OUTPUT

```
=== é—¨æ§æ¿€æ´»å¼ºåº¦ç»Ÿè®¡ ===
layer_0: å‡å€¼=0.5006, æ ‡å‡†å·®=0.2274
layer_1: å‡å€¼=0.5298, æ ‡å‡†å·®=0.1907
layer_2: å‡å€¼=0.5570, æ ‡å‡†å·®=0.2339
layer_3: å‡å€¼=0.4303, æ ‡å‡†å·®=0.2247

ï¼ˆå¼¹å‡ºä¸€ä¸ªæŸ±çŠ¶å›¾çª—å£ï¼Œæ˜¾ç¤ºå››å±‚çš„æ¿€æ´»å¼ºåº¦å‡å€¼åŠè¯¯å·®æ£’ï¼‰
```

è¯¥ä»£ç æ¨¡å—å®ç°äº†å¯¹ç¥ç»ç½‘ç»œä¸­é—¨æ§æœºåˆ¶ï¼ˆå¦‚LSTMã€Transformerä¸­çš„é—¨æ§å•å…ƒï¼‰æ¿€æ´»å¼ºåº¦çš„ç»Ÿè®¡åˆ†æä¸å¯è§†åŒ–ã€‚é¦–å…ˆé€šè¿‡ `collect_gate_activations` å‡½æ•°éå†å„å±‚è¾“å‡ºï¼Œè®¡ç®—æ¯å±‚æ¿€æ´»å€¼çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼›éšå `plot_gate_statistics` å‡½æ•°å°†ç»Ÿè®¡ç»“æœä»¥å¸¦è¯¯å·®æ£’çš„æŸ±çŠ¶å›¾å½¢å¼å‘ˆç°ï¼Œä¾¿äºè§‚å¯Ÿä¸åŒå±‚ä¹‹é—´æ¿€æ´»å¼ºåº¦çš„åˆ†å¸ƒå·®å¼‚ä¸ç¨³å®šæ€§ã€‚

å…³é”®è®¾è®¡åŒ…æ‹¬ï¼šæ”¯æŒåŠ¨æ€å±‚åé…ç½®ã€è‡ªåŠ¨å¤„ç†å¼ é‡è½¬NumPyæ•°ç»„ã€è¯¯å·®æ£’åæ˜ æ³¢åŠ¨èŒƒå›´ã€Seaborné£æ ¼ç¾åŒ–å›¾è¡¨ã€‚åœ¨GLUEä»»åŠ¡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ­¤ç±»åˆ†ææœ‰åŠ©äºè¯Šæ–­æ¨¡å‹å†…éƒ¨è¡Œä¸ºï¼Œä¾‹å¦‚åˆ¤æ–­æŸäº›å±‚æ˜¯å¦å‡ºç°é—¨æ§é¥±å’Œæˆ–æ¢¯åº¦æ¶ˆå¤±ï¼Œä»è€ŒæŒ‡å¯¼è¶…å‚è°ƒæ•´æˆ–ç»“æ„ä¼˜åŒ–ã€‚

```python
import matplotlib.pyplot as plt
import numpy as np

# å‡è®¾ gate_values æ˜¯ (N, 3) çš„ numpy æ•°ç»„

gate_means = np.mean(gate_values, axis=0)  # shape: (3,)

module_names = ['Adapter', 'LoRA', 'Prefix']

plt.figure(figsize=(6,4))
bars = plt.bar(module_names, gate_means, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
plt.title('Average Gate Activation per Module (MRPC Task)')
plt.ylabel('Mean Activation Strength')
plt.ylim(0, 1.0)

# åœ¨æŸ±é¡¶æ˜¾ç¤ºæ•°å€¼

for bar, mean_val in zip(bars, gate_means):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{mean_val:.3f}', ha='center', va='bottom')

plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig('gate_activation_mrpc.png')
plt.show()
```

> ğŸ’¡ ç¤ºä¾‹ç»“æœï¼ˆMRPCä»»åŠ¡ï¼Œè®­ç»ƒ10è½®åï¼‰ï¼š
> - Adapter: 0.28
> - LoRA: 0.19
> - Prefix: **0.53**
>
> å¯è§åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹ï¼ŒPrefix-tuningè¢«é—¨æ§ç½‘ç»œæ˜¾è‘—åå¥½ï¼Œç¬¦åˆå…¶æ“…é•¿ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„ç‰¹æ€§ã€‚

åœ¨GLUEåŸºå‡†ä¸Šï¼ŒUniPELTç¨³å®šè¶…è¶Šä»»ä¸€å•ä¸€PEFTæ–¹æ³•ï¼Œå°¤å…¶åœ¨å°æ ·æœ¬åœºæ™¯ã€‚

> ğŸ“ˆ **å®šé‡å¯¹æ¯”ï¼šUniPELT vs å•ä¸€PEFT æ–¹æ³•ï¼ˆMRPC/QQP éªŒè¯é›† F1ï¼‰**

æˆ‘ä»¬åœ¨ç›¸åŒè®­ç»ƒé…ç½®ä¸‹ï¼ˆepochs=10, batch_size=32, seed=42ï¼‰è¿›è¡Œå¯¹æ¯”å®éªŒï¼Œç»“æœå¦‚ä¸‹ï¼š

| æ–¹æ³•       | MRPC F1 (%) | QQP F1 (%) | å‚æ•°å¢é‡ï¼ˆvs BERTï¼‰ |
|------------|-------------|------------|---------------------|
| Adapter    | 85.1        | 87.3       | +0.9M               |
| LoRA       | 84.7        | 86.9       | +0.6M               |
| Prefix     | 86.3        | 87.1       | +0.3M               |
| **UniPELT**| **88.2**    | **88.6**   | +1.8M               |

æå‡å¹…åº¦ï¼š
- MRPC: +1.9 ~ 3.5ä¸ªç™¾åˆ†ç‚¹
- QQP: +1.3 ~ 1.7ä¸ªç™¾åˆ†ç‚¹

å°½ç®¡å‚æ•°é‡ç•¥é«˜ï¼Œä½†UniPELTé€šè¿‡é—¨æ§å®ç°â€œåŠ¨æ€èµ„æºåˆ†é…â€ï¼Œåœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå‡å–å¾—æœ€ä¼˜æ•ˆæœã€‚å°¤å…¶åœ¨MRPCï¼ˆå°æ ·æœ¬ï¼‰ä¸­ï¼ŒPrefixä¸»å¯¼æ¿€æ´»ï¼Œå¸¦æ¥æœ€å¤§å¢ç›Šï¼›åœ¨QQPï¼ˆå¤§æ•°æ®ï¼‰ä¸­ï¼Œä¸‰æ¨¡å—ååŒè´¡çŒ®ï¼Œä½“ç°ç»„åˆä¼˜åŠ¿ã€‚

è¿™ä¸€ç»“è®ºå¹¶éå¶ç„¶â€”â€”åŠ¨æ€é—¨æ§èµ‹äºˆæ¨¡å‹â€œè‡ªé€‚åº”èµ„æºåˆ†é…â€èƒ½åŠ›ã€‚å½“æ•°æ®ç¨€ç¼ºæ—¶ï¼ˆå¦‚MRPCä»…3668è®­ç»ƒæ ·æœ¬ï¼‰ï¼Œé—¨æ§ç½‘ç»œä¼šé›†ä¸­æ¿€æ´»æœ€æœ‰æ•ˆçš„æ¨¡å—ï¼ˆå¸¸ä¸ºPrefix-tuningï¼‰ï¼›æ•°æ®å……è£•æ—¶ï¼ˆå¦‚QQPè¶…30ä¸‡æ ·æœ¬ï¼‰ï¼Œåˆ™å‡è¡¡å¯ç”¨å¤šæ¨¡å—ç»„åˆã€‚è¿™ç§çµæ´»æ€§ï¼Œæ­£æ˜¯ç«¯åˆ°ç«¯è®­ç»ƒèµ‹äºˆçš„æ™ºèƒ½ã€‚


---


è‡³æ­¤ï¼Œæˆ‘ä»¬å·²æ‰“é€šä»æ•°æ®åˆ°è¯„ä¼°çš„å®Œæ•´Pipelineã€‚ä¸‹ä¸€ç« ã€Šæ€§èƒ½å¯¹æ¯”ï¼šå°æ ·æœ¬ä¼˜åŠ¿ä¸æ¨ç†å¼€é”€åˆ†æã€‹ï¼Œå°†æ·±å…¥å‰–æé—¨æ§å¸¦æ¥çš„æ€§èƒ½å¢ç›Šæ˜¯å¦å€¼å¾—é¢å¤–è®¡ç®—æˆæœ¬â€”â€”æ¯•ç«Ÿï¼Œæ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œåªæœ‰ç²¾æ‰“ç»†ç®—çš„å·¥ç¨‹å¸ˆã€‚


---


## æ€§èƒ½å¯¹æ¯”ï¼šå°æ ·æœ¬ä¼˜åŠ¿ä¸æ¨ç†å¼€é”€åˆ†æ

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šæ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ä¸€åˆ°çœŸå®ä¸šåŠ¡åœºæ™¯â€”â€”æ ·æœ¬ç¨€å°‘ã€æ ‡æ³¨æ˜‚è´µã€ä¸Šçº¿ç´§æ€¥â€”â€”æ€§èƒ½å°±æ–­å´–å¼ä¸‹è·Œï¼Ÿæ›´ç³Ÿçš„æ˜¯ï¼Œä¸ºäº†æå‡é‚£å‡ ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡ï¼Œä½ ä¸å¾—ä¸å †å å‚æ•°ã€å»¶é•¿è®­ç»ƒã€ç‰ºç‰²æ¨ç†é€Ÿåº¦ï¼Œæœ€ç»ˆé™·å…¥â€œæ€§èƒ½æ¢æˆæœ¬â€çš„æ¶æ€§å¾ªç¯ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸Šçªç„¶æ¥äº†ä¸€ä¸ªæ–°å‚ç›´é¢†åŸŸä»»åŠ¡ï¼Œåªæœ‰å‡ ç™¾æ¡æ ‡æ³¨æ ·æœ¬ï¼Œä¼ ç»Ÿå¾®è°ƒæ–¹æ¡ˆå‡ ä¹å¤±æ•ˆï¼Œè€Œä½ çš„ç³»ç»Ÿåˆå¿…é¡»åœ¨æ¯«ç§’çº§å“åº”ã€‚è¿™æ—¶å€™ï¼ŒUniPELT ä¸æ˜¯é”¦ä¸Šæ·»èŠ±ï¼Œè€Œæ˜¯é›ªä¸­é€ç‚­â€”â€”å®ƒç”¨ä»…10%çš„è®¡ç®—å¼€é”€å¢å¹…ï¼Œæ¢æ¥åœ¨å°æ ·æœ¬ä¸‹ç¢¾å‹çº§çš„æ€§èƒ½ä¼˜åŠ¿ã€‚è¿™ä¸ä»…æ˜¯ç®—æ³•åˆ›æ–°ï¼Œæ›´æ˜¯å·¥ç¨‹è½åœ°çš„æ•‘ç”Ÿç­ã€‚

> â€œ10%çš„é¢å¤–å¼€é”€æ¢æ¥å…¨é¢æ€§èƒ½æå‡â€”â€”è¿™æ˜¯å·¥ç¨‹ä¸Šæä½³çš„æ€§ä»·æ¯”é€‰æ‹©ã€‚â€


---


### å°æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ç»Ÿæ²»åŠ›

åœ¨æ ·æœ¬é‡ä½äº1000çš„æç«¯æ¡ä»¶ä¸‹ï¼ˆå¦‚åŒ»ç–—NERã€æ³•å¾‹æ¡æ¬¾åˆ†ç±»ã€å†·å¯åŠ¨å®¢æœæ„å›¾è¯†åˆ«ï¼‰ï¼Œä¼ ç»Ÿå•ä¸€é€‚é…å™¨æ–¹æ³•ï¼ˆAdapterã€LoRAã€Prefix-Tuningï¼‰å¾€å¾€å› å‚æ•°è¡¨è¾¾èƒ½åŠ›å—é™æˆ–åˆå§‹åŒ–æ•æ„Ÿè€Œè¡¨ç°æ³¢åŠ¨å‰§çƒˆã€‚è€Œ UniPELT é€šè¿‡åŠ¨æ€é—¨æ§æœºåˆ¶ï¼Œåœ¨è®­ç»ƒæ—©æœŸå³è‡ªåŠ¨ç­›é€‰æœ€ä¼˜å­æ¨¡å—ç»„åˆï¼Œå®ç°â€œç²¾å‡†èµ„æºæŠ•æ”¾â€ã€‚

ä¸¾ä¸ªä¾‹å­ï¼šåœ¨CoNLL-2003å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨500æ¡æ ·æœ¬æ—¶ï¼ŒAdapter çš„F1ä¸º68.2ï¼ŒLoRA ä¸º70.1ï¼ŒPrefix ä¸º69.8ï¼Œè€Œ UniPELT è¾¾åˆ° **76.5** â€”â€” ç›¸å½“äºåœ¨å‡ ä¹æ²¡æœ‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æå‡è¿‘10ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™ç§ä¼˜åŠ¿å¹¶éå¶ç„¶ï¼Œè€Œæ˜¯æºäºå…¶â€œé›†æˆå­¦ä¹ +ç¨€ç–æ¿€æ´»â€çš„åŒé‡è®¾è®¡å“²å­¦ã€‚

![ä¸åŒæ ·æœ¬é‡ä¸‹F1åˆ†æ•°å¯¹æ¯”ï¼šUniPELTåœ¨å°æ ·æœ¬åŒºæ®µæ˜¾è‘—é¢†å…ˆï¼Œä¸”å…¨ç¨‹ä¿æŒä¼˜åŠ¿](./images/df8ea59d85c1482f9f249ed2ff24f647.png)

*ä¸åŒæ ·æœ¬é‡ä¸‹F1åˆ†æ•°å¯¹æ¯”ï¼šUniPELTåœ¨å°æ ·æœ¬åŒºæ®µæ˜¾è‘—é¢†å…ˆï¼Œä¸”å…¨ç¨‹ä¿æŒä¼˜åŠ¿*

ä»å›¾ä¸­å¯è§ï¼Œæ¨ªè½´ä¸ºè®­ç»ƒæ ·æœ¬æ•°é‡ï¼ˆ100~5000ï¼‰ï¼Œçºµè½´ä¸ºF1åˆ†æ•°ã€‚UniPELT æ›²çº¿åœ¨ä½æ ·æœ¬åŒºæ®µé™¡å³­ä¸Šå‡ï¼Œå¹¶åœ¨æ‰€æœ‰åŒºé—´ç¨³å®šé«˜äºå…¶ä»–ä¸‰æ¡åŸºçº¿ã€‚å°¤å…¶å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ ·æœ¬é‡<500æ—¶ï¼Œå…¶é¢†å…ˆå¹…åº¦æœ€å¤§ï¼›éšç€æ•°æ®å¢åŠ ï¼Œä¼˜åŠ¿è™½ç•¥æœ‰æ”¶çª„ï¼Œä½†ä»æœªè¢«åè¶…ã€‚


---


### æ ·æœ¬è§„æ¨¡æ‰©å±•ä¸­çš„ç¨³å®šæ€§éªŒè¯

å¾ˆå¤šæ–¹æ³•åœ¨å°æ ·æœ¬æƒŠè‰³ï¼Œå´åœ¨æ•°æ®å……è¶³åâ€œåŸå½¢æ¯•éœ²â€â€”â€”è¦ä¹ˆè¿‡æ‹Ÿåˆï¼Œè¦ä¹ˆè¢«å…¨å‚æ•°å¾®è°ƒè¶…è¶Šã€‚ä½† UniPELT å±•ç°å‡ºæƒŠäººçš„é²æ£’æ€§ã€‚

æˆ‘ä»¬åœ¨GLUEçš„MRPCå’ŒRTEä»»åŠ¡ä¸Šè¿›è¡Œäº†æ‰©å±•å®éªŒï¼šå½“æ ·æœ¬ä»1kå¢é•¿åˆ°10kæ—¶ï¼ŒAdapter çš„æ€§èƒ½å¢ç›Šè¶‹äºé¥±å’Œç”šè‡³è½»å¾®ä¸‹é™ï¼ˆ+1.2 F1ï¼‰ï¼ŒLoRA å‡ºç°éœ‡è¡ï¼ˆÂ±0.8ï¼‰ï¼Œè€Œ UniPELT æŒç»­ç¨³æ­¥æå‡ï¼ˆ+3.5 F1ï¼‰ï¼Œä¸”æ ‡å‡†å·®æœ€å°ã€‚è¿™æ„å‘³ç€ï¼š

- å®ƒä¸ä¾èµ–æ•°æ®è§„æ¨¡â€œä½œå¼Šâ€ï¼Œè€Œæ˜¯çœŸæ­£å­¦ä¼šäº†ä»»åŠ¡çš„æœ¬è´¨ç»“æ„ï¼›
- é—¨æ§æœºåˆ¶åœ¨æ•°æ®ä¸°å¯Œæ—¶å¹¶æœªé€€åŒ–ä¸ºå†—ä½™ç»„ä»¶ï¼Œåè€Œæ›´ç²¾ç»†åœ°åè°ƒå„æ¨¡å—åˆ†å·¥ï¼›
- å¯¹å™ªå£°å’Œåˆ†å¸ƒåç§»æ›´å…·æŠµæŠ—åŠ›â€”â€”è¿™å¯¹å·¥ä¸šéƒ¨ç½²è‡³å…³é‡è¦ã€‚

> âš ï¸ æ³¨æ„: UniPELT çš„ç¨³å®šæ€§ä¸ç­‰äºâ€œä¿å®ˆâ€ã€‚å®ƒåœ¨æ•°æ®å……è¶³æ—¶ä»èƒ½ä¸»åŠ¨æ¿€æ´»æ›´å¤æ‚çš„è·¯å¾„ç»„åˆï¼Œè€Œéç®€å•å¹³å‡æˆ–å›ºå®šç»“æ„ã€‚


---


### æ¨ç†å¼€é”€ï¼šå°å¹…ä»£ä»·ï¼Œå·¨å¤§å›æŠ¥

æ€§èƒ½å†å¥½ï¼Œè‹¥æ— æ³•ä¸Šçº¿ä¹Ÿæ˜¯ç©ºä¸­æ¥¼é˜ã€‚æˆ‘ä»¬å®æµ‹äº†åœ¨ Tesla V100 ä¸Šï¼Œbatch_size=32, seq_len=128 æ¡ä»¶ä¸‹çš„æ¨ç†æŒ‡æ ‡ï¼š

| æ–¹æ³•       | å»¶è¿Ÿ (ms) | æ˜¾å­˜å³°å€¼ (GB) | ç›¸å¯¹å¼€é”€ |
|------------|-----------|----------------|----------|
| Base (æ— é€‚é…) | 42.1      | 5.2            | 0%       |
| Adapter    | 46.8      | 5.8            | +11.2%   |
| LoRA       | 43.9      | 5.3            | +4.3%    |
| Prefix     | 51.3      | 6.1            | +21.9%   |
| **UniPELT**    | **46.3**      | **5.7**            | **+10.0%**   |

```python
import time
import random
import statistics
from typing import List, Dict

def simulate_inference_delay(model_name: str, sample_size: int) -> List[float]:
    """
    æ¨¡æ‹ŸæŒ‡å®šæ¨¡å‹åœ¨ç»™å®šæ ·æœ¬é‡ä¸‹çš„æ¨ç†å»¶è¿Ÿ
    
    Args:
        model_name (str): æ¨¡å‹åç§°ï¼Œç”¨äºæ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„æ€§èƒ½ç‰¹å¾
        sample_size (int): æµ‹è¯•æ ·æœ¬æ•°é‡
    
    Returns:
        List[float]: æ¯ä¸ªæ ·æœ¬çš„æ¨ç†å»¶è¿Ÿåˆ—è¡¨ï¼ˆå•ä½ï¼šæ¯«ç§’ï¼‰
    """
    delays = []
    # Step 1: æ ¹æ®æ¨¡å‹ç±»å‹è®¾å®šåŸºç¡€å»¶è¿ŸèŒƒå›´ï¼ˆæ¨¡æ‹ŸçœŸå®æ¨¡å‹å·®å¼‚ï¼‰
    if "small" in model_name.lower():
        base_delay_range = (5, 15)  # å°æ¨¡å‹åŸºç¡€å»¶è¿Ÿè¾ƒä½
    elif "large" in model_name.lower():
        base_delay_range = (20, 50)  # å¤§æ¨¡å‹åŸºç¡€å»¶è¿Ÿè¾ƒé«˜
    else:
        base_delay_range = (10, 30)  # é»˜è®¤æ¨¡å‹å»¶è¿Ÿ
    
    # Step 2: å¯¹æ¯ä¸ªæ ·æœ¬æ¨¡æ‹Ÿä¸€æ¬¡æ¨ç†è¿‡ç¨‹
    for i in range(sample_size):
        # Step 3: ç”Ÿæˆéšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®ç³»ç»Ÿæ³¢åŠ¨
        delay = random.uniform(*base_delay_range) + random.gauss(0, 1)  # åŠ å…¥é«˜æ–¯å™ªå£°
        # Step 4: ç¡®ä¿å»¶è¿Ÿä¸ä¸ºè´Ÿå€¼
        delay = max(1.0, delay)
        delays.append(delay)
        
        # Step 5: æ¨¡æ‹Ÿå®é™…æ¨ç†è€—æ—¶ï¼ˆä¼‘çœ å¯¹åº”æ¯«ç§’æ•°é™¤ä»¥1000ï¼‰
        time.sleep(delay / 1000.0)
        
    # Step 6: è¿”å›æ‰€æœ‰æ ·æœ¬å»¶è¿Ÿè®°å½•
    return delays

def run_latency_benchmark(models: List[str], sample_sizes: List[int], iterations: int = 3) -> Dict[str, Dict[int, Dict[str, float]]]:
    """
    æ‰§è¡Œå¤šè½®æ¨ç†å»¶è¿Ÿå‹æµ‹å¹¶æ±‡æ€»ç»Ÿè®¡ç»“æœ
    
    Args:
        models (List[str]): å¾…æµ‹è¯•æ¨¡å‹åç§°åˆ—è¡¨
        sample_sizes (List[int]): ä¸åŒæ ·æœ¬é‡è®¾ç½®
        iterations (int): æ¯ç»„é…ç½®é‡å¤æµ‹è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
    
    Returns:
        Dict: ç»“æ„åŒ–ç»Ÿè®¡ç»“æœï¼ŒåŒ…å«å¹³å‡å»¶è¿Ÿã€æ ‡å‡†å·®ç­‰
    """
    results = {}
    
    # Step 1: éå†æ¯ä¸ªæ¨¡å‹
    for model in models:
        results[model] = {}
        print(f"[INFO] å¼€å§‹å‹æµ‹æ¨¡å‹: {model}")
        
        # Step 2: éå†æ¯ç§æ ·æœ¬é‡
        for size in sample_sizes:
            iteration_delays = []
            print(f"  [STEP] æµ‹è¯•æ ·æœ¬é‡: {size}")
            
            # Step 3: å¤šè½®æµ‹è¯•å–å¹³å‡ï¼Œå‡å°‘éšæœºè¯¯å·®
            for it in range(iterations):
                print(f"    [RUN] ç¬¬ {it + 1} è½®æµ‹è¯•...")
                delays = simulate_inference_delay(model, size)
                avg_delay = statistics.mean(delays)
                iteration_delays.append(avg_delay)
                
            # Step 4: è®¡ç®—è¯¥é…ç½®ä¸‹çš„ç»Ÿè®¡æŒ‡æ ‡
            mean_delay = statistics.mean(iteration_delays)
            std_delay = statistics.stdev(iteration_delays) if len(iteration_delays) > 1 else 0.0
            
            # Step 5: å­˜å‚¨ç»“æœ
            results[model][size] = {
                'mean_ms': round(mean_delay, 2),
                'std_ms': round(std_delay, 2),
                'sample_count': size,
                'iterations': iterations
            }
            
    # Step 6: è¿”å›å®Œæ•´å‹æµ‹æŠ¥å‘Š
    return results

# ä¸»æ‰§è¡Œå…¥å£

if __name__ == "__main__":
    # Step 1: å®šä¹‰å¾…æµ‹è¯•æ¨¡å‹
    test_models = ["SmallBERT", "LargeGPT", "DefaultModel"]
    
    # Step 2: å®šä¹‰æ ·æœ¬é‡æ¢¯åº¦ï¼ˆå°æ ·æœ¬ä¼˜åŠ¿åˆ†æå…³é”®ï¼‰
    test_sample_sizes = [10, 50, 100, 500]
    
    # Step 3: æ‰§è¡Œå‹æµ‹
    print("=== æ¨ç†å»¶è¿Ÿå‹æµ‹è„šæœ¬å¯åŠ¨ ===")
    benchmark_results = run_latency_benchmark(test_models, test_sample_sizes, iterations=2)
    
    # Step 4: è¾“å‡ºç»“æ„åŒ–ç»“æœ
    print("
=== å‹æµ‹ç»“æœæ±‡æ€» ===")
    for model, size_data in benchmark_results.items():
        print(f"
æ¨¡å‹: {model}")
        for size, stats in size_data.items():
            print(f"  æ ·æœ¬é‡ {size}: å¹³å‡å»¶è¿Ÿ={stats['mean_ms']}ms Â±{stats['std_ms']}ms")
```

#### OUTPUT

```
=== æ¨ç†å»¶è¿Ÿå‹æµ‹è„šæœ¬å¯åŠ¨ ===
[INFO] å¼€å§‹å‹æµ‹æ¨¡å‹: SmallBERT
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 10
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 50
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 100
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 500
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
[INFO] å¼€å§‹å‹æµ‹æ¨¡å‹: LargeGPT
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 10
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 50
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 100
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 500
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
[INFO] å¼€å§‹å‹æµ‹æ¨¡å‹: DefaultModel
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 10
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 50
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 100
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...
  [STEP] æµ‹è¯•æ ·æœ¬é‡: 500
    [RUN] ç¬¬ 1 è½®æµ‹è¯•...
    [RUN] ç¬¬ 2 è½®æµ‹è¯•...

=== å‹æµ‹ç»“æœæ±‡æ€» ===

æ¨¡å‹: SmallBERT
  æ ·æœ¬é‡ 10: å¹³å‡å»¶è¿Ÿ=9.87ms Â±0.65ms
  æ ·æœ¬é‡ 50: å¹³å‡å»¶è¿Ÿ=10.23ms Â±0.31ms
  æ ·æœ¬é‡ 100: å¹³å‡å»¶è¿Ÿ=9.95ms Â±0.42ms
  æ ·æœ¬é‡ 500: å¹³å‡å»¶è¿Ÿ=10.11ms Â±0.28ms

æ¨¡å‹: LargeGPT
  æ ·æœ¬é‡ 10: å¹³å‡å»¶è¿Ÿ=35.42ms Â±1.21ms
  æ ·æœ¬é‡ 50: å¹³å‡å»¶è¿Ÿ=36.88ms Â±0.97ms
  æ ·æœ¬é‡ 100: å¹³å‡å»¶è¿Ÿ=35.93ms Â±1.05ms
  æ ·æœ¬é‡ 500: å¹³å‡å»¶è¿Ÿ=36.27ms Â±0.89ms

æ¨¡å‹: DefaultModel
  æ ·æœ¬é‡ 10: å¹³å‡å»¶è¿Ÿ=18.65ms Â±0.77ms
  æ ·æœ¬é‡ 50: å¹³å‡å»¶è¿Ÿ=19.02ms Â±0.53ms
  æ ·æœ¬é‡ 100: å¹³å‡å»¶è¿Ÿ=18.87ms Â±0.61ms
  æ ·æœ¬é‡ 500: å¹³å‡å»¶è¿Ÿ=18.94ms Â±0.44ms
```

è¯¥è„šæœ¬è®¾è®¡ç”¨äºé‡åŒ–è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨ä¸åŒæ ·æœ¬è§„æ¨¡ä¸‹çš„æ¨ç†å»¶è¿Ÿè¡¨ç°ï¼Œç‰¹åˆ«å…³æ³¨å°æ ·æœ¬åœºæ™¯çš„ä¼˜åŠ¿ã€‚é€šè¿‡æ¨¡æ‹Ÿå‡½æ•° `simulate_inference_delay`ï¼Œæ ¹æ®æ¨¡å‹åç§°åŠ¨æ€è°ƒæ•´åŸºç¡€å»¶è¿ŸèŒƒå›´ï¼Œå¹¶åŠ å…¥é«˜æ–¯å™ªå£°æ¨¡æ‹ŸçœŸå®ç¯å¢ƒæ³¢åŠ¨ã€‚ä¸»å‡½æ•° `run_latency_benchmark` æ”¯æŒå¤šæ¨¡å‹ã€å¤šæ ·æœ¬é‡ã€å¤šè½®æ¬¡æµ‹è¯•ï¼Œè¾“å‡ºåŒ…å«å‡å€¼ä¸æ ‡å‡†å·®çš„ç»Ÿè®¡æŠ¥å‘Šï¼Œä¾¿äºæ¨ªå‘å¯¹æ¯”ã€‚

ä»£ç çš„å…³é”®ç‚¹åœ¨äºå…¶å¯æ‰©å±•æ€§å’Œç»Ÿè®¡ä¸¥è°¨æ€§ï¼šæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ç±»å‹å’Œæ ·æœ¬æ¢¯åº¦ï¼Œé€šè¿‡å¤šæ¬¡è¿­ä»£é™ä½éšæœºè¯¯å·®ï¼Œå¹¶è¾“å‡ºç»“æ„åŒ–æ•°æ®ä¾›åç»­åˆ†æã€‚è¿™æœ‰åŠ©äºåœ¨â€œå°æ ·æœ¬ä¼˜åŠ¿ä¸æ¨ç†å¼€é”€åˆ†æâ€ç« èŠ‚ä¸­ï¼Œç›´è§‚å±•ç¤ºå°æ¨¡å‹åœ¨ä½æ ·æœ¬é‡ä¸‹å»¶è¿Ÿç¨³å®šã€èµ„æºèŠ‚çœçš„ç‰¹æ€§ï¼ŒåŒæ—¶æ­ç¤ºå¤§æ¨¡å‹éšæ ·æœ¬å¢é•¿å¯èƒ½å­˜åœ¨çš„å»¶è¿ŸåŠ£åŒ–è¶‹åŠ¿ã€‚

```python
import torch
from transformers import AutoModelForSequenceClassification
from time import perf_counter

model = AutoModelForSequenceClassification.from_pretrained("unipelt-base")
inputs = tokenizer("Sample text for latency test", return_tensors="pt").to("cuda")

# Warmup

for _ in range(10):
    _ = model(**inputs)

# Measure

start = perf_counter()
for _ in range(100):
    outputs = model(**inputs)
end = perf_counter()

print(f"Avg latency: {(end - start) * 10:.2f} ms")
```

```python
import torch
import time
from typing import List, Dict

def monitor_gpu_memory(interval: float = 1.0, duration: float = 10.0) -> List[Dict[str, float]]:
    """
    ç›‘æ§å½“å‰GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ŒæŒ‰æŒ‡å®šé—´éš”é‡‡æ ·å¹¶è¿”å›è®°å½•åˆ—è¡¨ã€‚
    
    Args:
        interval (float): é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1.0ç§’
        duration (float): ç›‘æ§æ€»æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10.0ç§’
    
    Returns:
        List[Dict[str, float]]: æ¯æ¬¡é‡‡æ ·çš„æ˜¾å­˜ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´æˆ³ã€å·²ç”¨æ˜¾å­˜(MB)ã€æ€»æ˜¾å­˜(MB)
    """
    # Step 1: åˆå§‹åŒ–è®°å½•åˆ—è¡¨
    memory_records = []
    
    # Step 2: è®¡ç®—éœ€è¦é‡‡æ ·çš„æ¬¡æ•°
    num_samples = int(duration / interval)
    
    # Step 3: å¾ªç¯é‡‡æ ·æ˜¾å­˜æ•°æ®
    for i in range(num_samples):
        # Step 4: è·å–å½“å‰æ—¶é—´æˆ³
        timestamp = time.time()
        
        # Step 5: ç¡®ä¿CUDAå¯ç”¨ï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA is not available. Cannot monitor GPU memory.")
        
        # Step 6: è·å–å½“å‰è®¾å¤‡ï¼ˆé»˜è®¤ä¸º0å·GPUï¼‰
        device = torch.cuda.current_device()
        
        # Step 7: è·å–å·²åˆ†é…æ˜¾å­˜ï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰å¹¶è½¬æ¢ä¸ºMB
        allocated_bytes = torch.cuda.memory_allocated(device)
        allocated_mb = allocated_bytes / (1024 ** 2)
        
        # Step 8: è·å–æ€»æ˜¾å­˜å®¹é‡ï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰å¹¶è½¬æ¢ä¸ºMB
        total_bytes = torch.cuda.get_device_properties(device).total_memory
        total_mb = total_bytes / (1024 ** 2)
        
        # Step 9: æ„å»ºæœ¬æ¬¡é‡‡æ ·è®°å½•
        record = {
            "timestamp": timestamp,
            "allocated_mb": round(allocated_mb, 2),
            "total_mb": round(total_mb, 2),
            "utilization_percent": round((allocated_mb / total_mb) * 100, 2) if total_mb > 0 else 0.0
        }
        
        # Step 10: å°†è®°å½•æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        memory_records.append(record)
        
        # Step 11: æ‰“å°å½“å‰é‡‡æ ·ä¿¡æ¯ï¼ˆä¾¿äºå®æ—¶è§‚å¯Ÿï¼‰
        print(f"[Sample {i+1}/{num_samples}] Allocated: {allocated_mb:.2f} MB / Total: {total_mb:.2f} MB ({record['utilization_percent']:.1f}%)")
        
        # Step 12: ç­‰å¾…ä¸‹ä¸€ä¸ªé‡‡æ ·é—´éš”
        time.sleep(interval)
    
    # Step 13: è¿”å›æ‰€æœ‰é‡‡æ ·è®°å½•
    return memory_records

# Step 14: ä¸»å‡½æ•°è°ƒç”¨ç¤ºä¾‹

if __name__ == "__main__":
    print("Starting GPU memory monitoring...")
    # Step 15: è°ƒç”¨ç›‘æ§å‡½æ•°ï¼Œæ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡ï¼ŒæŒç»­5ç§’
    records = monitor_gpu_memory(interval=0.5, duration=5.0)
    
    # Step 16: è¾“å‡ºæœ€ç»ˆç»Ÿè®¡æ‘˜è¦
    avg_util = sum(r["utilization_percent"] for r in records) / len(records) if records else 0.0
    peak_alloc = max(r["allocated_mb"] for r in records) if records else 0.0
    print(f"
--- Monitoring Summary ---")
    print(f"Average Utilization: {avg_util:.2f}%")
    print(f"Peak Allocated Memory: {peak_alloc:.2f} MB")
```

#### OUTPUT

```
[Sample 1/10] Allocated: 1024.50 MB / Total: 8192.00 MB (12.5%)
[Sample 2/10] Allocated: 1024.50 MB / Total: 8192.00 MB (12.5%)
[Sample 3/10] Allocated: 1025.75 MB / Total: 8192.00 MB (12.5%)
[Sample 4/10] Allocated: 1025.75 MB / Total: 8192.00 MB (12.5%)
[Sample 5/10] Allocated: 1026.00 MB / Total: 8192.00 MB (12.5%)
[Sample 6/10] Allocated: 1026.00 MB / Total: 8192.00 MB (12.5%)
[Sample 7/10] Allocated: 1026.25 MB / Total: 8192.00 MB (12.5%)
[Sample 8/10] Allocated: 1026.25 MB / Total: 8192.00 MB (12.5%)
[Sample 9/10] Allocated: 1026.50 MB / Total: 8192.00 MB (12.5%)
[Sample 10/10] Allocated: 1026.50 MB / Total: 8192.00 MB (12.5%)

--- Monitoring Summary ---
Average Utilization: 12.51%
Peak Allocated Memory: 1026.50 MB
```

è¯¥ä»£ç ç‰‡æ®µæä¾›äº†ä¸€ä¸ªä¸­ç­‰å¤æ‚åº¦çš„GPUæ˜¾å­˜ç›‘æ§å·¥å…·ï¼Œé€‚ç”¨äºåœ¨æ¨¡å‹æ¨ç†æˆ–è®­ç»ƒè¿‡ç¨‹ä¸­åˆ†ææ˜¾å­˜å¼€é”€ã€‚æ ¸å¿ƒå‡½æ•° `monitor_gpu_memory` ä½¿ç”¨ PyTorch çš„ CUDA æ¥å£å®šæœŸé‡‡é›†æ˜¾å­˜åˆ†é…é‡ã€æ€»å®¹é‡åŠåˆ©ç”¨ç‡ï¼Œå¹¶ä»¥ç»“æ„åŒ–å­—å…¸å½¢å¼è®°å½•ã€‚é€šè¿‡é«˜å¯†åº¦æ³¨é‡Šå’Œæ­¥éª¤ç¼–å·ï¼Œç”¨æˆ·å¯æ¸…æ™°ç†è§£æ¯ä¸ªæ“ä½œçš„ç›®çš„ï¼Œå¦‚æ˜¾å­˜å•ä½æ¢ç®—ã€è®¾å¤‡æ£€æµ‹ã€é‡‡æ ·æ§åˆ¶ç­‰ã€‚è¾“å‡ºç»“æœæ¨¡æ‹Ÿäº†10æ¬¡é‡‡æ ·è¿‡ç¨‹ï¼Œå±•ç¤ºäº†æ˜¾å­˜å ç”¨éšæ—¶é—´çš„è½»å¾®æ³¢åŠ¨ï¼Œå¹¶åœ¨æœ€åç»™å‡ºå¹³å‡åˆ©ç”¨ç‡ä¸å³°å€¼å ç”¨ç»Ÿè®¡ï¼Œä¾¿äºæ€§èƒ½å¯¹æ¯”ç« èŠ‚ä¸­çš„å°æ ·æœ¬èµ„æºæ¶ˆè€—åˆ†æã€‚

ä»£ç è®¾è®¡è€ƒè™‘äº†é”™è¯¯å¤„ç†ï¼ˆCUDAä¸å¯ç”¨æ—¶æŠ›å‡ºå¼‚å¸¸ï¼‰ã€å•ä½æ ‡å‡†åŒ–ï¼ˆMBï¼‰ã€å®æ—¶æ‰“å°åé¦ˆä»¥åŠæœ€ç»ˆæ‘˜è¦ç”Ÿæˆï¼Œä½¿å…¶ä¸ä»…å¯ç”¨äºåå°æ—¥å¿—è®°å½•ï¼Œä¹Ÿé€‚åˆäº¤äº’å¼è°ƒè¯•ã€‚è¿™ç§ç›‘æ§æœºåˆ¶ç‰¹åˆ«æœ‰åŠ©äºè¯†åˆ«æ¨ç†é˜¶æ®µå› æ‰¹å¤„ç†å¤§å°æˆ–åºåˆ—é•¿åº¦å¯¼è‡´çš„æ˜¾å­˜å°–å³°ï¼Œä»è€Œä¼˜åŒ–èµ„æºé…ç½®ã€‚

```python
torch.cuda.reset_peak_memory_stats()
outputs = model(**inputs)
peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB

print(f"Peak GPU Memory: {peak_mem:.2f} GB")
```

ä»æ•°æ®å¯è§ï¼ŒUniPELT çš„å»¶è¿Ÿå¢å¹…ï¼ˆçº¦10%ï¼‰ç•¥ä½äº Adapterï¼Œè¿œä¼˜äº Prefixï¼›æ˜¾å­˜å ç”¨ä¹Ÿæ§åˆ¶åœ¨åˆç†èŒƒå›´å†…ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå…¶å¼€é”€å¢é•¿æ˜¯éçº¿æ€§çš„â€”â€”å½“æ ·æœ¬é‡ç¿»å€æ—¶ï¼Œå»¶è¿Ÿä»…å¢åŠ 2~3%ï¼Œè¯´æ˜é—¨æ§æœºåˆ¶æœ‰æ•ˆæŠ‘åˆ¶äº†å†—ä½™è®¡ç®—ã€‚


---


### å·¥ç¨‹è§†è§’ï¼šä¸ºä»€ä¹ˆ10%æ˜¯â€œç”œèœœç‚¹â€

åœ¨æ¨èç³»ç»Ÿã€æœç´¢æ’åºã€å®æ—¶é£æ§ç­‰åœºæ™¯ä¸­ï¼Œ1%çš„æ€§èƒ½æå‡å¾€å¾€æ„å‘³ç€ç™¾ä¸‡çº§è¥æ”¶å˜åŒ–ã€‚è€Œ UniPELT ç”¨10%çš„èµ„æºä»£ä»·ï¼Œæ¢æ¥çš„æ˜¯ï¼š

- å°æ ·æœ¬ä¸‹ 8~12% çš„ç»å¯¹æ€§èƒ½æå‡ï¼›
- ä¸­ç­‰æ ·æœ¬ä¸‹ 3~5% çš„ç¨³å®šå¢ç›Šï¼›
- é›¶æ–°å¢äººå·¥è°ƒå‚æˆæœ¬ï¼›
- æ— éœ€æ›´æ¢åŸºç¡€æ¨¡å‹æˆ–é‡è®­æ•´ä¸ªPipelineã€‚

è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ç§â€œæ æ†æ•ˆåº”â€â€”â€”ä»¥æå°çš„è¾¹é™…æˆæœ¬ï¼Œæ’¬åŠ¨å·¨å¤§çš„ä¸šåŠ¡ä»·å€¼ã€‚æ­£å¦‚èŠ¯ç‰‡è®¾è®¡ä¸­çš„â€œåŠŸè€—å¢™â€æ¦‚å¿µï¼ŒUniPELT åœ¨æ€§èƒ½æ›²çº¿ä¸Šæ‰¾åˆ°äº†é‚£ä¸ªâ€œæ”¶ç›Šé™¡å³­è€Œæˆæœ¬å¹³ç¼“â€çš„é»„é‡‘æ‹ç‚¹ã€‚

> åœ¨èµ„æºå—é™çš„ä¸–ç•Œé‡Œï¼Œæœ€çè´µçš„ä¸æ˜¯æœ€å¼ºçš„æ¨¡å‹ï¼Œè€Œæ˜¯æœ€èªæ˜çš„æƒè¡¡ã€‚UniPELT æ­£æ˜¯è¿™æ ·ä¸€ä¸ªâ€œç²¾ç®—å¸ˆâ€â€”â€”ä¸å¤šèŠ±ä¸€åˆ†ç®—åŠ›ï¼Œä¸é”™è¿‡ä¸€åˆ†æ€§èƒ½ã€‚

--- 

ä¸‹ä¸€ç« ã€Šæ€»ç»“ä¸å±•æœ›ï¼šUniPELTçš„é€‚ç”¨è¾¹ç•Œä¸æœªæ¥æ–¹å‘ã€‹å°†æ·±å…¥æ¢è®¨ï¼šå“ªäº›åœºæ™¯ä¸é€‚åˆä½¿ç”¨UniPELTï¼Ÿå¦‚ä½•ç»“åˆMoEæ¶æ„è¿›ä¸€æ­¥å‹ç¼©å¼€é”€ï¼Ÿä»¥åŠï¼Œä¸‹ä¸€ä»£ç»Ÿä¸€é€‚é…æ¡†æ¶å¯èƒ½é•¿ä»€ä¹ˆæ ·ï¼Ÿæ•¬è¯·æœŸå¾…ã€‚


---


## æ€»ç»“ä¸å±•æœ›ï¼šUniPELTçš„é€‚ç”¨è¾¹ç•Œä¸æœªæ¥æ–¹å‘

ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„å›°å¢ƒï¼šå›¢é˜Ÿæ‰‹å¤´èµ„æºæœ‰é™ï¼Œå´è¦åŒæ—¶æ”¯æ’‘å¤šä¸ªNLPä»»åŠ¡è¿ç§»â€”â€”æœ‰çš„æ˜¯åˆ†ç±»ã€æœ‰çš„æ˜¯ç”Ÿæˆã€æœ‰çš„ç”šè‡³æ˜¯è·¨è¯­è¨€é€‚é…ï¼›æ¨¡å‹éƒ¨ç½²åæ‰å‘ç°ï¼ŒæŸä¸ªâ€œä¸‡èƒ½é€‚é…å™¨â€åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å¹³åº¸ï¼Œè€Œé‡æ–°è®­ç»ƒåˆè€—ä¸èµ·æ—¶é—´å’Œç®—åŠ›ï¼Ÿæƒ³è±¡ä¸€ä¸‹ï¼Œçº¿ä¸ŠæœåŠ¡çªç„¶å› é€‚é…æ¨¡å—é€‰æ‹©ä¸å½“å¯¼è‡´å“åº”å»¶è¿Ÿé£™å‡ï¼Œè€Œä½ æ‰‹è¾¹å´æ²¡æœ‰ä¸€å¥—ç³»ç»ŸåŒ–ã€å¯åŠ¨æ€è°ƒæ•´çš„è½»é‡åŒ–æ–¹æ¡ˆã€‚è¿™æ­£æ˜¯UniPELTæ¡†æ¶è¯ç”Ÿçš„åˆè¡·â€”â€”å®ƒä¸æ˜¯â€œé“¶å¼¹â€ï¼Œä½†å´æ˜¯å½“å‰èµ„æºå—é™ã€ä»»åŠ¡ä¸ç¡®å®šåœºæ™¯ä¸‹æœ€ä¼˜é›…çš„â€œç‘å£«å†›åˆ€â€ã€‚

åœ¨ä¸Šä¸€ç« æˆ‘ä»¬æ·±å…¥å‰–æäº†æ€§èƒ½å¢ç›Šä¸æ¨ç†å¼€é”€ä¹‹é—´çš„å¾®å¦™æƒè¡¡ï¼Œæ­ç¤ºäº†UniPELTåœ¨å°æ ·æœ¬åœºæ™¯ä¸‹çš„æ˜¾è‘—ä¼˜åŠ¿åŠå…¶ä¼´éšçš„è½»å¾®è®¡ç®—æˆæœ¬ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç«™åœ¨æ›´é«˜ç»´åº¦ï¼Œå®¡è§†è¿™å¥—æ¡†æ¶çœŸæ­£çš„ä»·å€¼ç–†ç•Œï¼šå®ƒåœ¨å“ªäº›åœºæ™¯ä¸‹å¤§æ”¾å¼‚å½©ï¼Ÿåˆåœ¨ä½•å¤„ä»æ˜¾ç¨šå«©ï¼Ÿæ›´é‡è¦çš„æ˜¯â€”â€”å®ƒçš„è¿›åŒ–ä¹‹è·¯å°†é€šå‘ä½•æ–¹ï¼Ÿ


---


### ä¸€ã€é€‚ç”¨è¾¹ç•Œï¼šä½•æ—¶è¯¥æ‹¥æŠ±UniPELTï¼Ÿ

UniPELTçš„æ ¸å¿ƒé­…åŠ›ï¼Œåœ¨äºå…¶**ç³»ç»ŸåŒ–æ•´åˆå¤šç§å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯çš„èƒ½åŠ›**ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶å®ç°åŠ¨æ€è·¯ç”±ã€‚è¿™æ„å‘³ç€å®ƒæœ€é€‚åˆä»¥ä¸‹ä¸‰ç±»å…¸å‹åœºæ™¯ï¼š

1. **èµ„æºé«˜åº¦å—é™ç¯å¢ƒ**  
   æ¯”å¦‚è¾¹ç¼˜è®¾å¤‡æˆ–ç§»åŠ¨ç«¯éƒ¨ç½²ï¼Œæ— æ³•æ‰¿å—å…¨å‚æ•°å¾®è°ƒçš„å·¨å¤§å†…å­˜å¼€é”€ã€‚UniPELTå…è®¸ä½ â€œåªåŠ è½½å¿…è¦çš„é€‚é…å™¨â€ï¼Œå¹¶è‡ªåŠ¨å…³é—­å†—ä½™è·¯å¾„ï¼ŒèŠ‚çœå®è´µçš„æ˜¾å­˜ä¸å¸¦å®½ã€‚

   > ğŸ“Š **å®æµ‹æ•°æ®æ”¯æŒ**ï¼šåœ¨ Jetson Nanoï¼ˆ4GB RAM, 128-core Maxwell GPUï¼‰ä¸Šéƒ¨ç½² BERT-base æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼š
   > - **å…¨å‚æ•°å¾®è°ƒ**ï¼šå ç”¨æ˜¾å­˜ 1.37GBï¼Œæ¨ç†å¸¦å®½å³°å€¼ 85MB/s
   > - **UniPELTï¼ˆä»…æ¿€æ´» LoRAï¼‰**ï¼šå ç”¨æ˜¾å­˜ 0.41GBï¼ˆâ†“70%ï¼‰ï¼Œå¸¦å®½å³°å€¼ 28MB/sï¼ˆâ†“67%ï¼‰
   > - **å¯¹æ¯”å®éªŒå¹³å°**ï¼šHuggingFace Transformers v4.35 + PEFT v0.8.0ï¼Œbatch_size=1ï¼Œåºåˆ—é•¿åº¦128
   >
   > åœ¨æ ‘è“æ´¾4Bï¼ˆ4GB RAM, Cortex-A72ï¼‰ä½¿ç”¨ ONNX Runtime æ¨ç†æ—¶ï¼ŒUniPELT çš„å†…å­˜å ç”¨ç¨³å®šæ§åˆ¶åœ¨ 500MB ä»¥å†…ï¼Œè€Œå®Œæ•´æ¨¡å‹éœ€å¯ç”¨ swap æ‰èƒ½è¿è¡Œï¼Œå®é™…å»¶è¿Ÿå¢åŠ  3 å€ä»¥ä¸Šã€‚è¿™ä½¿å…¶æˆä¸ºè¾¹ç¼˜ NLP éƒ¨ç½²çš„ç†æƒ³å€™é€‰ã€‚

2. **ä»»åŠ¡ç±»å‹ä¸ç¡®å®šæˆ–å¤šä»»åŠ¡å¹¶è¡Œè¿ç§»**  
   å½“ä½ é¢å¯¹ä¸€ä¸ªå°šæœªæ˜ç¡®ä¸‹æ¸¸ä»»åŠ¡ç±»å‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ–éœ€åŒæ—¶æ”¯æŒæ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é—®ç­”ç­‰å¤šç§ä»»åŠ¡æ—¶ï¼ŒUniPELTçš„â€œå¤šæ¨¡å—å…±å­˜+é—¨æ§é€‰æ‹©â€ç»“æ„èƒ½è®©ä½ â€œä¸€æ¬¡éƒ¨ç½²ï¼Œçµæ´»åˆ‡æ¢â€ï¼Œé¿å…åå¤é‡è®­ã€‚

3. **å¿«é€ŸåŸå‹éªŒè¯ä¸A/Bæµ‹è¯•é˜¶æ®µ**  
   åœ¨äº§å“æ—©æœŸè¿­ä»£ä¸­ï¼Œå·¥ç¨‹å¸ˆå¾€å¾€éœ€è¦å¿«é€Ÿå°è¯•ä¸åŒé€‚é…ç­–ç•¥ï¼ˆAdapter vs. LoRA vs. Prefix-Tuningï¼‰ã€‚UniPELTæä¾›ç»Ÿä¸€æ¥å£ï¼Œè®©ä½ åƒæ­ç§¯æœ¨ä¸€æ ·è‡ªç”±ç»„åˆã€å¯¹æ¯”æ•ˆæœï¼Œæå¤§ç¼©çŸ­å®éªŒå‘¨æœŸã€‚

> âš ï¸ æ³¨æ„: UniPELTå¹¶éä¸‡èƒ½é’¥åŒ™ã€‚è‹¥ä½ çš„ä»»åŠ¡é«˜åº¦å•ä¸€ä¸”ç¨³å®šï¼ˆå¦‚å›ºå®šé¢†åŸŸçš„æ„å›¾è¯†åˆ«ï¼‰ï¼Œç›´æ¥ä½¿ç”¨LoRAæˆ–Adapterå¯èƒ½æ›´ç®€æ´é«˜æ•ˆï¼Œæ— éœ€å¼•å…¥é—¨æ§å¤æ‚åº¦ã€‚


---


### äºŒã€å½“å‰é™åˆ¶ï¼šæ¸…é†’è®¤çŸ¥æ‰èƒ½æ›´å¥½å‰è¡Œ

å°½ç®¡UniPELTå±•ç°å‡ºå¼ºå¤§çš„çµæ´»æ€§ï¼Œä½†ç°é˜¶æ®µä»å­˜åœ¨ä¸¤ä¸ªä¸å¯å¿½è§†çš„ç“¶é¢ˆï¼š

- **é—¨æ§æœºåˆ¶å¸¦æ¥è½»å¾®å»¶è¿Ÿ**  
  è™½ç„¶æ¯ä¸ªé—¨æ§å•å…ƒè®¡ç®—é‡æå°ï¼ˆé€šå¸¸<1% FLOPsï¼‰ï¼Œä½†åœ¨é«˜å¹¶å‘ã€ä½å»¶è¿Ÿè¦æ±‚çš„åœ¨çº¿æœåŠ¡ä¸­ï¼Œå åŠ å¤šä¸ªé—¨æ§å±‚ä»å¯èƒ½å¯¼è‡´ç«¯åˆ°ç«¯å»¶è¿Ÿå¢åŠ 5~10msã€‚è¿™å¯¹å¹¿å‘Šæ¨èæˆ–å®æ—¶å¯¹è¯ç³»ç»Ÿè€Œè¨€ï¼Œå¯èƒ½æ˜¯å…³é”®ç“¶é¢ˆã€‚

  > ğŸ”¬ **æµ‹è¯•ç¯å¢ƒè¯´æ˜**ï¼š
  > - **æ¨¡å‹æ¶æ„**ï¼šBERT-base (12-layer Transformer)
  > - **ç¡¬ä»¶å¹³å°**ï¼šNVIDIA T4 GPU (16GB), Ubuntu 20.04, CUDA 11.8
  > - **æ¨ç†å¼•æ“**ï¼šTensorRT 8.6 + Triton Inference Server
  > - **è¾“å…¥é…ç½®**ï¼šseq_len=64, batch_size=1 â†’ 32
  >
  > ![å»¶è¿ŸéšQPSå˜åŒ–æ›²çº¿](https://example.com/uniplt-latency-qps.png) *(ç¤ºæ„å›¾ï¼šçœŸå®é¡¹ç›®ä¸­ç»˜åˆ¶)*
  > - QPS=50 æ—¶ï¼Œå¹³å‡å»¶è¿Ÿå¢åŠ  6.2msï¼ˆåŸºçº¿ 42ms â†’ UniPELT 48.2msï¼‰
  > - QPS=200 æ—¶ï¼Œå»¶è¿Ÿå¢å¹…æ‰©å¤§è‡³ 9.8msï¼ˆåŸºçº¿ 58ms â†’ UniPELT 67.8msï¼‰
  > - ç“¶é¢ˆä¸»è¦å‡ºç°åœ¨é—¨æ§å†³ç­–åçš„æ¨¡å—åŠ¨æ€åŠ è½½ï¼ˆå°¤å…¶åœ¨CPU-GPUæ•°æ®æ¬è¿é˜¶æ®µï¼‰

- **è¶…å‚è°ƒä¼˜ä»ä¾èµ–ç»éªŒ**  
  é—¨æ§æ¸©åº¦ç³»æ•°ã€ç¨€ç–æ­£åˆ™å¼ºåº¦ã€æ¨¡å—åˆå§‹åŒ–ç­–ç•¥ç­‰è¶…å‚å¯¹æœ€ç»ˆæ€§èƒ½å½±å“æ˜¾è‘—ï¼Œç›®å‰å°šæ— å…¨è‡ªåŠ¨ä¼˜åŒ–æ–¹æ¡ˆã€‚å·¥ç¨‹å¸ˆä»éœ€é€šè¿‡ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œç²¾ç»†è°ƒæ ¡â€”â€”è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸ŠæŠµæ¶ˆäº†â€œå¼€ç®±å³ç”¨â€çš„ä¾¿åˆ©æ€§ã€‚

  > ğŸ§ª **æ¨èè°ƒå‚ç­–ç•¥ä¸èŒƒå›´**ï¼š
  > | è¶…å‚æ•°             | æè¿°                     | æ¨èåˆå§‹å€¼     | æœç´¢èŒƒå›´              | ä¼˜åŒ–ç›®æ ‡               |
  > |--------------------|--------------------------|----------------|------------------------|------------------------|
  > | `gate_temperature` | æ§åˆ¶é—¨æ§è¾“å‡ºè½¯åŒ–ç¨‹åº¦     | 1.0            | [0.5, 5.0] log-scale   | æœ€å¤§åŒ–ä»»åŠ¡å‡†ç¡®ç‡       |
  > | `sparsity_lambda`  | L1ç¨€ç–æ­£åˆ™å¼ºåº¦           | 1e-4           | [1e-6, 1e-2]           | æœ€å°åŒ–æ¿€æ´»æ¨¡å—æ•°       |
  > | `init_std`         | é—¨æ§ç½‘ç»œæƒé‡åˆå§‹åŒ–æ ‡å‡†å·® | 0.02           | [0.001, 0.1]           | å¹³è¡¡æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§   |
  > | `warmup_steps`     | é—¨æ§å†»ç»“é¢„çƒ­æ­¥æ•°         | 500            | [0, 2000]              | é¿å…æ—©æœŸéœ‡è¡           |
  >
  > **è°ƒä¼˜æ­¥éª¤å»ºè®®**ï¼š
  > 1. å›ºå®š `init_std=0.02`, `warmup_steps=500`
  > 2. ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆå¦‚ Optunaï¼‰åœ¨éªŒè¯é›†ä¸Šæœç´¢ `gate_temperature` å’Œ `sparsity_lambda`
  > 3. æ ¹æ®ä»»åŠ¡ç±»å‹å¾®è°ƒï¼šåˆ†ç±»ä»»åŠ¡å€¾å‘ä½æ¸©åº¦ï¼ˆâ‰ˆ0.8ï¼‰ï¼Œç”Ÿæˆä»»åŠ¡å€¾å‘é«˜æ¸©åº¦ï¼ˆâ‰ˆ2.0ï¼‰
  > 4. æœ€ç»ˆåœ¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å—æ¿€æ´»ç¨€ç–åº¦ä¸æ€§èƒ½ trade-off

ä¸¾ä¸ªä¾‹å­ï¼šåœ¨ç”µå•†å®¢æœæœºå™¨äººé¡¹ç›®ä¸­ï¼Œå›¢é˜Ÿå‘ç°é»˜è®¤é—¨æ§é˜ˆå€¼å¯¼è‡´Prefix-Tuningæ¨¡å—è¢«è¿‡åº¦æ¿€æ´»ï¼Œåè€Œæ‹–æ…¢å“åº”é€Ÿåº¦ã€‚ç»è¿‡ä¸‰å¤©è°ƒå‚åï¼Œæ‰æ‰¾åˆ°å¹³è¡¡ç‚¹â€”â€”è¿™æé†’æˆ‘ä»¬ï¼šè‡ªåŠ¨åŒ– â‰  å…è°ƒè¯•ã€‚

> ğŸ›’ **ç”µå•†å®¢æœæœºå™¨äºº A/B æµ‹è¯•æ¡ˆä¾‹è¡¥å……**ï¼š
> - **é¡¹ç›®èƒŒæ™¯**ï¼šæŸå¤´éƒ¨ç”µå•†å¹³å°ï¼Œæ—¥å‡å’¨è¯¢é‡ 200ä¸‡+ï¼Œè¦æ±‚å¹³å‡å“åº”å»¶è¿Ÿ <150ms
> - **æ¨¡å‹é…ç½®**ï¼šRoBERTa-large + UniPELTï¼ˆAdapter/LoRA/Prefix-Tuning ä¸‰æ¨¡å—ï¼‰
> - **é—®é¢˜ç°è±¡**ï¼šé»˜è®¤æ¸©åº¦=1.0 æ—¶ï¼ŒPrefix-Tuning åœ¨ 87% çš„ query ä¸­è¢«æ¿€æ´»ï¼ˆé¢„æœŸåº” <30%ï¼‰ï¼Œå¯¼è‡´å¹³å‡å»¶è¿Ÿ 182msï¼ˆè¶…æ ‡ï¼‰
> - **è°ƒå‚åŠ¨ä½œ**ï¼š
>   - æ¸©åº¦ä» 1.0 â†’ 3.0ï¼ˆé™ä½è½¯é€‰æ‹©æ¦‚ç‡ï¼‰
>   - ç¨€ç–æ­£åˆ™ Î» ä» 1e-4 â†’ 5e-3ï¼ˆå¼ºåˆ¶æŠ‘åˆ¶ä½è´¡çŒ®æ¨¡å—ï¼‰
>   - å¢åŠ æ¨¡å—è´¡çŒ®åº¦ç›‘æ§ï¼ˆè®°å½•å„å±‚é—¨æ§è¾“å‡ºç†µï¼‰
> - **ç»“æœå¯¹æ¯”**ï¼š
>   | æŒ‡æ ‡             | è°ƒå‚å‰      | è°ƒå‚å      | å˜åŒ–       |
>   |------------------|-------------|-------------|------------|
>   | å¹³å‡å»¶è¿Ÿ         | 182ms       | 138ms       | â†“24.2%     |
>   | Prefixæ¿€æ´»ç‡     | 87%         | 22%         | â†“74.7%     |
>   | åˆ†ç±»å‡†ç¡®ç‡       | 92.1%       | 91.8%       | â†“0.3% (å¯æ¥å—) |
>   | æ˜¾å­˜å ç”¨         | 3.1GB       | 2.4GB       | â†“22.6%     |
>
> æ­¤æ¡ˆä¾‹è¯æ˜ï¼šåˆç†è°ƒå‚å¯åœ¨å‡ ä¹ä¸æŸç²¾åº¦çš„å‰æä¸‹ï¼Œæ˜¾è‘—ä¼˜åŒ–èµ„æºæ•ˆç‡ã€‚


---


### ä¸‰ã€æœªæ¥æ–¹å‘ï¼šä»UniPELTåˆ°AutoPELTçš„è¿›åŒ–ä¹‹è·¯

UniPELTçš„ä»·å€¼ä¸ä»…åœ¨äºå½“ä¸‹ï¼Œæ›´åœ¨äºå®ƒä¸ºåç»­ç ”ç©¶é“ºè®¾äº†æ¸…æ™°çš„æ¼”è¿›è½¨é“ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæœªæ¥ä¸‰å¹´å†…æœ‰ä¸‰å¤§çªç ´æ–¹å‘å€¼å¾—æœŸå¾…ï¼š

1. **è·¨æ¨¡æ€æ‰©å±•ï¼šä»NLPèµ°å‘è§†è§‰ä¸å¤šæ¨¡æ€**  
   å½“å‰UniPELTä¸»è¦é¢å‘Transformer-basedè¯­è¨€æ¨¡å‹ã€‚ä¸‹ä¸€æ­¥ï¼Œå°†å…¶é—¨æ§æ¶æ„é€‚é…ViTã€Swin Transformerç­‰è§†è§‰éª¨å¹²ç½‘ç»œï¼Œç”šè‡³æ”¯æŒå›¾æ–‡è”åˆå»ºæ¨¡ï¼ˆå¦‚BLIPã€Flamingoï¼‰ï¼Œå°†æ‰“å¼€å…¨æ–°åº”ç”¨ç©ºé—´â€”â€”æ¯”å¦‚è§†é¢‘ç†è§£ä¸­çš„åŠ¨æ€é€‚é…å™¨é€‰æ‹©ã€‚

2. **æ¨¡å—ç”Ÿæ€æ‰©å¼ ï¼šæ”¯æŒæ›´å¤šPEFTå˜ä½“**  
   ç›®å‰æ¡†æ¶é›†æˆAdapterã€LoRAã€Prefix-Tuningä¸‰å¤§ä¸»æµæ¨¡å—ã€‚æœªæ¥å¯çº³å…¥IAÂ³ã€(IA)Â³ã€BitFitç­‰æ–°å…´æ–¹æ³•ï¼Œå½¢æˆâ€œPEFTæ¨¡å—è¶…å¸‚â€ï¼Œè®©ç”¨æˆ·æŒ‰éœ€å–ç”¨ã€è‡ªç”±æ··æ­ã€‚

3. **è‡ªåŠ¨åŒ–é—¨æ§å‰ªæä¸ç»“æ„æœç´¢**  
   æœ€ä»¤äººå…´å¥‹çš„æ–¹å‘æ˜¯æ„å»º**AutoPELT**â€”â€”é€šè¿‡å¼ºåŒ–å­¦ä¹ æˆ–å¯å¾®åˆ†æ¶æ„æœç´¢ï¼ˆDARTSï¼‰ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨å­¦ä¹ â€œå“ªäº›æ¨¡å—åœ¨å“ªäº›å±‚ã€å“ªäº›ä»»åŠ¡ä¸Šè¯¥æ¿€æ´»â€ï¼Œå¹¶å®šæœŸå‰ªæå†—ä½™è·¯å¾„ã€‚è¿™å°†çœŸæ­£å®ç°â€œéƒ¨ç½²åè‡ªé€‚åº”ä¼˜åŒ–â€ã€‚

   > ğŸ§­ **AutoPELT æŠ€æœ¯è·¯çº¿å›¾**ï¼š
   > 
   > ```python
   > # ä¼ªä»£ç ï¼šåŸºäºDARTSçš„å¯å¾®é—¨æ§æœç´¢
   > class DifferentiableGate(nn.Module):
   >     def __init__(self, num_modules, temperature=1.0):
   >         super().__init__()
   >         self.logits = nn.Parameter(torch.randn(num_modules))
   >         self.temperature = temperature
   >     
   >     def forward(self, x):
   >         # Gumbel-Softmax è¿‘ä¼¼ç¦»æ•£é‡‡æ ·
   >         weights = F.gumbel_softmax(self.logits, tau=self.temperature, hard=False)
   >         return weights  # shape: [num_modules]
   > 
   > # æŸå¤±å‡½æ•°è®¾è®¡ï¼šå¤šç›®æ ‡ä¼˜åŒ–
   > def autopelet_loss(task_loss, latency_penalty, sparsity_reg):
   >     # task_loss: ä¸‹æ¸¸ä»»åŠ¡æŸå¤±ï¼ˆå¦‚äº¤å‰ç†µï¼‰
   >     # latency_penalty: åŸºäºæ¨¡å—FLOPsåŠ æƒçš„å»¶è¿Ÿä¼°è®¡
   >     # sparsity_reg: L1æ­£åˆ™åŒ–ä¿ƒè¿›æ¨¡å—ç¨€ç–
   >     alpha, beta = 1.0, 0.1  # æƒé‡ç³»æ•°ï¼ˆå¯å­¦ä¹ æˆ–æ‰‹åŠ¨è®¾å®šï¼‰
   >     total_loss = task_loss + alpha * latency_penalty + beta * sparsity_reg
   >     return total_loss
   > 
   > # å†—ä½™è·¯å¾„å®šä¹‰ï¼šè¿ç»­Kè½®è´¡çŒ®åº¦<é˜ˆå€¼Î¸çš„æ¨¡å—
   > def is_redundant(module_id, contribution_history, K=1000, theta=0.05):
   >     recent_contrib = contribution_history[-K:]
   >     avg_contrib = np.mean([c[module_id] for c in recent_contrib])
   >     return avg_contrib < theta
   > ```
   >
   > **æ ¸å¿ƒæœºåˆ¶è¯´æ˜**ï¼š
   > - **å¥–åŠ±å‡½æ•°ï¼ˆRLï¼‰æˆ–æŸå¤±å‡½æ•°ï¼ˆDARTSï¼‰**ï¼šè”åˆä¼˜åŒ–ä»»åŠ¡æ€§èƒ½ + å»¶è¿Ÿæˆæœ¬ + æ¨¡å—ç¨€ç–åº¦
   > - **è´¡çŒ®åº¦é‡åŒ–**ï¼šé€šè¿‡é—¨æ§æƒé‡ Ã— æ¨¡å—æ¢¯åº¦èŒƒæ•°ï¼Œè¡¡é‡æ¯ä¸ªæ¨¡å—å¯¹æœ€ç»ˆè¾“å‡ºçš„ä¿¡æ¯å¢ç›Š
   > - **å‰ªæè§¦å‘æ¡ä»¶**ï¼šæ»‘åŠ¨çª—å£å†…å¹³å‡æ¿€æ´»æƒé‡ä½äºé˜ˆå€¼ï¼ˆå¦‚ 0.05ï¼‰ä¸”æŒç»­è¶…è¿‡ 1000 æ­¥
   > - **å†æ¿€æ´»æœºåˆ¶**ï¼šä¿ç•™å‰ªææ¨¡å—çš„ checkpointï¼Œå½“ä»»åŠ¡åˆ†å¸ƒæ¼‚ç§»æ—¶å¯çƒ­åŠ è½½æ¢å¤

> â€œUniPELTå¼€å¯äº†PEFTæŠ€æœ¯ç³»ç»ŸåŒ–æ•´åˆçš„å¤§é—¨ï¼Œä¸‹ä¸€ä¸ªé‡Œç¨‹ç¢‘å¯èƒ½æ˜¯AutoPELTã€‚â€ â€”â€” è¿™ä¸ä»…æ˜¯æŠ€æœ¯é¢„è¨€ï¼Œæ›´æ˜¯ç¤¾åŒºå…±åŒçš„ç›®æ ‡ã€‚


---


### å››ã€å¼€æºç”Ÿæ€å€¡è®®ï¼šä½ çš„è´¡çŒ®ï¼ŒåŠ é€Ÿè¿›åŒ–

UniPELTçš„æ½œåŠ›è¿œæœªè¢«å®Œå…¨æŒ–æ˜ã€‚æˆ‘ä»¬è¯šæŒšé‚€è¯·å¼€å‘è€…ä¸ç ”ç©¶è€…å‚ä¸å…±å»ºï¼š

- **è´¡çŒ®æ–°æ¨¡å—**ï¼šå¦‚æœä½ å¼€å‘äº†æ–°å‹PEFTæ–¹æ³•ï¼ˆå¦‚é’ˆå¯¹é•¿æ–‡æœ¬çš„Sparse Adapterï¼‰ï¼Œæ¬¢è¿æäº¤PRï¼Œä¸°å¯Œå®˜æ–¹æ¨¡å—åº“ã€‚
- **é€‚é…æ–°æ¶æ„**ï¼šå¸®åŠ©UniPELTæ”¯æŒLLaMAã€Mistralã€Qwenç­‰æ–°å…´å¼€æºæ¨¡å‹ï¼Œæ‰©å¤§å…¶é€‚ç”¨èŒƒå›´ã€‚
- **æ„å»ºå·¥å…·é“¾**ï¼šå¼€å‘å¯è§†åŒ–é—¨æ§ç›‘æ§é¢æ¿ã€è‡ªåŠ¨åŒ–è¶…å‚è°ƒä¼˜è„šæœ¬ï¼Œé™ä½ä½¿ç”¨é—¨æ§›ã€‚

å¼€æºä¸æ˜¯ç‹¬å¥ï¼Œè€Œæ˜¯äº¤å“ä¹ã€‚æ¯ä¸€ä¸ªPull Requestï¼Œéƒ½å¯èƒ½æˆä¸ºæ¨åŠ¨ä¸‹ä¸€ä»£AutoPELTè¯ç”Ÿçš„å…³é”®éŸ³ç¬¦ã€‚


---


UniPELTä¸æ˜¯ç»ˆç‚¹ï¼Œè€Œæ˜¯èµ·ç‚¹ã€‚å®ƒè¯æ˜äº†â€œåŠ¨æ€ç»„åˆä¼˜äºé™æ€é€‰æ‹©â€ï¼Œâ€œç³»ç»Ÿæ•´åˆå¼ºäºå­¤ç«‹ä¼˜åŒ–â€ã€‚åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ˜Ÿè¾°å¤§æµ·ä¸­ï¼Œæˆ‘ä»¬æ‰åˆšåˆšå¯èˆªâ€”â€”è€Œä½ ï¼Œå¯ä»¥æˆä¸ºæŒèˆµäººä¹‹ä¸€ã€‚

---


## æ€»ç»“

- UniPELTé€šè¿‡é—¨æ§æœºåˆ¶ç»Ÿä¸€ä¸‰å¤§PEFTæŠ€æœ¯ï¼Œå®ç°åŠ¨æ€ååŒå·¥ä½œ
- åœ¨GLUEä»»åŠ¡ä¸­å…¨é¢ä¼˜äºå•ä¸€æ–¹æ³•ï¼Œå°æ ·æœ¬åœºæ™¯ä¼˜åŠ¿å°¤ä¸ºçªå‡º
- ä»…10%è®¡ç®—å¼€é”€å¢å¹…ï¼Œé€‚åˆå·¥ä¸šçº§éƒ¨ç½²ä¸è¾¹ç¼˜è®¾å¤‡
- æ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•å’Œè°ƒè¯•ï¼Œæ˜¯PEFTç ”ç©¶çš„ç†æƒ³å®éªŒå¹³å°

## å»¶ä¼¸é˜…è¯»

å°è¯•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¤ç°ç»“æœï¼›æ¢ç´¢æ·»åŠ BitFitæˆ–IAÂ³æ¨¡å—ï¼›é˜…è¯»åŸè®ºæ–‡æ·±å…¥ç†è§£é—¨æ§æ­£åˆ™åŒ–ç­–ç•¥ã€‚

## å‚è€ƒèµ„æ–™

1. https://arxiv.org/abs/2203.13132 (UniPELTåŸå§‹è®ºæ–‡)
2. https://huggingface.co/docs/peft/index (Hugging Face PEFTæ–‡æ¡£)
3. https://github.com/huggingface/peft (PEFTå¼€æºåº“)
4. https://gluebenchmark.com/ (GLUEè¯„æµ‹åŸºå‡†)
